<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.15.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tensor Library: include/nn_layers.h Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tensor Library
   </div>
   <div id="projectbrief">High-performance C++ tensor library with GPU, BLAS, autograd, and linear algebra support</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.15.0 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search/",'.html');
</script>
<script type="text/javascript">
$(function() { codefold.init(); });
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(function(){initNavTree('nn__layers_8h_source.html','',''); });
</script>
<div id="container">
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="headertitle"><div class="title">nn_layers.h</div></div>
</div><!--header-->
<div class="contents">
<a href="nn__layers_8h.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a id="l00001" name="l00001"></a><span class="lineno">    1</span></div>
<div class="line"><a id="l00009" name="l00009"></a><span class="lineno">    9</span> </div>
<div class="line"><a id="l00010" name="l00010"></a><span class="lineno">   10</span><span class="preprocessor">#ifndef NN_LAYERS_H</span></div>
<div class="line"><a id="l00011" name="l00011"></a><span class="lineno">   11</span><span class="preprocessor">#define NN_LAYERS_H</span></div>
<div class="line"><a id="l00012" name="l00012"></a><span class="lineno">   12</span> </div>
<div class="line"><a id="l00013" name="l00013"></a><span class="lineno">   13</span><span class="preprocessor">#include &quot;<a class="code" href="tensor_8h.html">tensor.h</a>&quot;</span></div>
<div class="line"><a id="l00014" name="l00014"></a><span class="lineno">   14</span><span class="preprocessor">#include &quot;<a class="code" href="tensor__types_8h.html">tensor_types.h</a>&quot;</span></div>
<div class="line"><a id="l00015" name="l00015"></a><span class="lineno">   15</span><span class="preprocessor">#include &lt;random&gt;</span></div>
<div class="line"><a id="l00016" name="l00016"></a><span class="lineno">   16</span><span class="preprocessor">#include &lt;cmath&gt;</span></div>
<div class="line"><a id="l00017" name="l00017"></a><span class="lineno">   17</span> </div>
<div class="line"><a id="l00018" name="l00018"></a><span class="lineno">   18</span><span class="comment">// Only include CUDA headers if actually compiling with CUDA</span></div>
<div class="line"><a id="l00019" name="l00019"></a><span class="lineno">   19</span><span class="preprocessor">#if defined(USE_GPU) &amp;&amp; defined(__CUDACC__)</span></div>
<div class="line"><a id="l00020" name="l00020"></a><span class="lineno">   20</span><span class="preprocessor">#include &lt;cuda_runtime.h&gt;</span></div>
<div class="line"><a id="l00021" name="l00021"></a><span class="lineno">   21</span><span class="preprocessor">#endif</span></div>
<div class="line"><a id="l00022" name="l00022"></a><span class="lineno">   22</span> </div>
<div class="foldopen" id="foldopen00023" data-start="{" data-end="}">
<div class="line"><a id="l00023" name="l00023"></a><span class="lineno"><a class="line" href="namespacetensor4d.html">   23</a></span><span class="keyword">namespace </span><a class="code hl_namespace" href="namespacetensor4d.html">tensor4d</a> {</div>
<div class="foldopen" id="foldopen00024" data-start="{" data-end="}">
<div class="line"><a id="l00024" name="l00024"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html">   24</a></span><span class="keyword">namespace </span><a class="code hl_namespace" href="namespacetensor4d_1_1nn.html">nn</a> {</div>
<div class="line"><a id="l00025" name="l00025"></a><span class="lineno">   25</span> </div>
<div class="line"><a id="l00026" name="l00026"></a><span class="lineno">   26</span><span class="comment">// Forward declarations</span></div>
<div class="line"><a id="l00027" name="l00027"></a><span class="lineno">   27</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="line"><a id="l00028" name="l00028"></a><span class="lineno">   28</span><span class="keyword">class </span><a class="code hl_class" href="classtensor4d_1_1nn_1_1Layer.html">Layer</a>;</div>
<div class="line"><a id="l00029" name="l00029"></a><span class="lineno">   29</span> </div>
<div class="line"><a id="l00030" name="l00030"></a><span class="lineno">   30</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="line"><a id="l00031" name="l00031"></a><span class="lineno">   31</span><span class="keyword">inline</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="namespacetensor4d_1_1nn.html#af4ed0870610368776140c66a749f99ab">softmax_jacobian</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 1&gt;</a>&amp; softmax_output);</div>
<div class="line"><a id="l00032" name="l00032"></a><span class="lineno">   32</span> </div>
<div class="line"><a id="l00033" name="l00033"></a><span class="lineno">   33</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="line"><a id="l00034" name="l00034"></a><span class="lineno">   34</span><span class="keyword">inline</span> std::vector&lt;Tensor&lt;T, 2&gt;&gt; <a class="code hl_function" href="namespacetensor4d_1_1nn.html#ab2f304e755860ad8c80d7c67cfe953b2">softmax_jacobian_batch</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; softmax_output);</div>
<div class="line"><a id="l00035" name="l00035"></a><span class="lineno">   35</span></div>
<div class="line"><a id="l00039" name="l00039"></a><span class="lineno">   39</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00040" data-start="{" data-end="};">
<div class="line"><a id="l00040" name="l00040"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Layer.html">   40</a></span><span class="keyword">class </span><a class="code hl_class" href="classtensor4d_1_1nn_1_1Layer.html">Layer</a> {</div>
<div class="line"><a id="l00041" name="l00041"></a><span class="lineno">   41</span><span class="keyword">public</span>:</div>
<div class="line"><a id="l00042" name="l00042"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Layer.html#aabd4feffefbe1a92d584484413289a05">   42</a></span>    <span class="keyword">virtual</span> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Layer.html#aabd4feffefbe1a92d584484413289a05">~Layer</a>() = <span class="keywordflow">default</span>;</div>
<div class="line"><a id="l00043" name="l00043"></a><span class="lineno">   43</span>    </div>
<div class="line"><a id="l00049" name="l00049"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Layer.html#a2d171fcf4e726e2bd0f6c9399288436b">   49</a></span>    <span class="keyword">virtual</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Layer.html#a2d171fcf4e726e2bd0f6c9399288436b">forward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; input) = 0;</div>
<div class="line"><a id="l00050" name="l00050"></a><span class="lineno">   50</span>    </div>
<div class="line"><a id="l00056" name="l00056"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Layer.html#a3d9932a950faa4534cc8c468c9152f22">   56</a></span>    <span class="keyword">virtual</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Layer.html#a3d9932a950faa4534cc8c468c9152f22">backward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; grad_output) = 0;</div>
<div class="line"><a id="l00057" name="l00057"></a><span class="lineno">   57</span>    </div>
<div class="line"><a id="l00062" name="l00062"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Layer.html#a5a29143edf7136c7a3d944846f23d05d">   62</a></span>    <span class="keyword">virtual</span> std::vector&lt;Tensor&lt;T, 2&gt;*&gt; <a class="code hl_function" href="classtensor4d_1_1nn_1_1Layer.html#a5a29143edf7136c7a3d944846f23d05d">parameters</a>() { <span class="keywordflow">return</span> {}; }</div>
<div class="line"><a id="l00063" name="l00063"></a><span class="lineno">   63</span>    </div>
<div class="line"><a id="l00068" name="l00068"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Layer.html#a9f069156fc18ad5035bcfa99ddfdda83">   68</a></span>    <span class="keyword">virtual</span> <span class="keywordtype">void</span> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Layer.html#a9f069156fc18ad5035bcfa99ddfdda83">train</a>(<span class="keywordtype">bool</span> mode = <span class="keyword">true</span>) { <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Layer.html#a5046b1e468f41cc660af5f541be017d6">training_</a> = mode; }</div>
<div class="line"><a id="l00069" name="l00069"></a><span class="lineno">   69</span>    </div>
<div class="line"><a id="l00073" name="l00073"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Layer.html#ae5d280d927fb2b5757ae8de43af64a79">   73</a></span>    <span class="keywordtype">bool</span> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Layer.html#ae5d280d927fb2b5757ae8de43af64a79">is_training</a>()<span class="keyword"> const </span>{ <span class="keywordflow">return</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Layer.html#a5046b1e468f41cc660af5f541be017d6">training_</a>; }</div>
<div class="line"><a id="l00074" name="l00074"></a><span class="lineno">   74</span>    </div>
<div class="line"><a id="l00075" name="l00075"></a><span class="lineno">   75</span><span class="keyword">protected</span>:</div>
<div class="line"><a id="l00076" name="l00076"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Layer.html#a5046b1e468f41cc660af5f541be017d6">   76</a></span>    <span class="keywordtype">bool</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Layer.html#a5046b1e468f41cc660af5f541be017d6">training_</a> = <span class="keyword">true</span>;</div>
<div class="line"><a id="l00077" name="l00077"></a><span class="lineno">   77</span>};</div>
</div>
<div class="line"><a id="l00078" name="l00078"></a><span class="lineno">   78</span></div>
<div class="line"><a id="l00087" name="l00087"></a><span class="lineno">   87</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00088" data-start="{" data-end="};">
<div class="line"><a id="l00088" name="l00088"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html">   88</a></span><span class="keyword">class </span><a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#af3a3fecbb2390303fc899b9b2285ea24">Linear</a> : <span class="keyword">public</span> <a class="code hl_class" href="classtensor4d_1_1nn_1_1Layer.html">Layer</a>&lt;T&gt; {</div>
<div class="line"><a id="l00089" name="l00089"></a><span class="lineno">   89</span><span class="keyword">public</span>:</div>
<div class="foldopen" id="foldopen00099" data-start="{" data-end="}">
<div class="line"><a id="l00099" name="l00099"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#af3a3fecbb2390303fc899b9b2285ea24">   99</a></span>    <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#af3a3fecbb2390303fc899b9b2285ea24">Linear</a>(<span class="keywordtype">size_t</span> in_features, <span class="keywordtype">size_t</span> out_features, <span class="keywordtype">bool</span> use_bias = <span class="keyword">true</span>)</div>
<div class="line"><a id="l00100" name="l00100"></a><span class="lineno">  100</span>        : <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a7e262da58b4b499b6eeab66bdd6f84c5">in_features_</a>(in_features), <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a873ae71e9bf64f75ab818dc40c32442e">out_features_</a>(out_features), <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a3f45dfd88be3f8cb6348b6dc317ebc4a">use_bias_</a>(use_bias),</div>
<div class="line"><a id="l00101" name="l00101"></a><span class="lineno">  101</span>#ifdef USE_GPU</div>
<div class="line"><a id="l00102" name="l00102"></a><span class="lineno">  102</span>          <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a794be1290e09a38a6daad16f19aa7126">weights_</a>({out_features, in_features}, <span class="keyword">true</span>),</div>
<div class="line"><a id="l00103" name="l00103"></a><span class="lineno">  103</span>          <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aea284ce51288593c283f10f739ae1696">bias_</a>({1, out_features}, <span class="keyword">true</span>),</div>
<div class="line"><a id="l00104" name="l00104"></a><span class="lineno">  104</span>          <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a9af9fd6d9d5322f77ce1297c3edb82c7">input_</a>({1, in_features}, <span class="keyword">true</span>),</div>
<div class="line"><a id="l00105" name="l00105"></a><span class="lineno">  105</span>          <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a6e7fab012ef8b37597f46e3c82694df4">grad_weights_</a>({out_features, in_features}, <span class="keyword">true</span>),</div>
<div class="line"><a id="l00106" name="l00106"></a><span class="lineno">  106</span>          <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aa24e546b58f66fc945024eb35e5d20f2">grad_bias_</a>({1, out_features}, <span class="keyword">true</span>) {</div>
<div class="line"><a id="l00107" name="l00107"></a><span class="lineno">  107</span><span class="preprocessor">#else</span></div>
<div class="line"><a id="l00108" name="l00108"></a><span class="lineno">  108</span>          <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a794be1290e09a38a6daad16f19aa7126">weights_</a>({out_features, in_features}, <span class="keyword">false</span>),</div>
<div class="line"><a id="l00109" name="l00109"></a><span class="lineno">  109</span>          <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aea284ce51288593c283f10f739ae1696">bias_</a>({1, out_features}, <span class="keyword">false</span>),</div>
<div class="line"><a id="l00110" name="l00110"></a><span class="lineno">  110</span>          <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a9af9fd6d9d5322f77ce1297c3edb82c7">input_</a>({1, in_features}, <span class="keyword">false</span>),</div>
<div class="line"><a id="l00111" name="l00111"></a><span class="lineno">  111</span>          <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a6e7fab012ef8b37597f46e3c82694df4">grad_weights_</a>({out_features, in_features}, <span class="keyword">false</span>),</div>
<div class="line"><a id="l00112" name="l00112"></a><span class="lineno">  112</span>          <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aa24e546b58f66fc945024eb35e5d20f2">grad_bias_</a>({1, out_features}, <span class="keyword">false</span>) {</div>
<div class="line"><a id="l00113" name="l00113"></a><span class="lineno">  113</span><span class="preprocessor">#endif</span></div>
<div class="line"><a id="l00114" name="l00114"></a><span class="lineno">  114</span>        </div>
<div class="line"><a id="l00115" name="l00115"></a><span class="lineno">  115</span>        <span class="comment">// Initialize weights with Xavier/Glorot initialization using optimized pointer access</span></div>
<div class="line"><a id="l00116" name="l00116"></a><span class="lineno">  116</span>        T stddev = std::sqrt(2.0 / (in_features + out_features));</div>
<div class="line"><a id="l00117" name="l00117"></a><span class="lineno">  117</span>        </div>
<div class="line"><a id="l00118" name="l00118"></a><span class="lineno">  118</span>        std::random_device rd;</div>
<div class="line"><a id="l00119" name="l00119"></a><span class="lineno">  119</span>        std::mt19937 gen(rd());</div>
<div class="line"><a id="l00120" name="l00120"></a><span class="lineno">  120</span>        std::normal_distribution&lt;T&gt; dist(0.0, stddev);</div>
<div class="line"><a id="l00121" name="l00121"></a><span class="lineno">  121</span>        </div>
<div class="line"><a id="l00122" name="l00122"></a><span class="lineno">  122</span>        <span class="comment">// Use direct pointer access for efficient initialization</span></div>
<div class="line"><a id="l00123" name="l00123"></a><span class="lineno">  123</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a794be1290e09a38a6daad16f19aa7126">weights_</a>.randn(T(0), stddev);</div>
<div class="line"><a id="l00124" name="l00124"></a><span class="lineno">  124</span>        </div>
<div class="line"><a id="l00125" name="l00125"></a><span class="lineno">  125</span>        <span class="keywordflow">if</span> (<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a3f45dfd88be3f8cb6348b6dc317ebc4a">use_bias_</a>) {</div>
<div class="line"><a id="l00126" name="l00126"></a><span class="lineno">  126</span>            <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aea284ce51288593c283f10f739ae1696">bias_</a>.fill(0);</div>
<div class="line"><a id="l00127" name="l00127"></a><span class="lineno">  127</span>        }</div>
<div class="line"><a id="l00128" name="l00128"></a><span class="lineno">  128</span>    }</div>
</div>
<div class="line"><a id="l00129" name="l00129"></a><span class="lineno">  129</span>    </div>
<div class="foldopen" id="foldopen00130" data-start="{" data-end="}">
<div class="line"><a id="l00130" name="l00130"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#a0af1f08655e4b88cc7adc4e7d8a323e3">  130</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#a0af1f08655e4b88cc7adc4e7d8a323e3">forward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; input)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00131" name="l00131"></a><span class="lineno">  131</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a9af9fd6d9d5322f77ce1297c3edb82c7">input_</a> = input;</div>
<div class="line"><a id="l00132" name="l00132"></a><span class="lineno">  132</span>        </div>
<div class="line"><a id="l00133" name="l00133"></a><span class="lineno">  133</span>        <span class="comment">// output = input @ weights^T</span></div>
<div class="line"><a id="l00134" name="l00134"></a><span class="lineno">  134</span>        <span class="keyword">auto</span> weights_t = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a794be1290e09a38a6daad16f19aa7126">weights_</a>.transpose();</div>
<div class="line"><a id="l00135" name="l00135"></a><span class="lineno">  135</span>        <span class="keyword">auto</span> result_var = input.<a class="code hl_function" href="classTensor.html#a88d5a081d321cbd88ca46ed42ca1112c">matmul</a>(weights_t);</div>
<div class="line"><a id="l00136" name="l00136"></a><span class="lineno">  136</span>        <span class="keyword">auto</span> output = std::get&lt;Tensor&lt;T, 2&gt;&gt;(result_var);</div>
<div class="line"><a id="l00137" name="l00137"></a><span class="lineno">  137</span>        </div>
<div class="line"><a id="l00138" name="l00138"></a><span class="lineno">  138</span>        <span class="comment">// Add bias if present using broadcasting</span></div>
<div class="line"><a id="l00139" name="l00139"></a><span class="lineno">  139</span>        <span class="keywordflow">if</span> (<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a3f45dfd88be3f8cb6348b6dc317ebc4a">use_bias_</a>) {</div>
<div class="line"><a id="l00140" name="l00140"></a><span class="lineno">  140</span>            <span class="keyword">auto</span> output_with_bias_var = output + <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aea284ce51288593c283f10f739ae1696">bias_</a>;</div>
<div class="line"><a id="l00141" name="l00141"></a><span class="lineno">  141</span>            output = std::get&lt;Tensor&lt;T, 2&gt;&gt;(output_with_bias_var);</div>
<div class="line"><a id="l00142" name="l00142"></a><span class="lineno">  142</span>        }</div>
<div class="line"><a id="l00143" name="l00143"></a><span class="lineno">  143</span>        </div>
<div class="line"><a id="l00144" name="l00144"></a><span class="lineno">  144</span>        <span class="keywordflow">return</span> output;</div>
<div class="line"><a id="l00145" name="l00145"></a><span class="lineno">  145</span>    }</div>
</div>
<div class="line"><a id="l00146" name="l00146"></a><span class="lineno">  146</span>    </div>
<div class="foldopen" id="foldopen00147" data-start="{" data-end="}">
<div class="line"><a id="l00147" name="l00147"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#a5364162aed6bb8c7758d88142ac74c1b">  147</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#a5364162aed6bb8c7758d88142ac74c1b">backward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; grad_output)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00148" name="l00148"></a><span class="lineno">  148</span>        <span class="comment">// Gradient w.r.t. weights: grad_output^T @ input</span></div>
<div class="line"><a id="l00149" name="l00149"></a><span class="lineno">  149</span>        <span class="keyword">auto</span> grad_output_t = grad_output.<a class="code hl_function" href="classTensor.html#aed9419fabd015e2c1c35ff6723c1600a">transpose</a>();</div>
<div class="line"><a id="l00150" name="l00150"></a><span class="lineno">  150</span>        <span class="keyword">auto</span> grad_weights_var = grad_output_t.matmul(<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a9af9fd6d9d5322f77ce1297c3edb82c7">input_</a>);</div>
<div class="line"><a id="l00151" name="l00151"></a><span class="lineno">  151</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a6e7fab012ef8b37597f46e3c82694df4">grad_weights_</a> = std::get&lt;Tensor&lt;T, 2&gt;&gt;(grad_weights_var);</div>
<div class="line"><a id="l00152" name="l00152"></a><span class="lineno">  152</span>        </div>
<div class="line"><a id="l00153" name="l00153"></a><span class="lineno">  153</span>        <span class="comment">// Gradient w.r.t. bias: sum over batch dimension using tensor operations</span></div>
<div class="line"><a id="l00154" name="l00154"></a><span class="lineno">  154</span>        <span class="keywordflow">if</span> (<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a3f45dfd88be3f8cb6348b6dc317ebc4a">use_bias_</a>) {</div>
<div class="line"><a id="l00155" name="l00155"></a><span class="lineno">  155</span>            <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aa24e546b58f66fc945024eb35e5d20f2">grad_bias_</a> = grad_output.<a class="code hl_function" href="classTensor.html#ac20a4679e14de6f68e148b447496adfc">sum_axis</a>(0, <span class="keyword">true</span>);</div>
<div class="line"><a id="l00156" name="l00156"></a><span class="lineno">  156</span>        }</div>
<div class="line"><a id="l00157" name="l00157"></a><span class="lineno">  157</span>        </div>
<div class="line"><a id="l00158" name="l00158"></a><span class="lineno">  158</span>        <span class="comment">// Gradient w.r.t. input: grad_output @ weights</span></div>
<div class="line"><a id="l00159" name="l00159"></a><span class="lineno">  159</span>        <span class="keyword">auto</span> grad_input_var = grad_output.<a class="code hl_function" href="classTensor.html#a88d5a081d321cbd88ca46ed42ca1112c">matmul</a>(<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a794be1290e09a38a6daad16f19aa7126">weights_</a>);</div>
<div class="line"><a id="l00160" name="l00160"></a><span class="lineno">  160</span>        <span class="keywordflow">return</span> std::get&lt;Tensor&lt;T, 2&gt;&gt;(grad_input_var);</div>
<div class="line"><a id="l00161" name="l00161"></a><span class="lineno">  161</span>    }</div>
</div>
<div class="line"><a id="l00162" name="l00162"></a><span class="lineno">  162</span>    </div>
<div class="foldopen" id="foldopen00163" data-start="{" data-end="}">
<div class="line"><a id="l00163" name="l00163"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#aa0fab014d4634b17ea27cda3e120d238">  163</a></span>    std::vector&lt;Tensor&lt;T, 2&gt;*&gt; <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#aa0fab014d4634b17ea27cda3e120d238">parameters</a>()<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00164" name="l00164"></a><span class="lineno">  164</span>        std::vector&lt;Tensor&lt;T, 2&gt;*&gt; params;</div>
<div class="line"><a id="l00165" name="l00165"></a><span class="lineno">  165</span>        params.push_back(&amp;<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a794be1290e09a38a6daad16f19aa7126">weights_</a>);</div>
<div class="line"><a id="l00166" name="l00166"></a><span class="lineno">  166</span>        <span class="keywordflow">if</span> (<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a3f45dfd88be3f8cb6348b6dc317ebc4a">use_bias_</a>) {</div>
<div class="line"><a id="l00167" name="l00167"></a><span class="lineno">  167</span>            params.push_back(&amp;<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aea284ce51288593c283f10f739ae1696">bias_</a>);</div>
<div class="line"><a id="l00168" name="l00168"></a><span class="lineno">  168</span>        }</div>
<div class="line"><a id="l00169" name="l00169"></a><span class="lineno">  169</span>        <span class="keywordflow">return</span> params;</div>
<div class="line"><a id="l00170" name="l00170"></a><span class="lineno">  170</span>    }</div>
</div>
<div class="line"><a id="l00171" name="l00171"></a><span class="lineno">  171</span>    </div>
<div class="line"><a id="l00172" name="l00172"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#a08cc48c658f333a45ca0ea084f69ec54">  172</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#a08cc48c658f333a45ca0ea084f69ec54">weights</a>() { <span class="keywordflow">return</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a794be1290e09a38a6daad16f19aa7126">weights_</a>; }</div>
<div class="line"><a id="l00173" name="l00173"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#afefc9f73184487c8cb0abb37641a6932">  173</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#afefc9f73184487c8cb0abb37641a6932">bias</a>() { <span class="keywordflow">return</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aea284ce51288593c283f10f739ae1696">bias_</a>; }</div>
<div class="line"><a id="l00174" name="l00174"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#a0fab0d83bb52b8ed52b9d4679f5a4ead">  174</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#a0fab0d83bb52b8ed52b9d4679f5a4ead">grad_weights</a>() { <span class="keywordflow">return</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a6e7fab012ef8b37597f46e3c82694df4">grad_weights_</a>; }</div>
<div class="line"><a id="l00175" name="l00175"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#a24f7dbc86ee05b08278886cf1e8bf357">  175</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#a24f7dbc86ee05b08278886cf1e8bf357">grad_bias</a>() { <span class="keywordflow">return</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aa24e546b58f66fc945024eb35e5d20f2">grad_bias_</a>; }</div>
<div class="line"><a id="l00176" name="l00176"></a><span class="lineno">  176</span>    </div>
<div class="line"><a id="l00177" name="l00177"></a><span class="lineno">  177</span><span class="keyword">private</span>:</div>
<div class="line"><a id="l00178" name="l00178"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#a7e262da58b4b499b6eeab66bdd6f84c5">  178</a></span>    <span class="keywordtype">size_t</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a7e262da58b4b499b6eeab66bdd6f84c5">in_features_</a>;</div>
<div class="line"><a id="l00179" name="l00179"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#a873ae71e9bf64f75ab818dc40c32442e">  179</a></span>    <span class="keywordtype">size_t</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a873ae71e9bf64f75ab818dc40c32442e">out_features_</a>;</div>
<div class="line"><a id="l00180" name="l00180"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#a3f45dfd88be3f8cb6348b6dc317ebc4a">  180</a></span>    <span class="keywordtype">bool</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a3f45dfd88be3f8cb6348b6dc317ebc4a">use_bias_</a>;</div>
<div class="line"><a id="l00181" name="l00181"></a><span class="lineno">  181</span>    </div>
<div class="line"><a id="l00182" name="l00182"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#a794be1290e09a38a6daad16f19aa7126">  182</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a794be1290e09a38a6daad16f19aa7126">weights_</a>;</div>
<div class="line"><a id="l00183" name="l00183"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#aea284ce51288593c283f10f739ae1696">  183</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aea284ce51288593c283f10f739ae1696">bias_</a>;</div>
<div class="line"><a id="l00184" name="l00184"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#a9af9fd6d9d5322f77ce1297c3edb82c7">  184</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a9af9fd6d9d5322f77ce1297c3edb82c7">input_</a>;</div>
<div class="line"><a id="l00185" name="l00185"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#a6e7fab012ef8b37597f46e3c82694df4">  185</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#a6e7fab012ef8b37597f46e3c82694df4">grad_weights_</a>;</div>
<div class="line"><a id="l00186" name="l00186"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Linear.html#aa24e546b58f66fc945024eb35e5d20f2">  186</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Linear.html#aa24e546b58f66fc945024eb35e5d20f2">grad_bias_</a>;</div>
<div class="line"><a id="l00187" name="l00187"></a><span class="lineno">  187</span>};</div>
</div>
<div class="line"><a id="l00188" name="l00188"></a><span class="lineno">  188</span></div>
<div class="line"><a id="l00194" name="l00194"></a><span class="lineno">  194</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00195" data-start="{" data-end="};">
<div class="line"><a id="l00195" name="l00195"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1ReLU.html">  195</a></span><span class="keyword">class </span><a class="code hl_function" href="classtensor4d_1_1nn_1_1ReLU.html#aed0a602218dd422544cdf5ca46f6d056">ReLU</a> : <span class="keyword">public</span> <a class="code hl_class" href="classtensor4d_1_1nn_1_1Layer.html">Layer</a>&lt;T&gt; {</div>
<div class="line"><a id="l00196" name="l00196"></a><span class="lineno">  196</span><span class="keyword">public</span>:</div>
<div class="line"><a id="l00197" name="l00197"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1ReLU.html#aed0a602218dd422544cdf5ca46f6d056">  197</a></span>    <a class="code hl_function" href="classtensor4d_1_1nn_1_1ReLU.html#aed0a602218dd422544cdf5ca46f6d056">ReLU</a>() : <a class="code hl_variable" href="classtensor4d_1_1nn_1_1ReLU.html#a30f2eabf132b2de1b31ff67d0ad1a5c5">input_</a>({1, 1}) {}  <span class="comment">// Will auto-detect GPU when used</span></div>
<div class="line"><a id="l00198" name="l00198"></a><span class="lineno">  198</span>    </div>
<div class="foldopen" id="foldopen00199" data-start="{" data-end="}">
<div class="line"><a id="l00199" name="l00199"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1ReLU.html#a07b3b9bee90208e9510c4cc740639689">  199</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1ReLU.html#a07b3b9bee90208e9510c4cc740639689">forward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; input)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00200" name="l00200"></a><span class="lineno">  200</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1ReLU.html#a30f2eabf132b2de1b31ff67d0ad1a5c5">input_</a> = input;</div>
<div class="line"><a id="l00201" name="l00201"></a><span class="lineno">  201</span>        <span class="keywordflow">return</span> input.<a class="code hl_function" href="classTensor.html#a74db7588a7150d94b620ad63de1f5980">relu</a>();</div>
<div class="line"><a id="l00202" name="l00202"></a><span class="lineno">  202</span>    }</div>
</div>
<div class="line"><a id="l00203" name="l00203"></a><span class="lineno">  203</span>    </div>
<div class="foldopen" id="foldopen00204" data-start="{" data-end="}">
<div class="line"><a id="l00204" name="l00204"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1ReLU.html#ac991c01d3b1607d533183075fb02f354">  204</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1ReLU.html#ac991c01d3b1607d533183075fb02f354">backward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; grad_output)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00205" name="l00205"></a><span class="lineno">  205</span>        <span class="comment">// ReLU gradient: grad * (input &gt; 0)</span></div>
<div class="line"><a id="l00206" name="l00206"></a><span class="lineno">  206</span>        <span class="comment">// Optimized using tensor comparison operator</span></div>
<div class="line"><a id="l00207" name="l00207"></a><span class="lineno">  207</span>        <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="tensor_8h.html#a249e6ed21f6e3540a11b62433237175a">zeros</a>(<a class="code hl_variable" href="classtensor4d_1_1nn_1_1ReLU.html#a30f2eabf132b2de1b31ff67d0ad1a5c5">input_</a>.shape(), <a class="code hl_variable" href="classtensor4d_1_1nn_1_1ReLU.html#a30f2eabf132b2de1b31ff67d0ad1a5c5">input_</a>.uses_gpu());</div>
<div class="line"><a id="l00208" name="l00208"></a><span class="lineno">  208</span>        <a class="code hl_function" href="tensor_8h.html#a249e6ed21f6e3540a11b62433237175a">zeros</a>.fill(T(0));</div>
<div class="line"><a id="l00209" name="l00209"></a><span class="lineno">  209</span>        <span class="keyword">auto</span> mask = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1ReLU.html#a30f2eabf132b2de1b31ff67d0ad1a5c5">input_</a> &gt; <a class="code hl_function" href="tensor_8h.html#a249e6ed21f6e3540a11b62433237175a">zeros</a>;  <span class="comment">// Returns 0 or 1 tensor</span></div>
<div class="line"><a id="l00210" name="l00210"></a><span class="lineno">  210</span>        <span class="keyword">auto</span> result = grad_output * mask;</div>
<div class="line"><a id="l00211" name="l00211"></a><span class="lineno">  211</span>        <span class="keywordflow">return</span> std::get&lt;Tensor&lt;T, 2&gt;&gt;(result);</div>
<div class="line"><a id="l00212" name="l00212"></a><span class="lineno">  212</span>    }</div>
</div>
<div class="line"><a id="l00213" name="l00213"></a><span class="lineno">  213</span>    </div>
<div class="line"><a id="l00214" name="l00214"></a><span class="lineno">  214</span><span class="keyword">private</span>:</div>
<div class="line"><a id="l00215" name="l00215"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1ReLU.html#a30f2eabf132b2de1b31ff67d0ad1a5c5">  215</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1ReLU.html#a30f2eabf132b2de1b31ff67d0ad1a5c5">input_</a>;</div>
<div class="line"><a id="l00216" name="l00216"></a><span class="lineno">  216</span>};</div>
</div>
<div class="line"><a id="l00217" name="l00217"></a><span class="lineno">  217</span></div>
<div class="line"><a id="l00221" name="l00221"></a><span class="lineno">  221</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00222" data-start="{" data-end="};">
<div class="line"><a id="l00222" name="l00222"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Sigmoid.html">  222</a></span><span class="keyword">class </span><a class="code hl_function" href="classtensor4d_1_1nn_1_1Sigmoid.html#a7a699853be0806bf11af345641f690fd">Sigmoid</a> : <span class="keyword">public</span> <a class="code hl_class" href="classtensor4d_1_1nn_1_1Layer.html">Layer</a>&lt;T&gt; {</div>
<div class="line"><a id="l00223" name="l00223"></a><span class="lineno">  223</span><span class="keyword">public</span>:</div>
<div class="line"><a id="l00224" name="l00224"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Sigmoid.html#a7a699853be0806bf11af345641f690fd">  224</a></span>    <a class="code hl_function" href="classtensor4d_1_1nn_1_1Sigmoid.html#a7a699853be0806bf11af345641f690fd">Sigmoid</a>() : <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Sigmoid.html#aa8058b771d3d30a043d853026cfa06ea">output_</a>({1, 1}) {}</div>
<div class="line"><a id="l00225" name="l00225"></a><span class="lineno">  225</span>    </div>
<div class="foldopen" id="foldopen00226" data-start="{" data-end="}">
<div class="line"><a id="l00226" name="l00226"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Sigmoid.html#a5bde4d78573f5ea7c6c5e4d624bda3fb">  226</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Sigmoid.html#a5bde4d78573f5ea7c6c5e4d624bda3fb">forward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; input)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00227" name="l00227"></a><span class="lineno">  227</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Sigmoid.html#aa8058b771d3d30a043d853026cfa06ea">output_</a> = input.<a class="code hl_function" href="classTensor.html#afff455663faa35d6cfa72aa03f689e94">sigmoid</a>();</div>
<div class="line"><a id="l00228" name="l00228"></a><span class="lineno">  228</span>        <span class="keywordflow">return</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Sigmoid.html#aa8058b771d3d30a043d853026cfa06ea">output_</a>;</div>
<div class="line"><a id="l00229" name="l00229"></a><span class="lineno">  229</span>    }</div>
</div>
<div class="line"><a id="l00230" name="l00230"></a><span class="lineno">  230</span>    </div>
<div class="foldopen" id="foldopen00231" data-start="{" data-end="}">
<div class="line"><a id="l00231" name="l00231"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Sigmoid.html#a41e695161d280883a0002fe199efdf98">  231</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Sigmoid.html#a41e695161d280883a0002fe199efdf98">backward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; grad_output)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00232" name="l00232"></a><span class="lineno">  232</span>        <span class="comment">// sigmoid&#39;(x) = sigmoid(x) * (1 - sigmoid(x))</span></div>
<div class="line"><a id="l00233" name="l00233"></a><span class="lineno">  233</span>        <span class="comment">// Optimized using tensor operations</span></div>
<div class="line"><a id="l00234" name="l00234"></a><span class="lineno">  234</span>        <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="tensor_8h.html#a37ee39b629ca93dc83259b6c8591d0d9">ones</a>(<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Sigmoid.html#aa8058b771d3d30a043d853026cfa06ea">output_</a>.shape(), <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Sigmoid.html#aa8058b771d3d30a043d853026cfa06ea">output_</a>.uses_gpu());</div>
<div class="line"><a id="l00235" name="l00235"></a><span class="lineno">  235</span>        <a class="code hl_function" href="tensor_8h.html#a37ee39b629ca93dc83259b6c8591d0d9">ones</a>.fill(T(1));</div>
<div class="line"><a id="l00236" name="l00236"></a><span class="lineno">  236</span>        <span class="keyword">auto</span> one_minus_output_var = <a class="code hl_function" href="tensor_8h.html#a37ee39b629ca93dc83259b6c8591d0d9">ones</a> - <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Sigmoid.html#aa8058b771d3d30a043d853026cfa06ea">output_</a>;</div>
<div class="line"><a id="l00237" name="l00237"></a><span class="lineno">  237</span>        <span class="keyword">auto</span>&amp; one_minus_output = std::get&lt;Tensor&lt;T, 2&gt;&gt;(one_minus_output_var);</div>
<div class="line"><a id="l00238" name="l00238"></a><span class="lineno">  238</span>        <span class="keyword">auto</span> temp = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Sigmoid.html#aa8058b771d3d30a043d853026cfa06ea">output_</a> * one_minus_output;</div>
<div class="line"><a id="l00239" name="l00239"></a><span class="lineno">  239</span>        <span class="keyword">auto</span> result = grad_output * std::get&lt;Tensor&lt;T, 2&gt;&gt;(temp);</div>
<div class="line"><a id="l00240" name="l00240"></a><span class="lineno">  240</span>        <span class="keywordflow">return</span> std::get&lt;Tensor&lt;T, 2&gt;&gt;(result);</div>
<div class="line"><a id="l00241" name="l00241"></a><span class="lineno">  241</span>    }</div>
</div>
<div class="line"><a id="l00242" name="l00242"></a><span class="lineno">  242</span>    </div>
<div class="line"><a id="l00243" name="l00243"></a><span class="lineno">  243</span><span class="keyword">private</span>:</div>
<div class="line"><a id="l00244" name="l00244"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Sigmoid.html#aa8058b771d3d30a043d853026cfa06ea">  244</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Sigmoid.html#aa8058b771d3d30a043d853026cfa06ea">output_</a>;</div>
<div class="line"><a id="l00245" name="l00245"></a><span class="lineno">  245</span>};</div>
</div>
<div class="line"><a id="l00246" name="l00246"></a><span class="lineno">  246</span></div>
<div class="line"><a id="l00250" name="l00250"></a><span class="lineno">  250</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00251" data-start="{" data-end="};">
<div class="line"><a id="l00251" name="l00251"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Tanh.html">  251</a></span><span class="keyword">class </span><a class="code hl_function" href="classtensor4d_1_1nn_1_1Tanh.html#a0e1b8dd43bc9599b91da6a6beba02e22">Tanh</a> : <span class="keyword">public</span> <a class="code hl_class" href="classtensor4d_1_1nn_1_1Layer.html">Layer</a>&lt;T&gt; {</div>
<div class="line"><a id="l00252" name="l00252"></a><span class="lineno">  252</span><span class="keyword">public</span>:</div>
<div class="line"><a id="l00253" name="l00253"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Tanh.html#a0e1b8dd43bc9599b91da6a6beba02e22">  253</a></span>    <a class="code hl_function" href="classtensor4d_1_1nn_1_1Tanh.html#a0e1b8dd43bc9599b91da6a6beba02e22">Tanh</a>() : <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Tanh.html#a657c2dab547dc900fda0a86818ea1172">output_</a>({1, 1}) {}</div>
<div class="line"><a id="l00254" name="l00254"></a><span class="lineno">  254</span>    </div>
<div class="foldopen" id="foldopen00255" data-start="{" data-end="}">
<div class="line"><a id="l00255" name="l00255"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Tanh.html#a5bf1911c8b5865f171c90efbefbbe437">  255</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Tanh.html#a5bf1911c8b5865f171c90efbefbbe437">forward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; input)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00256" name="l00256"></a><span class="lineno">  256</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Tanh.html#a657c2dab547dc900fda0a86818ea1172">output_</a> = input.<a class="code hl_function" href="classTensor.html#adcc5341aa2156f24c0f4069543c958e1">tanh</a>();</div>
<div class="line"><a id="l00257" name="l00257"></a><span class="lineno">  257</span>        <span class="keywordflow">return</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Tanh.html#a657c2dab547dc900fda0a86818ea1172">output_</a>;</div>
<div class="line"><a id="l00258" name="l00258"></a><span class="lineno">  258</span>    }</div>
</div>
<div class="line"><a id="l00259" name="l00259"></a><span class="lineno">  259</span>    </div>
<div class="foldopen" id="foldopen00260" data-start="{" data-end="}">
<div class="line"><a id="l00260" name="l00260"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Tanh.html#a8211c640d2b962ecad7a5c29203e663c">  260</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Tanh.html#a8211c640d2b962ecad7a5c29203e663c">backward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; grad_output)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00261" name="l00261"></a><span class="lineno">  261</span>        <span class="comment">// tanh&#39;(x) = 1 - tanh^2(x)</span></div>
<div class="line"><a id="l00262" name="l00262"></a><span class="lineno">  262</span>        <span class="comment">// Optimized using tensor operations</span></div>
<div class="line"><a id="l00263" name="l00263"></a><span class="lineno">  263</span>        <span class="keyword">auto</span> tanh_squared_var = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Tanh.html#a657c2dab547dc900fda0a86818ea1172">output_</a> * <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Tanh.html#a657c2dab547dc900fda0a86818ea1172">output_</a>;</div>
<div class="line"><a id="l00264" name="l00264"></a><span class="lineno">  264</span>        <span class="keyword">auto</span>&amp; tanh_squared = std::get&lt;Tensor&lt;T, 2&gt;&gt;(tanh_squared_var);</div>
<div class="line"><a id="l00265" name="l00265"></a><span class="lineno">  265</span>        <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="tensor_8h.html#a37ee39b629ca93dc83259b6c8591d0d9">ones</a>(<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Tanh.html#a657c2dab547dc900fda0a86818ea1172">output_</a>.shape(), <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Tanh.html#a657c2dab547dc900fda0a86818ea1172">output_</a>.uses_gpu());</div>
<div class="line"><a id="l00266" name="l00266"></a><span class="lineno">  266</span>        <a class="code hl_function" href="tensor_8h.html#a37ee39b629ca93dc83259b6c8591d0d9">ones</a>.fill(T(1));</div>
<div class="line"><a id="l00267" name="l00267"></a><span class="lineno">  267</span>        <span class="keyword">auto</span> derivative_var = <a class="code hl_function" href="tensor_8h.html#a37ee39b629ca93dc83259b6c8591d0d9">ones</a> - tanh_squared;</div>
<div class="line"><a id="l00268" name="l00268"></a><span class="lineno">  268</span>        <span class="keyword">auto</span>&amp; derivative = std::get&lt;Tensor&lt;T, 2&gt;&gt;(derivative_var);</div>
<div class="line"><a id="l00269" name="l00269"></a><span class="lineno">  269</span>        <span class="keyword">auto</span> result = grad_output * derivative;</div>
<div class="line"><a id="l00270" name="l00270"></a><span class="lineno">  270</span>        <span class="keywordflow">return</span> std::get&lt;Tensor&lt;T, 2&gt;&gt;(result);</div>
<div class="line"><a id="l00271" name="l00271"></a><span class="lineno">  271</span>    }</div>
</div>
<div class="line"><a id="l00272" name="l00272"></a><span class="lineno">  272</span>    </div>
<div class="line"><a id="l00273" name="l00273"></a><span class="lineno">  273</span><span class="keyword">private</span>:</div>
<div class="line"><a id="l00274" name="l00274"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Tanh.html#a657c2dab547dc900fda0a86818ea1172">  274</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Tanh.html#a657c2dab547dc900fda0a86818ea1172">output_</a>;</div>
<div class="line"><a id="l00275" name="l00275"></a><span class="lineno">  275</span>};</div>
</div>
<div class="line"><a id="l00276" name="l00276"></a><span class="lineno">  276</span></div>
<div class="line"><a id="l00280" name="l00280"></a><span class="lineno">  280</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00281" data-start="{" data-end="};">
<div class="line"><a id="l00281" name="l00281"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Dropout.html">  281</a></span><span class="keyword">class </span><a class="code hl_function" href="classtensor4d_1_1nn_1_1Dropout.html#aea77c1d5fe4c7acaa55174fde51883c5">Dropout</a> : <span class="keyword">public</span> <a class="code hl_class" href="classtensor4d_1_1nn_1_1Layer.html">Layer</a>&lt;T&gt; {</div>
<div class="line"><a id="l00282" name="l00282"></a><span class="lineno">  282</span><span class="keyword">public</span>:</div>
<div class="foldopen" id="foldopen00287" data-start="{" data-end="}">
<div class="line"><a id="l00287" name="l00287"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Dropout.html#aea77c1d5fe4c7acaa55174fde51883c5">  287</a></span>    <span class="keyword">explicit</span> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Dropout.html#aea77c1d5fe4c7acaa55174fde51883c5">Dropout</a>(T p = 0.5) : <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Dropout.html#add8163856fcd8550fc2d2bdbafdf7115">p_</a>(p), <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Dropout.html#a156bdcf7dd306241dfc47d6224c08b1c">mask_</a>({1, 1}, <span class="keyword">false</span>) {</div>
<div class="line"><a id="l00288" name="l00288"></a><span class="lineno">  288</span>        <span class="keywordflow">if</span> (p &lt; 0 || p &gt; 1) {</div>
<div class="line"><a id="l00289" name="l00289"></a><span class="lineno">  289</span>            <span class="keywordflow">throw</span> std::invalid_argument(<span class="stringliteral">&quot;Dropout probability must be between 0 and 1&quot;</span>);</div>
<div class="line"><a id="l00290" name="l00290"></a><span class="lineno">  290</span>        }</div>
<div class="line"><a id="l00291" name="l00291"></a><span class="lineno">  291</span>    }</div>
</div>
<div class="line"><a id="l00292" name="l00292"></a><span class="lineno">  292</span>    </div>
<div class="foldopen" id="foldopen00293" data-start="{" data-end="}">
<div class="line"><a id="l00293" name="l00293"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Dropout.html#aff0f87aa27d1aceca0bbcc6b0655ce24">  293</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Dropout.html#aff0f87aa27d1aceca0bbcc6b0655ce24">forward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; input)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00294" name="l00294"></a><span class="lineno">  294</span>        <span class="keywordflow">if</span> (!this-&gt;<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Layer.html#a5046b1e468f41cc660af5f541be017d6">training_</a>) {</div>
<div class="line"><a id="l00295" name="l00295"></a><span class="lineno">  295</span>            <span class="keywordflow">return</span> input;  <span class="comment">// No dropout during inference</span></div>
<div class="line"><a id="l00296" name="l00296"></a><span class="lineno">  296</span>        }</div>
<div class="line"><a id="l00297" name="l00297"></a><span class="lineno">  297</span>        </div>
<div class="line"><a id="l00298" name="l00298"></a><span class="lineno">  298</span>        <span class="keyword">auto</span> shape = input.<a class="code hl_function" href="classTensor.html#a80b3ffaf92ed36f02d6f4d4230b5d2a0">shape</a>();</div>
<div class="line"><a id="l00299" name="l00299"></a><span class="lineno">  299</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Dropout.html#a156bdcf7dd306241dfc47d6224c08b1c">mask_</a> = <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>(shape, input.<a class="code hl_function" href="classTensor.html#a83ae42925e21d2871d755407d7505c10">uses_gpu</a>());</div>
<div class="line"><a id="l00300" name="l00300"></a><span class="lineno">  300</span>        </div>
<div class="line"><a id="l00301" name="l00301"></a><span class="lineno">  301</span>        <span class="comment">// Generate random mask</span></div>
<div class="line"><a id="l00302" name="l00302"></a><span class="lineno">  302</span>        std::random_device rd;</div>
<div class="line"><a id="l00303" name="l00303"></a><span class="lineno">  303</span>        std::mt19937 gen(rd());</div>
<div class="line"><a id="l00304" name="l00304"></a><span class="lineno">  304</span>        std::bernoulli_distribution dist(1.0 - <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Dropout.html#add8163856fcd8550fc2d2bdbafdf7115">p_</a>);</div>
<div class="line"><a id="l00305" name="l00305"></a><span class="lineno">  305</span>        </div>
<div class="line"><a id="l00306" name="l00306"></a><span class="lineno">  306</span>        T scale = 1.0 / (1.0 - <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Dropout.html#add8163856fcd8550fc2d2bdbafdf7115">p_</a>);</div>
<div class="line"><a id="l00307" name="l00307"></a><span class="lineno">  307</span>        T* mask_ptr = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Dropout.html#a156bdcf7dd306241dfc47d6224c08b1c">mask_</a>.data_ptr();</div>
<div class="line"><a id="l00308" name="l00308"></a><span class="lineno">  308</span>        <span class="keywordtype">size_t</span> total_size = shape[0] * shape[1];</div>
<div class="line"><a id="l00309" name="l00309"></a><span class="lineno">  309</span>        </div>
<div class="line"><a id="l00310" name="l00310"></a><span class="lineno">  310</span>        <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; total_size; ++i) {</div>
<div class="line"><a id="l00311" name="l00311"></a><span class="lineno">  311</span>            mask_ptr[i] = dist(gen) ? scale : 0;</div>
<div class="line"><a id="l00312" name="l00312"></a><span class="lineno">  312</span>        }</div>
<div class="line"><a id="l00313" name="l00313"></a><span class="lineno">  313</span>        </div>
<div class="line"><a id="l00314" name="l00314"></a><span class="lineno">  314</span>        <span class="comment">// Apply mask using tensor multiplication</span></div>
<div class="line"><a id="l00315" name="l00315"></a><span class="lineno">  315</span>        <span class="keyword">auto</span> result = input * <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Dropout.html#a156bdcf7dd306241dfc47d6224c08b1c">mask_</a>;</div>
<div class="line"><a id="l00316" name="l00316"></a><span class="lineno">  316</span>        <span class="keywordflow">return</span> std::get&lt;Tensor&lt;T, 2&gt;&gt;(result);</div>
<div class="line"><a id="l00317" name="l00317"></a><span class="lineno">  317</span>    }</div>
</div>
<div class="line"><a id="l00318" name="l00318"></a><span class="lineno">  318</span>    </div>
<div class="foldopen" id="foldopen00319" data-start="{" data-end="}">
<div class="line"><a id="l00319" name="l00319"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Dropout.html#ad7713d85865e3238e14e920b47f44237">  319</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Dropout.html#ad7713d85865e3238e14e920b47f44237">backward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; grad_output)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00320" name="l00320"></a><span class="lineno">  320</span>        <span class="keywordflow">if</span> (!this-&gt;<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Layer.html#a5046b1e468f41cc660af5f541be017d6">training_</a>) {</div>
<div class="line"><a id="l00321" name="l00321"></a><span class="lineno">  321</span>            <span class="keywordflow">return</span> grad_output;</div>
<div class="line"><a id="l00322" name="l00322"></a><span class="lineno">  322</span>        }</div>
<div class="line"><a id="l00323" name="l00323"></a><span class="lineno">  323</span>        </div>
<div class="line"><a id="l00324" name="l00324"></a><span class="lineno">  324</span>        <span class="comment">// Gradient passes through mask</span></div>
<div class="line"><a id="l00325" name="l00325"></a><span class="lineno">  325</span>        <span class="keyword">auto</span> result = grad_output * <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Dropout.html#a156bdcf7dd306241dfc47d6224c08b1c">mask_</a>;</div>
<div class="line"><a id="l00326" name="l00326"></a><span class="lineno">  326</span>        <span class="keywordflow">return</span> std::get&lt;Tensor&lt;T, 2&gt;&gt;(result);</div>
<div class="line"><a id="l00327" name="l00327"></a><span class="lineno">  327</span>    }</div>
</div>
<div class="line"><a id="l00328" name="l00328"></a><span class="lineno">  328</span>    </div>
<div class="line"><a id="l00329" name="l00329"></a><span class="lineno">  329</span><span class="keyword">private</span>:</div>
<div class="line"><a id="l00330" name="l00330"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Dropout.html#add8163856fcd8550fc2d2bdbafdf7115">  330</a></span>    T <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Dropout.html#add8163856fcd8550fc2d2bdbafdf7115">p_</a>;  <span class="comment">// Dropout probability</span></div>
<div class="line"><a id="l00331" name="l00331"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Dropout.html#a156bdcf7dd306241dfc47d6224c08b1c">  331</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Dropout.html#a156bdcf7dd306241dfc47d6224c08b1c">mask_</a>;</div>
<div class="line"><a id="l00332" name="l00332"></a><span class="lineno">  332</span>};</div>
</div>
<div class="line"><a id="l00333" name="l00333"></a><span class="lineno">  333</span></div>
<div class="line"><a id="l00339" name="l00339"></a><span class="lineno">  339</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00340" data-start="{" data-end="};">
<div class="line"><a id="l00340" name="l00340"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html">  340</a></span><span class="keyword">class </span><a class="code hl_function" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a2775a2774d8a6c4216b4cf2c5691335b">BatchNorm1d</a> : <span class="keyword">public</span> <a class="code hl_class" href="classtensor4d_1_1nn_1_1Layer.html">Layer</a>&lt;T&gt; {</div>
<div class="line"><a id="l00341" name="l00341"></a><span class="lineno">  341</span><span class="keyword">public</span>:</div>
<div class="foldopen" id="foldopen00350" data-start="{" data-end="}">
<div class="line"><a id="l00350" name="l00350"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a2775a2774d8a6c4216b4cf2c5691335b">  350</a></span>    <a class="code hl_function" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a2775a2774d8a6c4216b4cf2c5691335b">BatchNorm1d</a>(<span class="keywordtype">size_t</span> num_features, T eps = 1e-5, T momentum = 0.1)</div>
<div class="line"><a id="l00351" name="l00351"></a><span class="lineno">  351</span>        : <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a2c9cea8478dc951c47bc95c219e87db1">num_features_</a>(num_features), <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a66a9964817b54848afc8b46d7367207d">eps_</a>(eps), <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5615d826982a2f132397419ca7881609">momentum_</a>(momentum),</div>
<div class="line"><a id="l00352" name="l00352"></a><span class="lineno">  352</span>          <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a311f849757fe8d8f70cfd55ce896b5da">gamma_</a>({1, num_features}), beta_({1, num_features}),</div>
<div class="line"><a id="l00353" name="l00353"></a><span class="lineno">  353</span>          running_mean_({1, num_features}), running_var_({1, num_features}),</div>
<div class="line"><a id="l00354" name="l00354"></a><span class="lineno">  354</span>          batch_mean_({1, num_features}), batch_var_({1, num_features}),</div>
<div class="line"><a id="l00355" name="l00355"></a><span class="lineno">  355</span>          input_({1, num_features}), input_normalized_({1, num_features}),</div>
<div class="line"><a id="l00356" name="l00356"></a><span class="lineno">  356</span>          grad_gamma_({1, num_features}), grad_beta_({1, num_features}) {</div>
<div class="line"><a id="l00357" name="l00357"></a><span class="lineno">  357</span>        </div>
<div class="line"><a id="l00358" name="l00358"></a><span class="lineno">  358</span>        gamma_.fill(1);</div>
<div class="line"><a id="l00359" name="l00359"></a><span class="lineno">  359</span>        beta_.fill(0);</div>
<div class="line"><a id="l00360" name="l00360"></a><span class="lineno">  360</span>        running_mean_.fill(0);</div>
<div class="line"><a id="l00361" name="l00361"></a><span class="lineno">  361</span>        running_var_.fill(1);</div>
<div class="line"><a id="l00362" name="l00362"></a><span class="lineno">  362</span>    }</div>
</div>
<div class="line"><a id="l00363" name="l00363"></a><span class="lineno">  363</span>    </div>
<div class="foldopen" id="foldopen00364" data-start="{" data-end="}">
<div class="line"><a id="l00364" name="l00364"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a8e589b2051e3a2c38b1a0feccc33bdb2">  364</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a8e589b2051e3a2c38b1a0feccc33bdb2">forward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; input)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00365" name="l00365"></a><span class="lineno">  365</span>        <span class="keyword">auto</span> shape = input.<a class="code hl_function" href="classTensor.html#a80b3ffaf92ed36f02d6f4d4230b5d2a0">shape</a>();</div>
<div class="line"><a id="l00366" name="l00366"></a><span class="lineno">  366</span>        <span class="keywordtype">size_t</span> batch_size = shape[0];</div>
<div class="line"><a id="l00367" name="l00367"></a><span class="lineno">  367</span>        </div>
<div class="line"><a id="l00368" name="l00368"></a><span class="lineno">  368</span>        <span class="keywordflow">if</span> (this-&gt;<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Layer.html#a5046b1e468f41cc660af5f541be017d6">training_</a>) {</div>
<div class="line"><a id="l00369" name="l00369"></a><span class="lineno">  369</span>            <span class="comment">// Compute batch statistics using tensor operations</span></div>
<div class="line"><a id="l00370" name="l00370"></a><span class="lineno">  370</span>            <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ae56fd1f24787c14016e2fb0f14719b65">batch_mean_</a> = input.<a class="code hl_function" href="classTensor.html#adf1fcb0ddd0ef2db3003401db519ec1c">mean_axis</a>(0, <span class="keyword">true</span>);</div>
<div class="line"><a id="l00371" name="l00371"></a><span class="lineno">  371</span>            </div>
<div class="line"><a id="l00372" name="l00372"></a><span class="lineno">  372</span>            <span class="comment">// Compute variance: E[(X - mean)^2] using broadcasting</span></div>
<div class="line"><a id="l00373" name="l00373"></a><span class="lineno">  373</span>            <span class="keyword">auto</span> centered_var = input - <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ae56fd1f24787c14016e2fb0f14719b65">batch_mean_</a>;</div>
<div class="line"><a id="l00374" name="l00374"></a><span class="lineno">  374</span>            <span class="keyword">auto</span> centered = std::get&lt;Tensor&lt;T, 2&gt;&gt;(centered_var);</div>
<div class="line"><a id="l00375" name="l00375"></a><span class="lineno">  375</span>            <span class="keyword">auto</span> squared_var = centered * centered;</div>
<div class="line"><a id="l00376" name="l00376"></a><span class="lineno">  376</span>            <span class="keyword">auto</span> squared = std::get&lt;Tensor&lt;T, 2&gt;&gt;(squared_var);</div>
<div class="line"><a id="l00377" name="l00377"></a><span class="lineno">  377</span>            <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#af4cb5a26201a5cb84493fd4ade774989">batch_var_</a> = squared.mean_axis(0, <span class="keyword">true</span>);</div>
<div class="line"><a id="l00378" name="l00378"></a><span class="lineno">  378</span>            </div>
<div class="line"><a id="l00379" name="l00379"></a><span class="lineno">  379</span>            <span class="comment">// Update running statistics</span></div>
<div class="line"><a id="l00380" name="l00380"></a><span class="lineno">  380</span>            <span class="keyword">auto</span> running_mean_scaled = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5804439c12b21f719bdab6b8bac01fdc">running_mean_</a> * (T(1) - <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5615d826982a2f132397419ca7881609">momentum_</a>);</div>
<div class="line"><a id="l00381" name="l00381"></a><span class="lineno">  381</span>            <span class="keyword">auto</span> batch_mean_scaled = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ae56fd1f24787c14016e2fb0f14719b65">batch_mean_</a> * <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5615d826982a2f132397419ca7881609">momentum_</a>;</div>
<div class="line"><a id="l00382" name="l00382"></a><span class="lineno">  382</span>            <span class="keyword">auto</span> new_running_mean_var = running_mean_scaled + batch_mean_scaled;</div>
<div class="line"><a id="l00383" name="l00383"></a><span class="lineno">  383</span>            <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5804439c12b21f719bdab6b8bac01fdc">running_mean_</a> = std::get&lt;Tensor&lt;T, 2&gt;&gt;(new_running_mean_var);</div>
<div class="line"><a id="l00384" name="l00384"></a><span class="lineno">  384</span>            </div>
<div class="line"><a id="l00385" name="l00385"></a><span class="lineno">  385</span>            <span class="keyword">auto</span> running_var_scaled = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#afeec797fa0b23c42329aecb063a826b4">running_var_</a> * (T(1) - <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5615d826982a2f132397419ca7881609">momentum_</a>);</div>
<div class="line"><a id="l00386" name="l00386"></a><span class="lineno">  386</span>            <span class="keyword">auto</span> batch_var_scaled = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#af4cb5a26201a5cb84493fd4ade774989">batch_var_</a> * <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5615d826982a2f132397419ca7881609">momentum_</a>;</div>
<div class="line"><a id="l00387" name="l00387"></a><span class="lineno">  387</span>            <span class="keyword">auto</span> new_running_var_var = running_var_scaled + batch_var_scaled;</div>
<div class="line"><a id="l00388" name="l00388"></a><span class="lineno">  388</span>            <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#afeec797fa0b23c42329aecb063a826b4">running_var_</a> = std::get&lt;Tensor&lt;T, 2&gt;&gt;(new_running_var_var);</div>
<div class="line"><a id="l00389" name="l00389"></a><span class="lineno">  389</span>        }</div>
<div class="line"><a id="l00390" name="l00390"></a><span class="lineno">  390</span>        </div>
<div class="line"><a id="l00391" name="l00391"></a><span class="lineno">  391</span>        <span class="comment">// Normalize and scale using broadcasting</span></div>
<div class="line"><a id="l00392" name="l00392"></a><span class="lineno">  392</span>        <span class="keyword">const</span> <span class="keyword">auto</span>&amp; mean = this-&gt;<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Layer.html#a5046b1e468f41cc660af5f541be017d6">training_</a> ? <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ae56fd1f24787c14016e2fb0f14719b65">batch_mean_</a> : <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5804439c12b21f719bdab6b8bac01fdc">running_mean_</a>;</div>
<div class="line"><a id="l00393" name="l00393"></a><span class="lineno">  393</span>        <span class="keyword">const</span> <span class="keyword">auto</span>&amp; var = this-&gt;<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Layer.html#a5046b1e468f41cc660af5f541be017d6">training_</a> ? <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#af4cb5a26201a5cb84493fd4ade774989">batch_var_</a> : <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#afeec797fa0b23c42329aecb063a826b4">running_var_</a>;</div>
<div class="line"><a id="l00394" name="l00394"></a><span class="lineno">  394</span>        </div>
<div class="line"><a id="l00395" name="l00395"></a><span class="lineno">  395</span>        <span class="keyword">auto</span> std_dev = var.map([<span class="keyword">this</span>](T v) { <span class="keywordflow">return</span> std::sqrt(v + <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a66a9964817b54848afc8b46d7367207d">eps_</a>); });</div>
<div class="line"><a id="l00396" name="l00396"></a><span class="lineno">  396</span>        <span class="keyword">auto</span> centered_var = input - mean;</div>
<div class="line"><a id="l00397" name="l00397"></a><span class="lineno">  397</span>        <span class="keyword">auto</span> centered = std::get&lt;Tensor&lt;T, 2&gt;&gt;(centered_var);</div>
<div class="line"><a id="l00398" name="l00398"></a><span class="lineno">  398</span>        <span class="keyword">auto</span> normalized_var = centered / std_dev;</div>
<div class="line"><a id="l00399" name="l00399"></a><span class="lineno">  399</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ac9f443c1167f9978a0ab6a0dc97f02b2">input_normalized_</a> = std::get&lt;Tensor&lt;T, 2&gt;&gt;(normalized_var);</div>
<div class="line"><a id="l00400" name="l00400"></a><span class="lineno">  400</span>        <span class="keyword">auto</span> scaled_var = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ac9f443c1167f9978a0ab6a0dc97f02b2">input_normalized_</a> * <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a311f849757fe8d8f70cfd55ce896b5da">gamma_</a>;</div>
<div class="line"><a id="l00401" name="l00401"></a><span class="lineno">  401</span>        <span class="keyword">auto</span> scaled = std::get&lt;Tensor&lt;T, 2&gt;&gt;(scaled_var);</div>
<div class="line"><a id="l00402" name="l00402"></a><span class="lineno">  402</span>        <span class="keyword">auto</span> output_var = scaled + <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a06aa43a886c3349131e4cec95a847a5e">beta_</a>;</div>
<div class="line"><a id="l00403" name="l00403"></a><span class="lineno">  403</span>        <span class="keyword">auto</span> output = std::get&lt;Tensor&lt;T, 2&gt;&gt;(output_var);</div>
<div class="line"><a id="l00404" name="l00404"></a><span class="lineno">  404</span>        </div>
<div class="line"><a id="l00405" name="l00405"></a><span class="lineno">  405</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a20ef166dce9b80f2697e645af2bb451e">input_</a> = input;</div>
<div class="line"><a id="l00406" name="l00406"></a><span class="lineno">  406</span>        <span class="keywordflow">return</span> output;</div>
<div class="line"><a id="l00407" name="l00407"></a><span class="lineno">  407</span>    }</div>
</div>
<div class="line"><a id="l00408" name="l00408"></a><span class="lineno">  408</span>    </div>
<div class="foldopen" id="foldopen00409" data-start="{" data-end="}">
<div class="line"><a id="l00409" name="l00409"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a48ea5fa069810067b9756e3d24b48b73">  409</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a48ea5fa069810067b9756e3d24b48b73">backward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; grad_output)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00410" name="l00410"></a><span class="lineno">  410</span>        <span class="comment">// Gradients w.r.t. gamma and beta using tensor operations</span></div>
<div class="line"><a id="l00411" name="l00411"></a><span class="lineno">  411</span>        <span class="keyword">auto</span> temp_var = grad_output * <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ac9f443c1167f9978a0ab6a0dc97f02b2">input_normalized_</a>;</div>
<div class="line"><a id="l00412" name="l00412"></a><span class="lineno">  412</span>        <span class="keyword">auto</span> temp = std::get&lt;Tensor&lt;T, 2&gt;&gt;(temp_var);</div>
<div class="line"><a id="l00413" name="l00413"></a><span class="lineno">  413</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a3ba4cd0f7addeb22751af6893a23491f">grad_gamma_</a> = temp.sum_axis(0, <span class="keyword">true</span>);</div>
<div class="line"><a id="l00414" name="l00414"></a><span class="lineno">  414</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a8295c6940c580ef2aed68f78099c7698">grad_beta_</a> = grad_output.<a class="code hl_function" href="classTensor.html#ac20a4679e14de6f68e148b447496adfc">sum_axis</a>(0, <span class="keyword">true</span>);</div>
<div class="line"><a id="l00415" name="l00415"></a><span class="lineno">  415</span>        </div>
<div class="line"><a id="l00416" name="l00416"></a><span class="lineno">  416</span>        <span class="comment">// Gradient w.r.t. input (simplified) using broadcasting</span></div>
<div class="line"><a id="l00417" name="l00417"></a><span class="lineno">  417</span>        <span class="keyword">auto</span> inv_std = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#af4cb5a26201a5cb84493fd4ade774989">batch_var_</a>.map([<span class="keyword">this</span>](T v) { <span class="keywordflow">return</span> T(1) / std::sqrt(v + <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a66a9964817b54848afc8b46d7367207d">eps_</a>); });</div>
<div class="line"><a id="l00418" name="l00418"></a><span class="lineno">  418</span>        <span class="keyword">auto</span> temp1_var = grad_output * <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a311f849757fe8d8f70cfd55ce896b5da">gamma_</a>;</div>
<div class="line"><a id="l00419" name="l00419"></a><span class="lineno">  419</span>        <span class="keyword">auto</span> temp1 = std::get&lt;Tensor&lt;T, 2&gt;&gt;(temp1_var);</div>
<div class="line"><a id="l00420" name="l00420"></a><span class="lineno">  420</span>        <span class="keyword">auto</span> grad_input_var = temp1 * inv_std;</div>
<div class="line"><a id="l00421" name="l00421"></a><span class="lineno">  421</span>        <span class="keywordflow">return</span> std::get&lt;Tensor&lt;T, 2&gt;&gt;(grad_input_var);</div>
<div class="line"><a id="l00422" name="l00422"></a><span class="lineno">  422</span>    }</div>
</div>
<div class="line"><a id="l00423" name="l00423"></a><span class="lineno">  423</span>    </div>
<div class="foldopen" id="foldopen00424" data-start="{" data-end="}">
<div class="line"><a id="l00424" name="l00424"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a00a6dc7b408c9edaf7072301c75b4ccf">  424</a></span>    std::vector&lt;Tensor&lt;T, 2&gt;*&gt; <a class="code hl_function" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a00a6dc7b408c9edaf7072301c75b4ccf">parameters</a>()<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00425" name="l00425"></a><span class="lineno">  425</span>        <span class="keywordflow">return</span> {&amp;<a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a311f849757fe8d8f70cfd55ce896b5da">gamma_</a>, &amp;<a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a06aa43a886c3349131e4cec95a847a5e">beta_</a>};</div>
<div class="line"><a id="l00426" name="l00426"></a><span class="lineno">  426</span>    }</div>
</div>
<div class="line"><a id="l00427" name="l00427"></a><span class="lineno">  427</span>    </div>
<div class="line"><a id="l00428" name="l00428"></a><span class="lineno">  428</span><span class="keyword">private</span>:</div>
<div class="line"><a id="l00429" name="l00429"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a2c9cea8478dc951c47bc95c219e87db1">  429</a></span>    <span class="keywordtype">size_t</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a2c9cea8478dc951c47bc95c219e87db1">num_features_</a>;</div>
<div class="line"><a id="l00430" name="l00430"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a66a9964817b54848afc8b46d7367207d">  430</a></span>    T <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a66a9964817b54848afc8b46d7367207d">eps_</a>;</div>
<div class="line"><a id="l00431" name="l00431"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5615d826982a2f132397419ca7881609">  431</a></span>    T <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5615d826982a2f132397419ca7881609">momentum_</a>;</div>
<div class="line"><a id="l00432" name="l00432"></a><span class="lineno">  432</span>    </div>
<div class="line"><a id="l00433" name="l00433"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a311f849757fe8d8f70cfd55ce896b5da">  433</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a311f849757fe8d8f70cfd55ce896b5da">gamma_</a>;  <span class="comment">// Scale parameter</span></div>
<div class="line"><a id="l00434" name="l00434"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a06aa43a886c3349131e4cec95a847a5e">  434</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a06aa43a886c3349131e4cec95a847a5e">beta_</a>;   <span class="comment">// Shift parameter</span></div>
<div class="line"><a id="l00435" name="l00435"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5804439c12b21f719bdab6b8bac01fdc">  435</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5804439c12b21f719bdab6b8bac01fdc">running_mean_</a>;</div>
<div class="line"><a id="l00436" name="l00436"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#afeec797fa0b23c42329aecb063a826b4">  436</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#afeec797fa0b23c42329aecb063a826b4">running_var_</a>;</div>
<div class="line"><a id="l00437" name="l00437"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ae56fd1f24787c14016e2fb0f14719b65">  437</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ae56fd1f24787c14016e2fb0f14719b65">batch_mean_</a>;</div>
<div class="line"><a id="l00438" name="l00438"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#af4cb5a26201a5cb84493fd4ade774989">  438</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#af4cb5a26201a5cb84493fd4ade774989">batch_var_</a>;</div>
<div class="line"><a id="l00439" name="l00439"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a20ef166dce9b80f2697e645af2bb451e">  439</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a20ef166dce9b80f2697e645af2bb451e">input_</a>;</div>
<div class="line"><a id="l00440" name="l00440"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ac9f443c1167f9978a0ab6a0dc97f02b2">  440</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ac9f443c1167f9978a0ab6a0dc97f02b2">input_normalized_</a>;</div>
<div class="line"><a id="l00441" name="l00441"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a3ba4cd0f7addeb22751af6893a23491f">  441</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a3ba4cd0f7addeb22751af6893a23491f">grad_gamma_</a>;</div>
<div class="line"><a id="l00442" name="l00442"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a8295c6940c580ef2aed68f78099c7698">  442</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a8295c6940c580ef2aed68f78099c7698">grad_beta_</a>;</div>
<div class="line"><a id="l00443" name="l00443"></a><span class="lineno">  443</span>};</div>
</div>
<div class="line"><a id="l00444" name="l00444"></a><span class="lineno">  444</span></div>
<div class="line"><a id="l00450" name="l00450"></a><span class="lineno">  450</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00451" data-start="{" data-end="};">
<div class="line"><a id="l00451" name="l00451"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Softmax.html">  451</a></span><span class="keyword">class </span><a class="code hl_function" href="classtensor4d_1_1nn_1_1Softmax.html#aa600157567a716269b66a30f701921d5">Softmax</a> : <span class="keyword">public</span> <a class="code hl_class" href="classtensor4d_1_1nn_1_1Layer.html">Layer</a>&lt;T&gt; {</div>
<div class="line"><a id="l00452" name="l00452"></a><span class="lineno">  452</span><span class="keyword">public</span>:</div>
<div class="line"><a id="l00453" name="l00453"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Softmax.html#aa600157567a716269b66a30f701921d5">  453</a></span>    <a class="code hl_function" href="classtensor4d_1_1nn_1_1Softmax.html#aa600157567a716269b66a30f701921d5">Softmax</a>() : <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Softmax.html#ac1e254f8cb7a007b6ea0491024ceeead">output_</a>({1, 1}) {}</div>
<div class="line"><a id="l00454" name="l00454"></a><span class="lineno">  454</span>    </div>
<div class="foldopen" id="foldopen00455" data-start="{" data-end="}">
<div class="line"><a id="l00455" name="l00455"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Softmax.html#a62cc4acc056fbd4f8a365637e1159a9e">  455</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Softmax.html#a62cc4acc056fbd4f8a365637e1159a9e">forward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; input)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00456" name="l00456"></a><span class="lineno">  456</span>        <span class="comment">// Use optimized softmax_rows() from tensor.h</span></div>
<div class="line"><a id="l00457" name="l00457"></a><span class="lineno">  457</span>        <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Softmax.html#ac1e254f8cb7a007b6ea0491024ceeead">output_</a> = input.<a class="code hl_function" href="classTensor.html#a65eedb0d5f6932449b64d798c9dd9b5d">softmax_rows</a>();</div>
<div class="line"><a id="l00458" name="l00458"></a><span class="lineno">  458</span>        <span class="keywordflow">return</span> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Softmax.html#ac1e254f8cb7a007b6ea0491024ceeead">output_</a>;</div>
<div class="line"><a id="l00459" name="l00459"></a><span class="lineno">  459</span>    }</div>
</div>
<div class="line"><a id="l00460" name="l00460"></a><span class="lineno">  460</span>    </div>
<div class="foldopen" id="foldopen00461" data-start="{" data-end="}">
<div class="line"><a id="l00461" name="l00461"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Softmax.html#abda8fce936f0e7f02d1779a120db27de">  461</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="classtensor4d_1_1nn_1_1Softmax.html#abda8fce936f0e7f02d1779a120db27de">backward</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; grad_output)<span class="keyword"> override </span>{</div>
<div class="line"><a id="l00462" name="l00462"></a><span class="lineno">  462</span>        <span class="keyword">auto</span> shape = <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Softmax.html#ac1e254f8cb7a007b6ea0491024ceeead">output_</a>.shape();</div>
<div class="line"><a id="l00463" name="l00463"></a><span class="lineno">  463</span>        <span class="keywordtype">size_t</span> batch_size = shape[0];</div>
<div class="line"><a id="l00464" name="l00464"></a><span class="lineno">  464</span>        <span class="keywordtype">size_t</span> num_classes = shape[1];</div>
<div class="line"><a id="l00465" name="l00465"></a><span class="lineno">  465</span>        </div>
<div class="line"><a id="l00466" name="l00466"></a><span class="lineno">  466</span><span class="preprocessor">#ifdef USE_GPU</span></div>
<div class="line"><a id="l00467" name="l00467"></a><span class="lineno">  467</span>        <span class="keyword">auto</span> grad_input = <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>(shape, <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Softmax.html#ac1e254f8cb7a007b6ea0491024ceeead">output_</a>.uses_gpu());</div>
<div class="line"><a id="l00468" name="l00468"></a><span class="lineno">  468</span><span class="preprocessor">#else</span></div>
<div class="line"><a id="l00469" name="l00469"></a><span class="lineno">  469</span>        <span class="keyword">auto</span> grad_input = <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>(shape, <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Softmax.html#ac1e254f8cb7a007b6ea0491024ceeead">output_</a>.uses_gpu());</div>
<div class="line"><a id="l00470" name="l00470"></a><span class="lineno">  470</span><span class="preprocessor">#endif</span></div>
<div class="line"><a id="l00471" name="l00471"></a><span class="lineno">  471</span>        </div>
<div class="line"><a id="l00472" name="l00472"></a><span class="lineno">  472</span>        <span class="comment">// Compute Jacobian matrices for all samples in batch</span></div>
<div class="line"><a id="l00473" name="l00473"></a><span class="lineno">  473</span>        <span class="keyword">auto</span> jacobians = <a class="code hl_function" href="namespacetensor4d_1_1nn.html#ab2f304e755860ad8c80d7c67cfe953b2">softmax_jacobian_batch</a>(<a class="code hl_variable" href="classtensor4d_1_1nn_1_1Softmax.html#ac1e254f8cb7a007b6ea0491024ceeead">output_</a>);</div>
<div class="line"><a id="l00474" name="l00474"></a><span class="lineno">  474</span>        </div>
<div class="line"><a id="l00475" name="l00475"></a><span class="lineno">  475</span>        <span class="comment">// For each sample: grad_input[i] = grad_output[i] @ Jacobian[i]</span></div>
<div class="line"><a id="l00476" name="l00476"></a><span class="lineno">  476</span>        <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; batch_size; ++i) {</div>
<div class="line"><a id="l00477" name="l00477"></a><span class="lineno">  477</span>            <span class="comment">// Extract row i from grad_output as (1 x num_classes)</span></div>
<div class="line"><a id="l00478" name="l00478"></a><span class="lineno">  478</span>            <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> grad_row({1, num_classes}, <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Softmax.html#ac1e254f8cb7a007b6ea0491024ceeead">output_</a>.uses_gpu());</div>
<div class="line"><a id="l00479" name="l00479"></a><span class="lineno">  479</span>            <span class="keyword">const</span> T* grad_data = grad_output.<a class="code hl_function" href="classTensor.html#a3f3c97d177cb960a4423afc3dafc612a">data_ptr</a>() + i * num_classes;</div>
<div class="line"><a id="l00480" name="l00480"></a><span class="lineno">  480</span>            std::copy_n(grad_data, num_classes, grad_row.<a class="code hl_function" href="classTensor.html#a3f3c97d177cb960a4423afc3dafc612a">data_ptr</a>());</div>
<div class="line"><a id="l00481" name="l00481"></a><span class="lineno">  481</span>            </div>
<div class="line"><a id="l00482" name="l00482"></a><span class="lineno">  482</span>            <span class="comment">// Multiply: (1 x num_classes) @ (num_classes x num_classes) = (1 x num_classes)</span></div>
<div class="line"><a id="l00483" name="l00483"></a><span class="lineno">  483</span>            <span class="keyword">auto</span> result_var = grad_row.<a class="code hl_function" href="classTensor.html#a88d5a081d321cbd88ca46ed42ca1112c">matmul</a>(jacobians[i]);</div>
<div class="line"><a id="l00484" name="l00484"></a><span class="lineno">  484</span>            <span class="keyword">auto</span> result = std::get&lt;Tensor&lt;T, 2&gt;&gt;(result_var);</div>
<div class="line"><a id="l00485" name="l00485"></a><span class="lineno">  485</span>            </div>
<div class="line"><a id="l00486" name="l00486"></a><span class="lineno">  486</span>            <span class="comment">// Copy result back to grad_input row i</span></div>
<div class="line"><a id="l00487" name="l00487"></a><span class="lineno">  487</span>            T* grad_input_row = grad_input.data_ptr() + i * num_classes;</div>
<div class="line"><a id="l00488" name="l00488"></a><span class="lineno">  488</span>            std::copy_n(result.data_ptr(), num_classes, grad_input_row);</div>
<div class="line"><a id="l00489" name="l00489"></a><span class="lineno">  489</span>        }</div>
<div class="line"><a id="l00490" name="l00490"></a><span class="lineno">  490</span>        </div>
<div class="line"><a id="l00491" name="l00491"></a><span class="lineno">  491</span>        <span class="keywordflow">return</span> grad_input;</div>
<div class="line"><a id="l00492" name="l00492"></a><span class="lineno">  492</span>    }</div>
</div>
<div class="line"><a id="l00493" name="l00493"></a><span class="lineno">  493</span>    </div>
<div class="line"><a id="l00494" name="l00494"></a><span class="lineno">  494</span><span class="keyword">private</span>:</div>
<div class="line"><a id="l00495" name="l00495"></a><span class="lineno"><a class="line" href="classtensor4d_1_1nn_1_1Softmax.html#ac1e254f8cb7a007b6ea0491024ceeead">  495</a></span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_variable" href="classtensor4d_1_1nn_1_1Softmax.html#ac1e254f8cb7a007b6ea0491024ceeead">output_</a>;</div>
<div class="line"><a id="l00496" name="l00496"></a><span class="lineno">  496</span>};</div>
</div>
<div class="line"><a id="l00497" name="l00497"></a><span class="lineno">  497</span> </div>
<div class="line"><a id="l00498" name="l00498"></a><span class="lineno">  498</span><span class="comment">// Type aliases for convenience</span></div>
<div class="line"><a id="l00499" name="l00499"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#aaf2467899756d1b58455dff42f976904">  499</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#aaf2467899756d1b58455dff42f976904">Linearf</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1Linear.html">Linear&lt;float&gt;</a>;</div>
<div class="line"><a id="l00500" name="l00500"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#a762e1706124db702d93be5bb8b18e629">  500</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#a762e1706124db702d93be5bb8b18e629">Lineard</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1Linear.html">Linear&lt;double&gt;</a>;</div>
<div class="line"><a id="l00501" name="l00501"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#ac63545216aad7e4be2c45c0b70509653">  501</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#ac63545216aad7e4be2c45c0b70509653">ReLUf</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1ReLU.html">ReLU&lt;float&gt;</a>;</div>
<div class="line"><a id="l00502" name="l00502"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#ad4dcbd0157fcb32c118ed38f1f40c58f">  502</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#ad4dcbd0157fcb32c118ed38f1f40c58f">ReLUd</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1ReLU.html">ReLU&lt;double&gt;</a>;</div>
<div class="line"><a id="l00503" name="l00503"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#a83ad7bb2795465463d3684533657dc45">  503</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#a83ad7bb2795465463d3684533657dc45">Sigmoidf</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1Sigmoid.html">Sigmoid&lt;float&gt;</a>;</div>
<div class="line"><a id="l00504" name="l00504"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#a4d5c82c731007a58002b1973150bb7e5">  504</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#a4d5c82c731007a58002b1973150bb7e5">Sigmoidd</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1Sigmoid.html">Sigmoid&lt;double&gt;</a>;</div>
<div class="line"><a id="l00505" name="l00505"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#a322020c52ad6d285a72c09a50ea4b467">  505</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#a322020c52ad6d285a72c09a50ea4b467">Tanhf</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1Tanh.html">Tanh&lt;float&gt;</a>;</div>
<div class="line"><a id="l00506" name="l00506"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#a684fa7cf2cac6ab7b8d62edc2b1da9d6">  506</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#a684fa7cf2cac6ab7b8d62edc2b1da9d6">Tanhd</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1Tanh.html">Tanh&lt;double&gt;</a>;</div>
<div class="line"><a id="l00507" name="l00507"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#a3315ab248317bf7d71c75b77ccc5f985">  507</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#a3315ab248317bf7d71c75b77ccc5f985">Dropoutf</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1Dropout.html">Dropout&lt;float&gt;</a>;</div>
<div class="line"><a id="l00508" name="l00508"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#ac2f3327e85a4c471ec2f30634bde0928">  508</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#ac2f3327e85a4c471ec2f30634bde0928">Dropoutd</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1Dropout.html">Dropout&lt;double&gt;</a>;</div>
<div class="line"><a id="l00509" name="l00509"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#add8e6f802c9d7c435a6c981f545a37f2">  509</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#add8e6f802c9d7c435a6c981f545a37f2">BatchNorm1df</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1BatchNorm1d.html">BatchNorm1d&lt;float&gt;</a>;</div>
<div class="line"><a id="l00510" name="l00510"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#a676f9d19eb47312f01a5c2ef0262e300">  510</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#a676f9d19eb47312f01a5c2ef0262e300">BatchNorm1dd</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1BatchNorm1d.html">BatchNorm1d&lt;double&gt;</a>;</div>
<div class="line"><a id="l00511" name="l00511"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#a227492f3cd887244ddf283565d4f5327">  511</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#a227492f3cd887244ddf283565d4f5327">Softmaxf</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1Softmax.html">Softmax&lt;float&gt;</a>;</div>
<div class="line"><a id="l00512" name="l00512"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#a35558d36da40b43a93f6743d3aaf91eb">  512</a></span><span class="keyword">using </span><a class="code hl_typedef" href="namespacetensor4d_1_1nn.html#a35558d36da40b43a93f6743d3aaf91eb">Softmaxd</a> = <a class="code hl_class" href="classtensor4d_1_1nn_1_1Softmax.html">Softmax&lt;double&gt;</a>;</div>
<div class="line"><a id="l00513" name="l00513"></a><span class="lineno">  513</span> </div>
<div class="line"><a id="l00514" name="l00514"></a><span class="lineno">  514</span><span class="comment">// ============================================</span></div>
<div class="line"><a id="l00515" name="l00515"></a><span class="lineno">  515</span><span class="comment">// Utility Functions for Classification Tasks</span></div>
<div class="line"><a id="l00516" name="l00516"></a><span class="lineno">  516</span><span class="comment">// ============================================</span></div>
<div class="line"><a id="l00517" name="l00517"></a><span class="lineno">  517</span></div>
<div class="line"><a id="l00533" name="l00533"></a><span class="lineno">  533</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00534" data-start="{" data-end="}">
<div class="line"><a id="l00534" name="l00534"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#ac2a195a98fc946944abd82c47d997189">  534</a></span><span class="keyword">inline</span> <span class="keywordtype">void</span> <a class="code hl_function" href="namespacetensor4d_1_1nn.html#ac2a195a98fc946944abd82c47d997189">label_to_onehot</a>(uint8_t label, <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; onehot, <span class="keywordtype">size_t</span> batch_idx, <span class="keywordtype">size_t</span> num_classes) {</div>
<div class="line"><a id="l00535" name="l00535"></a><span class="lineno">  535</span>    <span class="comment">// Zero the entire row first, then set the appropriate index</span></div>
<div class="line"><a id="l00536" name="l00536"></a><span class="lineno">  536</span>    T* row_ptr = onehot.<a class="code hl_function" href="classTensor.html#a3f3c97d177cb960a4423afc3dafc612a">data_ptr</a>() + batch_idx * num_classes;</div>
<div class="line"><a id="l00537" name="l00537"></a><span class="lineno">  537</span>    std::fill_n(row_ptr, num_classes, T(0));</div>
<div class="line"><a id="l00538" name="l00538"></a><span class="lineno">  538</span>    row_ptr[label] = T(1);</div>
<div class="line"><a id="l00539" name="l00539"></a><span class="lineno">  539</span>}</div>
</div>
<div class="line"><a id="l00540" name="l00540"></a><span class="lineno">  540</span></div>
<div class="line"><a id="l00557" name="l00557"></a><span class="lineno">  557</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00558" data-start="{" data-end="}">
<div class="line"><a id="l00558" name="l00558"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#a7682f7b06565fe530c833139aa5c52cd">  558</a></span><span class="keyword">inline</span> T <a class="code hl_function" href="namespacetensor4d_1_1nn.html#a7682f7b06565fe530c833139aa5c52cd">cross_entropy_loss</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; predictions, <span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; targets, T epsilon = T(1e-7)) {</div>
<div class="line"><a id="l00559" name="l00559"></a><span class="lineno">  559</span>    <span class="comment">// Compute using optimized tensor operations:</span></div>
<div class="line"><a id="l00560" name="l00560"></a><span class="lineno">  560</span>    <span class="comment">// loss = -sum(targets * log(predictions + epsilon)) / batch_size</span></div>
<div class="line"><a id="l00561" name="l00561"></a><span class="lineno">  561</span>    </div>
<div class="line"><a id="l00562" name="l00562"></a><span class="lineno">  562</span>    <span class="comment">// Add epsilon for numerical stability: predictions + epsilon</span></div>
<div class="line"><a id="l00563" name="l00563"></a><span class="lineno">  563</span>    <span class="keyword">auto</span> pred_stable = predictions.<a class="code hl_function" href="classTensor.html#a314dfaedc4f6b2dc8e3b44bbc5381eb3">map</a>([epsilon](T x) { <span class="keywordflow">return</span> x + epsilon; });</div>
<div class="line"><a id="l00564" name="l00564"></a><span class="lineno">  564</span>    </div>
<div class="line"><a id="l00565" name="l00565"></a><span class="lineno">  565</span>    <span class="comment">// Compute log: log(predictions + epsilon)</span></div>
<div class="line"><a id="l00566" name="l00566"></a><span class="lineno">  566</span>    <span class="keyword">auto</span> log_pred = pred_stable.log();</div>
<div class="line"><a id="l00567" name="l00567"></a><span class="lineno">  567</span>    </div>
<div class="line"><a id="l00568" name="l00568"></a><span class="lineno">  568</span>    <span class="comment">// Element-wise multiplication: targets * log(predictions + epsilon)</span></div>
<div class="line"><a id="l00569" name="l00569"></a><span class="lineno">  569</span>    <span class="keyword">auto</span> product_var = targets * log_pred;</div>
<div class="line"><a id="l00570" name="l00570"></a><span class="lineno">  570</span>    <span class="keyword">auto</span> product = std::get&lt;Tensor&lt;T, 2&gt;&gt;(product_var);</div>
<div class="line"><a id="l00571" name="l00571"></a><span class="lineno">  571</span>    </div>
<div class="line"><a id="l00572" name="l00572"></a><span class="lineno">  572</span>    <span class="comment">// Sum all elements and negate</span></div>
<div class="line"><a id="l00573" name="l00573"></a><span class="lineno">  573</span>    T total_loss = -product.<a class="code hl_function" href="classTensor.html#a3d279878b61732de2f1986dfc7de8400">sum</a>();</div>
<div class="line"><a id="l00574" name="l00574"></a><span class="lineno">  574</span>    </div>
<div class="line"><a id="l00575" name="l00575"></a><span class="lineno">  575</span>    <span class="comment">// Divide by batch size</span></div>
<div class="line"><a id="l00576" name="l00576"></a><span class="lineno">  576</span>    <span class="keyword">auto</span> shape = predictions.<a class="code hl_function" href="classTensor.html#a80b3ffaf92ed36f02d6f4d4230b5d2a0">shape</a>();</div>
<div class="line"><a id="l00577" name="l00577"></a><span class="lineno">  577</span>    <span class="keywordflow">return</span> total_loss / <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(shape[0]);</div>
<div class="line"><a id="l00578" name="l00578"></a><span class="lineno">  578</span>}</div>
</div>
<div class="line"><a id="l00579" name="l00579"></a><span class="lineno">  579</span></div>
<div class="line"><a id="l00589" name="l00589"></a><span class="lineno">  589</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00590" data-start="{" data-end="}">
<div class="line"><a id="l00590" name="l00590"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#aabc59d678ea2b307e86f063f32102470">  590</a></span><span class="keyword">inline</span> T <a class="code hl_function" href="namespacetensor4d_1_1nn.html#aabc59d678ea2b307e86f063f32102470">compute_accuracy</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; predictions, <span class="keyword">const</span> std::vector&lt;uint8_t&gt;&amp; labels, <span class="keywordtype">size_t</span> offset = 0) {</div>
<div class="line"><a id="l00591" name="l00591"></a><span class="lineno">  591</span>    <span class="keyword">auto</span> shape = predictions.<a class="code hl_function" href="classTensor.html#a80b3ffaf92ed36f02d6f4d4230b5d2a0">shape</a>();</div>
<div class="line"><a id="l00592" name="l00592"></a><span class="lineno">  592</span>    <span class="keywordtype">size_t</span> batch_size = shape[0];</div>
<div class="line"><a id="l00593" name="l00593"></a><span class="lineno">  593</span>    </div>
<div class="line"><a id="l00594" name="l00594"></a><span class="lineno">  594</span>    <span class="comment">// Use tensor operation to get predicted classes for all rows at once</span></div>
<div class="line"><a id="l00595" name="l00595"></a><span class="lineno">  595</span>    <span class="keyword">auto</span> pred_classes = predictions.<a class="code hl_function" href="classTensor.html#a7e486401bc789c0d2344b2b271a3316b">argmax_rows</a>();</div>
<div class="line"><a id="l00596" name="l00596"></a><span class="lineno">  596</span>    </div>
<div class="line"><a id="l00597" name="l00597"></a><span class="lineno">  597</span>    <span class="comment">// Count correct predictions</span></div>
<div class="line"><a id="l00598" name="l00598"></a><span class="lineno">  598</span>    <span class="keywordtype">size_t</span> correct = 0;</div>
<div class="line"><a id="l00599" name="l00599"></a><span class="lineno">  599</span>    <span class="keyword">const</span> <span class="keywordtype">size_t</span>* pred_data = pred_classes.data_ptr();</div>
<div class="line"><a id="l00600" name="l00600"></a><span class="lineno">  600</span>    </div>
<div class="line"><a id="l00601" name="l00601"></a><span class="lineno">  601</span>    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; batch_size; ++i) {</div>
<div class="line"><a id="l00602" name="l00602"></a><span class="lineno">  602</span>        <span class="keywordflow">if</span> (pred_data[i] == labels[offset + i]) {</div>
<div class="line"><a id="l00603" name="l00603"></a><span class="lineno">  603</span>            ++correct;</div>
<div class="line"><a id="l00604" name="l00604"></a><span class="lineno">  604</span>        }</div>
<div class="line"><a id="l00605" name="l00605"></a><span class="lineno">  605</span>    }</div>
<div class="line"><a id="l00606" name="l00606"></a><span class="lineno">  606</span>    </div>
<div class="line"><a id="l00607" name="l00607"></a><span class="lineno">  607</span>    <span class="keywordflow">return</span> <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(correct) / <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(batch_size);</div>
<div class="line"><a id="l00608" name="l00608"></a><span class="lineno">  608</span>}</div>
</div>
<div class="line"><a id="l00609" name="l00609"></a><span class="lineno">  609</span></div>
<div class="line"><a id="l00628" name="l00628"></a><span class="lineno">  628</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00629" data-start="{" data-end="}">
<div class="line"><a id="l00629" name="l00629"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#a834567aedd488a7451d76fec1bbba55e">  629</a></span><span class="keyword">inline</span> <span class="keywordtype">void</span> <a class="code hl_function" href="namespacetensor4d_1_1nn.html#a834567aedd488a7451d76fec1bbba55e">update_linear_layer</a>(<a class="code hl_class" href="classtensor4d_1_1nn_1_1Linear.html">Linear&lt;T&gt;</a>&amp; layer, T lr) {</div>
<div class="line"><a id="l00630" name="l00630"></a><span class="lineno">  630</span>    <span class="comment">// Get weights and their gradients</span></div>
<div class="line"><a id="l00631" name="l00631"></a><span class="lineno">  631</span>    <span class="keyword">auto</span>&amp; <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#a08cc48c658f333a45ca0ea084f69ec54">weights</a> = layer.<a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#a08cc48c658f333a45ca0ea084f69ec54">weights</a>();</div>
<div class="line"><a id="l00632" name="l00632"></a><span class="lineno">  632</span>    <span class="keyword">auto</span>&amp; <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#afefc9f73184487c8cb0abb37641a6932">bias</a> = layer.<a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#afefc9f73184487c8cb0abb37641a6932">bias</a>();</div>
<div class="line"><a id="l00633" name="l00633"></a><span class="lineno">  633</span>    <span class="keyword">auto</span>&amp; grad_w = layer.<a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#a0fab0d83bb52b8ed52b9d4679f5a4ead">grad_weights</a>();</div>
<div class="line"><a id="l00634" name="l00634"></a><span class="lineno">  634</span>    <span class="keyword">auto</span>&amp; grad_b = layer.<a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#a24f7dbc86ee05b08278886cf1e8bf357">grad_bias</a>();</div>
<div class="line"><a id="l00635" name="l00635"></a><span class="lineno">  635</span>    </div>
<div class="line"><a id="l00636" name="l00636"></a><span class="lineno">  636</span>    <span class="comment">// Update weights: w -= lr * grad_w using tensor operations</span></div>
<div class="line"><a id="l00637" name="l00637"></a><span class="lineno">  637</span>    <span class="keyword">auto</span> weight_update_var = <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#a08cc48c658f333a45ca0ea084f69ec54">weights</a> - (grad_w * lr);</div>
<div class="line"><a id="l00638" name="l00638"></a><span class="lineno">  638</span>    <span class="keyword">auto</span> weight_update = std::get&lt;Tensor&lt;T, 2&gt;&gt;(weight_update_var);</div>
<div class="line"><a id="l00639" name="l00639"></a><span class="lineno">  639</span>    <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#a08cc48c658f333a45ca0ea084f69ec54">weights</a> = weight_update;</div>
<div class="line"><a id="l00640" name="l00640"></a><span class="lineno">  640</span>    </div>
<div class="line"><a id="l00641" name="l00641"></a><span class="lineno">  641</span>    <span class="comment">// Update bias: b -= lr * grad_b using tensor operations</span></div>
<div class="line"><a id="l00642" name="l00642"></a><span class="lineno">  642</span>    <span class="keyword">auto</span> bias_update_var = <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#afefc9f73184487c8cb0abb37641a6932">bias</a> - (grad_b * lr);</div>
<div class="line"><a id="l00643" name="l00643"></a><span class="lineno">  643</span>    <span class="keyword">auto</span> bias_update = std::get&lt;Tensor&lt;T, 2&gt;&gt;(bias_update_var);</div>
<div class="line"><a id="l00644" name="l00644"></a><span class="lineno">  644</span>    <a class="code hl_function" href="classtensor4d_1_1nn_1_1Linear.html#afefc9f73184487c8cb0abb37641a6932">bias</a> = bias_update;</div>
<div class="line"><a id="l00645" name="l00645"></a><span class="lineno">  645</span>}</div>
</div>
<div class="line"><a id="l00646" name="l00646"></a><span class="lineno">  646</span></div>
<div class="line"><a id="l00661" name="l00661"></a><span class="lineno">  661</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00662" data-start="{" data-end="}">
<div class="line"><a id="l00662" name="l00662"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#af4ed0870610368776140c66a749f99ab">  662</a></span><span class="keyword">inline</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> <a class="code hl_function" href="namespacetensor4d_1_1nn.html#af4ed0870610368776140c66a749f99ab">softmax_jacobian</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 1&gt;</a>&amp; softmax_output) {</div>
<div class="line"><a id="l00663" name="l00663"></a><span class="lineno">  663</span>    <span class="keyword">auto</span> dims = softmax_output.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>();</div>
<div class="line"><a id="l00664" name="l00664"></a><span class="lineno">  664</span>    <span class="keywordtype">size_t</span> n = dims[0];</div>
<div class="line"><a id="l00665" name="l00665"></a><span class="lineno">  665</span>    <span class="keywordtype">bool</span> use_gpu = softmax_output.<a class="code hl_function" href="classTensor.html#a83ae42925e21d2871d755407d7505c10">uses_gpu</a>();</div>
<div class="line"><a id="l00666" name="l00666"></a><span class="lineno">  666</span>    </div>
<div class="line"><a id="l00667" name="l00667"></a><span class="lineno">  667</span>    <span class="comment">// For jacobian computation, we use CPU since it involves small matrices</span></div>
<div class="line"><a id="l00668" name="l00668"></a><span class="lineno">  668</span>    <span class="comment">// (typically 10-1000 classes) and complex diagonal operations</span></div>
<div class="line"><a id="l00669" name="l00669"></a><span class="lineno">  669</span>    <span class="comment">// The main GPU benefit is in the forward/backward matmul operations</span></div>
<div class="line"><a id="l00670" name="l00670"></a><span class="lineno">  670</span>    </div>
<div class="line"><a id="l00671" name="l00671"></a><span class="lineno">  671</span>    <span class="comment">// Get CPU data</span></div>
<div class="line"><a id="l00672" name="l00672"></a><span class="lineno">  672</span>    <span class="keyword">const</span> T* s_data = softmax_output.<a class="code hl_function" href="classTensor.html#a3f3c97d177cb960a4423afc3dafc612a">data_ptr</a>();  <span class="comment">// This syncs to CPU if needed</span></div>
<div class="line"><a id="l00673" name="l00673"></a><span class="lineno">  673</span>    </div>
<div class="line"><a id="l00674" name="l00674"></a><span class="lineno">  674</span>    <span class="comment">// Compute outer product on CPU for simplicity</span></div>
<div class="line"><a id="l00675" name="l00675"></a><span class="lineno">  675</span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> outer_product({n, n}, <span class="keyword">false</span>);  <span class="comment">// CPU tensor</span></div>
<div class="line"><a id="l00676" name="l00676"></a><span class="lineno">  676</span>    T* outer_data = outer_product.<a class="code hl_function" href="classTensor.html#a3f3c97d177cb960a4423afc3dafc612a">data_ptr</a>();</div>
<div class="line"><a id="l00677" name="l00677"></a><span class="lineno">  677</span>    </div>
<div class="line"><a id="l00678" name="l00678"></a><span class="lineno">  678</span>    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; n; ++i) {</div>
<div class="line"><a id="l00679" name="l00679"></a><span class="lineno">  679</span>        <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> j = 0; j &lt; n; ++j) {</div>
<div class="line"><a id="l00680" name="l00680"></a><span class="lineno">  680</span>            outer_data[i * n + j] = s_data[i] * s_data[j];</div>
<div class="line"><a id="l00681" name="l00681"></a><span class="lineno">  681</span>        }</div>
<div class="line"><a id="l00682" name="l00682"></a><span class="lineno">  682</span>    }</div>
<div class="line"><a id="l00683" name="l00683"></a><span class="lineno">  683</span>    </div>
<div class="line"><a id="l00684" name="l00684"></a><span class="lineno">  684</span>    <span class="comment">// Create diagonal matrix on CPU</span></div>
<div class="line"><a id="l00685" name="l00685"></a><span class="lineno">  685</span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a> diag_s({n, n}, <span class="keyword">false</span>);  <span class="comment">// CPU tensor</span></div>
<div class="line"><a id="l00686" name="l00686"></a><span class="lineno">  686</span>    diag_s.<a class="code hl_function" href="classTensor.html#a234f74cd16bd13561f7963ba29efb163">fill</a>(T(0));</div>
<div class="line"><a id="l00687" name="l00687"></a><span class="lineno">  687</span>    T* diag_data = diag_s.data_ptr();</div>
<div class="line"><a id="l00688" name="l00688"></a><span class="lineno">  688</span>    </div>
<div class="line"><a id="l00689" name="l00689"></a><span class="lineno">  689</span>    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; n; ++i) {</div>
<div class="line"><a id="l00690" name="l00690"></a><span class="lineno">  690</span>        diag_data[i * n + i] = s_data[i];</div>
<div class="line"><a id="l00691" name="l00691"></a><span class="lineno">  691</span>    }</div>
<div class="line"><a id="l00692" name="l00692"></a><span class="lineno">  692</span>    </div>
<div class="line"><a id="l00693" name="l00693"></a><span class="lineno">  693</span>    <span class="comment">// Compute J = diag(s) - outer_product using tensor operation</span></div>
<div class="line"><a id="l00694" name="l00694"></a><span class="lineno">  694</span>    <span class="keyword">auto</span> result_var = diag_s - outer_product;</div>
<div class="line"><a id="l00695" name="l00695"></a><span class="lineno">  695</span>    <span class="keyword">auto</span> jacobian = std::get&lt;Tensor&lt;T, 2&gt;&gt;(result_var);</div>
<div class="line"><a id="l00696" name="l00696"></a><span class="lineno">  696</span>    </div>
<div class="line"><a id="l00697" name="l00697"></a><span class="lineno">  697</span>    <span class="comment">// If input was on GPU, optionally move result to GPU</span></div>
<div class="line"><a id="l00698" name="l00698"></a><span class="lineno">  698</span>    <span class="comment">// For now, keep on CPU since jacobian is used immediately for gradient computation</span></div>
<div class="line"><a id="l00699" name="l00699"></a><span class="lineno">  699</span>    <span class="comment">// which may involve element-wise operations easier on CPU</span></div>
<div class="line"><a id="l00700" name="l00700"></a><span class="lineno">  700</span>    </div>
<div class="line"><a id="l00701" name="l00701"></a><span class="lineno">  701</span>    <span class="keywordflow">return</span> jacobian;</div>
<div class="line"><a id="l00702" name="l00702"></a><span class="lineno">  702</span>}</div>
</div>
<div class="line"><a id="l00703" name="l00703"></a><span class="lineno">  703</span></div>
<div class="line"><a id="l00714" name="l00714"></a><span class="lineno">  714</span><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="foldopen" id="foldopen00715" data-start="{" data-end="}">
<div class="line"><a id="l00715" name="l00715"></a><span class="lineno"><a class="line" href="namespacetensor4d_1_1nn.html#ab2f304e755860ad8c80d7c67cfe953b2">  715</a></span><span class="keyword">inline</span> std::vector&lt;Tensor&lt;T, 2&gt;&gt; <a class="code hl_function" href="namespacetensor4d_1_1nn.html#ab2f304e755860ad8c80d7c67cfe953b2">softmax_jacobian_batch</a>(<span class="keyword">const</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 2&gt;</a>&amp; softmax_output) {</div>
<div class="line"><a id="l00716" name="l00716"></a><span class="lineno">  716</span>    <span class="keyword">auto</span> shape = softmax_output.<a class="code hl_function" href="classTensor.html#a80b3ffaf92ed36f02d6f4d4230b5d2a0">shape</a>();</div>
<div class="line"><a id="l00717" name="l00717"></a><span class="lineno">  717</span>    <span class="keywordtype">size_t</span> batch_size = shape[0];</div>
<div class="line"><a id="l00718" name="l00718"></a><span class="lineno">  718</span>    <span class="keywordtype">size_t</span> num_classes = shape[1];</div>
<div class="line"><a id="l00719" name="l00719"></a><span class="lineno">  719</span>    </div>
<div class="line"><a id="l00720" name="l00720"></a><span class="lineno">  720</span>    std::vector&lt;Tensor&lt;T, 2&gt;&gt; jacobians;</div>
<div class="line"><a id="l00721" name="l00721"></a><span class="lineno">  721</span>    jacobians.reserve(batch_size);</div>
<div class="line"><a id="l00722" name="l00722"></a><span class="lineno">  722</span>    </div>
<div class="line"><a id="l00723" name="l00723"></a><span class="lineno">  723</span>    <span class="comment">// Get CPU data for processing</span></div>
<div class="line"><a id="l00724" name="l00724"></a><span class="lineno">  724</span>    <span class="keyword">const</span> T* data = softmax_output.<a class="code hl_function" href="classTensor.html#a3f3c97d177cb960a4423afc3dafc612a">data_ptr</a>();</div>
<div class="line"><a id="l00725" name="l00725"></a><span class="lineno">  725</span>    </div>
<div class="line"><a id="l00726" name="l00726"></a><span class="lineno">  726</span>    <span class="comment">// Process each sample - extract row and compute Jacobian</span></div>
<div class="line"><a id="l00727" name="l00727"></a><span class="lineno">  727</span>    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; batch_size; ++i) {</div>
<div class="line"><a id="l00728" name="l00728"></a><span class="lineno">  728</span>        <span class="comment">// Extract row i into a 1D tensor (on CPU)</span></div>
<div class="line"><a id="l00729" name="l00729"></a><span class="lineno">  729</span>        <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 1&gt;</a> <a class="code hl_function" href="tensor_8h.html#adf94b1b28dd9932dd278abaa3cb7a9ee">row</a>({num_classes}, <span class="keyword">false</span>);</div>
<div class="line"><a id="l00730" name="l00730"></a><span class="lineno">  730</span>        std::copy_n(data + i * num_classes, num_classes, <a class="code hl_function" href="tensor_8h.html#adf94b1b28dd9932dd278abaa3cb7a9ee">row</a>.data_ptr());</div>
<div class="line"><a id="l00731" name="l00731"></a><span class="lineno">  731</span>        </div>
<div class="line"><a id="l00732" name="l00732"></a><span class="lineno">  732</span>        <span class="comment">// Compute Jacobian for this row</span></div>
<div class="line"><a id="l00733" name="l00733"></a><span class="lineno">  733</span>        jacobians.push_back(<a class="code hl_function" href="namespacetensor4d_1_1nn.html#af4ed0870610368776140c66a749f99ab">softmax_jacobian</a>(<a class="code hl_function" href="tensor_8h.html#adf94b1b28dd9932dd278abaa3cb7a9ee">row</a>));</div>
<div class="line"><a id="l00734" name="l00734"></a><span class="lineno">  734</span>    }</div>
<div class="line"><a id="l00735" name="l00735"></a><span class="lineno">  735</span>    </div>
<div class="line"><a id="l00736" name="l00736"></a><span class="lineno">  736</span>    <span class="keywordflow">return</span> jacobians;</div>
<div class="line"><a id="l00737" name="l00737"></a><span class="lineno">  737</span>}</div>
</div>
<div class="line"><a id="l00738" name="l00738"></a><span class="lineno">  738</span> </div>
<div class="line"><a id="l00739" name="l00739"></a><span class="lineno">  739</span>} <span class="comment">// namespace nn</span></div>
</div>
<div class="line"><a id="l00740" name="l00740"></a><span class="lineno">  740</span>} <span class="comment">// namespace tensor4d</span></div>
</div>
<div class="line"><a id="l00741" name="l00741"></a><span class="lineno">  741</span> </div>
<div class="line"><a id="l00742" name="l00742"></a><span class="lineno">  742</span><span class="preprocessor">#endif </span><span class="comment">// NN_LAYERS_H</span></div>
<div class="ttc" id="aclassTensor_html"><div class="ttname"><a href="classTensor.html">Tensor</a></div><div class="ttdoc">Forward declaration for autograd.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00518">tensor.h:518</a></div></div>
<div class="ttc" id="aclassTensor_html_a03d314303958ffc5644796d8025cd012"><div class="ttname"><a href="classTensor.html#a03d314303958ffc5644796d8025cd012">Tensor::dims</a></div><div class="ttdeci">TensorIndices&lt; N &gt; dims() const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00854">tensor.h:854</a></div></div>
<div class="ttc" id="aclassTensor_html_a234f74cd16bd13561f7963ba29efb163"><div class="ttname"><a href="classTensor.html#a234f74cd16bd13561f7963ba29efb163">Tensor::fill</a></div><div class="ttdeci">void fill(const T &amp;value)</div><div class="ttdoc">Fill the tensor with a specified value.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00842">tensor.h:842</a></div></div>
<div class="ttc" id="aclassTensor_html_a314dfaedc4f6b2dc8e3b44bbc5381eb3"><div class="ttname"><a href="classTensor.html#a314dfaedc4f6b2dc8e3b44bbc5381eb3">Tensor::map</a></div><div class="ttdeci">Tensor&lt; T, N &gt; map(Func func) const</div><div class="ttdoc">Apply a function to each element of the tensor (creates new tensor).</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l01917">tensor.h:1917</a></div></div>
<div class="ttc" id="aclassTensor_html_a3d279878b61732de2f1986dfc7de8400"><div class="ttname"><a href="classTensor.html#a3d279878b61732de2f1986dfc7de8400">Tensor::sum</a></div><div class="ttdeci">T sum() const</div><div class="ttdoc">Compute sum reduction along all dimensions.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l02749">tensor.h:2749</a></div></div>
<div class="ttc" id="aclassTensor_html_a3f3c97d177cb960a4423afc3dafc612a"><div class="ttname"><a href="classTensor.html#a3f3c97d177cb960a4423afc3dafc612a">Tensor::data_ptr</a></div><div class="ttdeci">const T * data_ptr() const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00632">tensor.h:632</a></div></div>
<div class="ttc" id="aclassTensor_html_a65eedb0d5f6932449b64d798c9dd9b5d"><div class="ttname"><a href="classTensor.html#a65eedb0d5f6932449b64d798c9dd9b5d">Tensor::softmax_rows</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; softmax_rows() const</div><div class="ttdoc">Apply softmax activation along rows (axis=1) for 2D tensors.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l05767">tensor.h:5767</a></div></div>
<div class="ttc" id="aclassTensor_html_a74db7588a7150d94b620ad63de1f5980"><div class="ttname"><a href="classTensor.html#a74db7588a7150d94b620ad63de1f5980">Tensor::relu</a></div><div class="ttdeci">Tensor&lt; T, N &gt; relu() const</div><div class="ttdoc">Apply ReLU (Rectified Linear Unit) to all elements (creates new tensor).</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l02364">tensor.h:2364</a></div></div>
<div class="ttc" id="aclassTensor_html_a7e486401bc789c0d2344b2b271a3316b"><div class="ttname"><a href="classTensor.html#a7e486401bc789c0d2344b2b271a3316b">Tensor::argmax_rows</a></div><div class="ttdeci">Tensor&lt; size_t, 1 &gt; argmax_rows() const</div><div class="ttdoc">Find argmax for each row in a 2D tensor.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l05815">tensor.h:5815</a></div></div>
<div class="ttc" id="aclassTensor_html_a80b3ffaf92ed36f02d6f4d4230b5d2a0"><div class="ttname"><a href="classTensor.html#a80b3ffaf92ed36f02d6f4d4230b5d2a0">Tensor::shape</a></div><div class="ttdeci">TensorIndices&lt; N &gt; shape() const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00860">tensor.h:860</a></div></div>
<div class="ttc" id="aclassTensor_html_a83ae42925e21d2871d755407d7505c10"><div class="ttname"><a href="classTensor.html#a83ae42925e21d2871d755407d7505c10">Tensor::uses_gpu</a></div><div class="ttdeci">bool uses_gpu() const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00866">tensor.h:866</a></div></div>
<div class="ttc" id="aclassTensor_html_a88d5a081d321cbd88ca46ed42ca1112c"><div class="ttname"><a href="classTensor.html#a88d5a081d321cbd88ca46ed42ca1112c">Tensor::matmul</a></div><div class="ttdeci">std::enable_if&lt; M==2, TensorResult&lt; Tensor&lt; T, 2 &gt; &gt; &gt;::type matmul(const Tensor&lt; T, 2 &gt; &amp;other) const</div><div class="ttdoc">Matrix multiplication with autograd support (matmul).</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l03921">tensor.h:3921</a></div></div>
<div class="ttc" id="aclassTensor_html_ac20a4679e14de6f68e148b447496adfc"><div class="ttname"><a href="classTensor.html#ac20a4679e14de6f68e148b447496adfc">Tensor::sum_axis</a></div><div class="ttdeci">Tensor&lt; T, N &gt; sum_axis(int axis, bool keepdims=false) const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l04089">tensor.h:4089</a></div></div>
<div class="ttc" id="aclassTensor_html_adcc5341aa2156f24c0f4069543c958e1"><div class="ttname"><a href="classTensor.html#adcc5341aa2156f24c0f4069543c958e1">Tensor::tanh</a></div><div class="ttdeci">Tensor&lt; T, N &gt; tanh() const</div><div class="ttdoc">Apply hyperbolic tangent (tanh) to all elements (creates new tensor).</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l02226">tensor.h:2226</a></div></div>
<div class="ttc" id="aclassTensor_html_adf1fcb0ddd0ef2db3003401db519ec1c"><div class="ttname"><a href="classTensor.html#adf1fcb0ddd0ef2db3003401db519ec1c">Tensor::mean_axis</a></div><div class="ttdeci">Tensor&lt; T, N &gt; mean_axis(int axis, bool keepdims=false) const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l04213">tensor.h:4213</a></div></div>
<div class="ttc" id="aclassTensor_html_aed9419fabd015e2c1c35ff6723c1600a"><div class="ttname"><a href="classTensor.html#aed9419fabd015e2c1c35ff6723c1600a">Tensor::transpose</a></div><div class="ttdeci">Tensor&lt; T, N &gt; transpose() const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l04576">tensor.h:4576</a></div></div>
<div class="ttc" id="aclassTensor_html_afff455663faa35d6cfa72aa03f689e94"><div class="ttname"><a href="classTensor.html#afff455663faa35d6cfa72aa03f689e94">Tensor::sigmoid</a></div><div class="ttdeci">Tensor&lt; T, N &gt; sigmoid() const</div><div class="ttdoc">Apply sigmoid function to all elements (creates new tensor).</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l02273">tensor.h:2273</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html">tensor4d::nn::BatchNorm1d</a></div><div class="ttdoc">Batch Normalization layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00340">nn_layers.h:340</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a00a6dc7b408c9edaf7072301c75b4ccf"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a00a6dc7b408c9edaf7072301c75b4ccf">tensor4d::nn::BatchNorm1d::parameters</a></div><div class="ttdeci">std::vector&lt; Tensor&lt; T, 2 &gt; * &gt; parameters() override</div><div class="ttdoc">Get trainable parameters.</div><div class="ttdef"><b>Definition</b> <a href="#l00424">nn_layers.h:424</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a06aa43a886c3349131e4cec95a847a5e"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a06aa43a886c3349131e4cec95a847a5e">tensor4d::nn::BatchNorm1d&lt; float &gt;::beta_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; beta_</div><div class="ttdef"><b>Definition</b> <a href="#l00434">nn_layers.h:434</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a20ef166dce9b80f2697e645af2bb451e"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a20ef166dce9b80f2697e645af2bb451e">tensor4d::nn::BatchNorm1d&lt; float &gt;::input_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; input_</div><div class="ttdef"><b>Definition</b> <a href="#l00439">nn_layers.h:439</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a2775a2774d8a6c4216b4cf2c5691335b"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a2775a2774d8a6c4216b4cf2c5691335b">tensor4d::nn::BatchNorm1d::BatchNorm1d</a></div><div class="ttdeci">BatchNorm1d(size_t num_features, T eps=1e-5, T momentum=0.1)</div><div class="ttdoc">Construct batch normalization layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00350">nn_layers.h:350</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a2c9cea8478dc951c47bc95c219e87db1"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a2c9cea8478dc951c47bc95c219e87db1">tensor4d::nn::BatchNorm1d&lt; float &gt;::num_features_</a></div><div class="ttdeci">size_t num_features_</div><div class="ttdef"><b>Definition</b> <a href="#l00429">nn_layers.h:429</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a311f849757fe8d8f70cfd55ce896b5da"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a311f849757fe8d8f70cfd55ce896b5da">tensor4d::nn::BatchNorm1d&lt; float &gt;::gamma_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; gamma_</div><div class="ttdef"><b>Definition</b> <a href="#l00433">nn_layers.h:433</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a3ba4cd0f7addeb22751af6893a23491f"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a3ba4cd0f7addeb22751af6893a23491f">tensor4d::nn::BatchNorm1d&lt; float &gt;::grad_gamma_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; grad_gamma_</div><div class="ttdef"><b>Definition</b> <a href="#l00441">nn_layers.h:441</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a48ea5fa069810067b9756e3d24b48b73"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a48ea5fa069810067b9756e3d24b48b73">tensor4d::nn::BatchNorm1d::backward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; backward(const Tensor&lt; T, 2 &gt; &amp;grad_output) override</div><div class="ttdoc">Backward pass (gradient computation).</div><div class="ttdef"><b>Definition</b> <a href="#l00409">nn_layers.h:409</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a5615d826982a2f132397419ca7881609"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5615d826982a2f132397419ca7881609">tensor4d::nn::BatchNorm1d&lt; float &gt;::momentum_</a></div><div class="ttdeci">float momentum_</div><div class="ttdef"><b>Definition</b> <a href="#l00431">nn_layers.h:431</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a5804439c12b21f719bdab6b8bac01fdc"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a5804439c12b21f719bdab6b8bac01fdc">tensor4d::nn::BatchNorm1d&lt; float &gt;::running_mean_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; running_mean_</div><div class="ttdef"><b>Definition</b> <a href="#l00435">nn_layers.h:435</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a66a9964817b54848afc8b46d7367207d"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a66a9964817b54848afc8b46d7367207d">tensor4d::nn::BatchNorm1d&lt; float &gt;::eps_</a></div><div class="ttdeci">float eps_</div><div class="ttdef"><b>Definition</b> <a href="#l00430">nn_layers.h:430</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a8295c6940c580ef2aed68f78099c7698"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a8295c6940c580ef2aed68f78099c7698">tensor4d::nn::BatchNorm1d&lt; float &gt;::grad_beta_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; grad_beta_</div><div class="ttdef"><b>Definition</b> <a href="#l00442">nn_layers.h:442</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_a8e589b2051e3a2c38b1a0feccc33bdb2"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#a8e589b2051e3a2c38b1a0feccc33bdb2">tensor4d::nn::BatchNorm1d::forward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; forward(const Tensor&lt; T, 2 &gt; &amp;input) override</div><div class="ttdoc">Forward pass through the layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00364">nn_layers.h:364</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_ac9f443c1167f9978a0ab6a0dc97f02b2"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ac9f443c1167f9978a0ab6a0dc97f02b2">tensor4d::nn::BatchNorm1d&lt; float &gt;::input_normalized_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; input_normalized_</div><div class="ttdef"><b>Definition</b> <a href="#l00440">nn_layers.h:440</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_ae56fd1f24787c14016e2fb0f14719b65"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#ae56fd1f24787c14016e2fb0f14719b65">tensor4d::nn::BatchNorm1d&lt; float &gt;::batch_mean_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; batch_mean_</div><div class="ttdef"><b>Definition</b> <a href="#l00437">nn_layers.h:437</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_af4cb5a26201a5cb84493fd4ade774989"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#af4cb5a26201a5cb84493fd4ade774989">tensor4d::nn::BatchNorm1d&lt; float &gt;::batch_var_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; batch_var_</div><div class="ttdef"><b>Definition</b> <a href="#l00438">nn_layers.h:438</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1BatchNorm1d_html_afeec797fa0b23c42329aecb063a826b4"><div class="ttname"><a href="classtensor4d_1_1nn_1_1BatchNorm1d.html#afeec797fa0b23c42329aecb063a826b4">tensor4d::nn::BatchNorm1d&lt; float &gt;::running_var_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; running_var_</div><div class="ttdef"><b>Definition</b> <a href="#l00436">nn_layers.h:436</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Dropout_html"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Dropout.html">tensor4d::nn::Dropout</a></div><div class="ttdoc">Dropout layer for regularization.</div><div class="ttdef"><b>Definition</b> <a href="#l00281">nn_layers.h:281</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Dropout_html_a156bdcf7dd306241dfc47d6224c08b1c"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Dropout.html#a156bdcf7dd306241dfc47d6224c08b1c">tensor4d::nn::Dropout&lt; float &gt;::mask_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; mask_</div><div class="ttdef"><b>Definition</b> <a href="#l00331">nn_layers.h:331</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Dropout_html_ad7713d85865e3238e14e920b47f44237"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Dropout.html#ad7713d85865e3238e14e920b47f44237">tensor4d::nn::Dropout::backward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; backward(const Tensor&lt; T, 2 &gt; &amp;grad_output) override</div><div class="ttdoc">Backward pass (gradient computation).</div><div class="ttdef"><b>Definition</b> <a href="#l00319">nn_layers.h:319</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Dropout_html_add8163856fcd8550fc2d2bdbafdf7115"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Dropout.html#add8163856fcd8550fc2d2bdbafdf7115">tensor4d::nn::Dropout&lt; float &gt;::p_</a></div><div class="ttdeci">float p_</div><div class="ttdef"><b>Definition</b> <a href="#l00330">nn_layers.h:330</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Dropout_html_aea77c1d5fe4c7acaa55174fde51883c5"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Dropout.html#aea77c1d5fe4c7acaa55174fde51883c5">tensor4d::nn::Dropout::Dropout</a></div><div class="ttdeci">Dropout(T p=0.5)</div><div class="ttdoc">Construct dropout layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00287">nn_layers.h:287</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Dropout_html_aff0f87aa27d1aceca0bbcc6b0655ce24"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Dropout.html#aff0f87aa27d1aceca0bbcc6b0655ce24">tensor4d::nn::Dropout::forward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; forward(const Tensor&lt; T, 2 &gt; &amp;input) override</div><div class="ttdoc">Forward pass through the layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00293">nn_layers.h:293</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Layer_html"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Layer.html">tensor4d::nn::Layer</a></div><div class="ttdoc">Base class for all neural network layers.</div><div class="ttdef"><b>Definition</b> <a href="#l00040">nn_layers.h:40</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Layer_html_a2d171fcf4e726e2bd0f6c9399288436b"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Layer.html#a2d171fcf4e726e2bd0f6c9399288436b">tensor4d::nn::Layer::forward</a></div><div class="ttdeci">virtual Tensor&lt; T, 2 &gt; forward(const Tensor&lt; T, 2 &gt; &amp;input)=0</div><div class="ttdoc">Forward pass through the layer.</div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Layer_html_a3d9932a950faa4534cc8c468c9152f22"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Layer.html#a3d9932a950faa4534cc8c468c9152f22">tensor4d::nn::Layer::backward</a></div><div class="ttdeci">virtual Tensor&lt; T, 2 &gt; backward(const Tensor&lt; T, 2 &gt; &amp;grad_output)=0</div><div class="ttdoc">Backward pass (gradient computation).</div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Layer_html_a5046b1e468f41cc660af5f541be017d6"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Layer.html#a5046b1e468f41cc660af5f541be017d6">tensor4d::nn::Layer::training_</a></div><div class="ttdeci">bool training_</div><div class="ttdef"><b>Definition</b> <a href="#l00076">nn_layers.h:76</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Layer_html_a5a29143edf7136c7a3d944846f23d05d"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Layer.html#a5a29143edf7136c7a3d944846f23d05d">tensor4d::nn::Layer::parameters</a></div><div class="ttdeci">virtual std::vector&lt; Tensor&lt; T, 2 &gt; * &gt; parameters()</div><div class="ttdoc">Get trainable parameters.</div><div class="ttdef"><b>Definition</b> <a href="#l00062">nn_layers.h:62</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Layer_html_a9f069156fc18ad5035bcfa99ddfdda83"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Layer.html#a9f069156fc18ad5035bcfa99ddfdda83">tensor4d::nn::Layer::train</a></div><div class="ttdeci">virtual void train(bool mode=true)</div><div class="ttdoc">Set training mode.</div><div class="ttdef"><b>Definition</b> <a href="#l00068">nn_layers.h:68</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Layer_html_aabd4feffefbe1a92d584484413289a05"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Layer.html#aabd4feffefbe1a92d584484413289a05">tensor4d::nn::Layer::~Layer</a></div><div class="ttdeci">virtual ~Layer()=default</div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Layer_html_ae5d280d927fb2b5757ae8de43af64a79"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Layer.html#ae5d280d927fb2b5757ae8de43af64a79">tensor4d::nn::Layer::is_training</a></div><div class="ttdeci">bool is_training() const</div><div class="ttdoc">Check if layer is in training mode.</div><div class="ttdef"><b>Definition</b> <a href="#l00073">nn_layers.h:73</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html">tensor4d::nn::Linear</a></div><div class="ttdoc">Linear (Dense/Fully Connected) layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00088">nn_layers.h:88</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_a08cc48c658f333a45ca0ea084f69ec54"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#a08cc48c658f333a45ca0ea084f69ec54">tensor4d::nn::Linear::weights</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; &amp; weights()</div><div class="ttdef"><b>Definition</b> <a href="#l00172">nn_layers.h:172</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_a0af1f08655e4b88cc7adc4e7d8a323e3"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#a0af1f08655e4b88cc7adc4e7d8a323e3">tensor4d::nn::Linear::forward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; forward(const Tensor&lt; T, 2 &gt; &amp;input) override</div><div class="ttdoc">Forward pass through the layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00130">nn_layers.h:130</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_a0fab0d83bb52b8ed52b9d4679f5a4ead"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#a0fab0d83bb52b8ed52b9d4679f5a4ead">tensor4d::nn::Linear::grad_weights</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; &amp; grad_weights()</div><div class="ttdef"><b>Definition</b> <a href="#l00174">nn_layers.h:174</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_a24f7dbc86ee05b08278886cf1e8bf357"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#a24f7dbc86ee05b08278886cf1e8bf357">tensor4d::nn::Linear::grad_bias</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; &amp; grad_bias()</div><div class="ttdef"><b>Definition</b> <a href="#l00175">nn_layers.h:175</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_a3f45dfd88be3f8cb6348b6dc317ebc4a"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#a3f45dfd88be3f8cb6348b6dc317ebc4a">tensor4d::nn::Linear&lt; float &gt;::use_bias_</a></div><div class="ttdeci">bool use_bias_</div><div class="ttdef"><b>Definition</b> <a href="#l00180">nn_layers.h:180</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_a5364162aed6bb8c7758d88142ac74c1b"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#a5364162aed6bb8c7758d88142ac74c1b">tensor4d::nn::Linear::backward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; backward(const Tensor&lt; T, 2 &gt; &amp;grad_output) override</div><div class="ttdoc">Backward pass (gradient computation).</div><div class="ttdef"><b>Definition</b> <a href="#l00147">nn_layers.h:147</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_a6e7fab012ef8b37597f46e3c82694df4"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#a6e7fab012ef8b37597f46e3c82694df4">tensor4d::nn::Linear::grad_weights_</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; grad_weights_</div><div class="ttdef"><b>Definition</b> <a href="#l00185">nn_layers.h:185</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_a794be1290e09a38a6daad16f19aa7126"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#a794be1290e09a38a6daad16f19aa7126">tensor4d::nn::Linear&lt; float &gt;::weights_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; weights_</div><div class="ttdef"><b>Definition</b> <a href="#l00182">nn_layers.h:182</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_a7e262da58b4b499b6eeab66bdd6f84c5"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#a7e262da58b4b499b6eeab66bdd6f84c5">tensor4d::nn::Linear&lt; float &gt;::in_features_</a></div><div class="ttdeci">size_t in_features_</div><div class="ttdef"><b>Definition</b> <a href="#l00178">nn_layers.h:178</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_a873ae71e9bf64f75ab818dc40c32442e"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#a873ae71e9bf64f75ab818dc40c32442e">tensor4d::nn::Linear&lt; float &gt;::out_features_</a></div><div class="ttdeci">size_t out_features_</div><div class="ttdef"><b>Definition</b> <a href="#l00179">nn_layers.h:179</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_a9af9fd6d9d5322f77ce1297c3edb82c7"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#a9af9fd6d9d5322f77ce1297c3edb82c7">tensor4d::nn::Linear::input_</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; input_</div><div class="ttdef"><b>Definition</b> <a href="#l00184">nn_layers.h:184</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_aa0fab014d4634b17ea27cda3e120d238"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#aa0fab014d4634b17ea27cda3e120d238">tensor4d::nn::Linear::parameters</a></div><div class="ttdeci">std::vector&lt; Tensor&lt; T, 2 &gt; * &gt; parameters() override</div><div class="ttdoc">Get trainable parameters.</div><div class="ttdef"><b>Definition</b> <a href="#l00163">nn_layers.h:163</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_aa24e546b58f66fc945024eb35e5d20f2"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#aa24e546b58f66fc945024eb35e5d20f2">tensor4d::nn::Linear::grad_bias_</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; grad_bias_</div><div class="ttdef"><b>Definition</b> <a href="#l00186">nn_layers.h:186</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_aea284ce51288593c283f10f739ae1696"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#aea284ce51288593c283f10f739ae1696">tensor4d::nn::Linear::bias_</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; bias_</div><div class="ttdef"><b>Definition</b> <a href="#l00183">nn_layers.h:183</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_af3a3fecbb2390303fc899b9b2285ea24"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#af3a3fecbb2390303fc899b9b2285ea24">tensor4d::nn::Linear::Linear</a></div><div class="ttdeci">Linear(size_t in_features, size_t out_features, bool use_bias=true)</div><div class="ttdoc">Construct a linear layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00099">nn_layers.h:99</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Linear_html_afefc9f73184487c8cb0abb37641a6932"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Linear.html#afefc9f73184487c8cb0abb37641a6932">tensor4d::nn::Linear::bias</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; &amp; bias()</div><div class="ttdef"><b>Definition</b> <a href="#l00173">nn_layers.h:173</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1ReLU_html"><div class="ttname"><a href="classtensor4d_1_1nn_1_1ReLU.html">tensor4d::nn::ReLU</a></div><div class="ttdoc">ReLU activation layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00195">nn_layers.h:195</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1ReLU_html_a07b3b9bee90208e9510c4cc740639689"><div class="ttname"><a href="classtensor4d_1_1nn_1_1ReLU.html#a07b3b9bee90208e9510c4cc740639689">tensor4d::nn::ReLU::forward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; forward(const Tensor&lt; T, 2 &gt; &amp;input) override</div><div class="ttdoc">Forward pass through the layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00199">nn_layers.h:199</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1ReLU_html_a30f2eabf132b2de1b31ff67d0ad1a5c5"><div class="ttname"><a href="classtensor4d_1_1nn_1_1ReLU.html#a30f2eabf132b2de1b31ff67d0ad1a5c5">tensor4d::nn::ReLU&lt; float &gt;::input_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; input_</div><div class="ttdef"><b>Definition</b> <a href="#l00215">nn_layers.h:215</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1ReLU_html_ac991c01d3b1607d533183075fb02f354"><div class="ttname"><a href="classtensor4d_1_1nn_1_1ReLU.html#ac991c01d3b1607d533183075fb02f354">tensor4d::nn::ReLU::backward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; backward(const Tensor&lt; T, 2 &gt; &amp;grad_output) override</div><div class="ttdoc">Backward pass (gradient computation).</div><div class="ttdef"><b>Definition</b> <a href="#l00204">nn_layers.h:204</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1ReLU_html_aed0a602218dd422544cdf5ca46f6d056"><div class="ttname"><a href="classtensor4d_1_1nn_1_1ReLU.html#aed0a602218dd422544cdf5ca46f6d056">tensor4d::nn::ReLU::ReLU</a></div><div class="ttdeci">ReLU()</div><div class="ttdef"><b>Definition</b> <a href="#l00197">nn_layers.h:197</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Sigmoid_html"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Sigmoid.html">tensor4d::nn::Sigmoid</a></div><div class="ttdoc">Sigmoid activation layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00222">nn_layers.h:222</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Sigmoid_html_a41e695161d280883a0002fe199efdf98"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Sigmoid.html#a41e695161d280883a0002fe199efdf98">tensor4d::nn::Sigmoid::backward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; backward(const Tensor&lt; T, 2 &gt; &amp;grad_output) override</div><div class="ttdoc">Backward pass (gradient computation).</div><div class="ttdef"><b>Definition</b> <a href="#l00231">nn_layers.h:231</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Sigmoid_html_a5bde4d78573f5ea7c6c5e4d624bda3fb"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Sigmoid.html#a5bde4d78573f5ea7c6c5e4d624bda3fb">tensor4d::nn::Sigmoid::forward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; forward(const Tensor&lt; T, 2 &gt; &amp;input) override</div><div class="ttdoc">Forward pass through the layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00226">nn_layers.h:226</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Sigmoid_html_a7a699853be0806bf11af345641f690fd"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Sigmoid.html#a7a699853be0806bf11af345641f690fd">tensor4d::nn::Sigmoid::Sigmoid</a></div><div class="ttdeci">Sigmoid()</div><div class="ttdef"><b>Definition</b> <a href="#l00224">nn_layers.h:224</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Sigmoid_html_aa8058b771d3d30a043d853026cfa06ea"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Sigmoid.html#aa8058b771d3d30a043d853026cfa06ea">tensor4d::nn::Sigmoid&lt; float &gt;::output_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; output_</div><div class="ttdef"><b>Definition</b> <a href="#l00244">nn_layers.h:244</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Softmax_html"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Softmax.html">tensor4d::nn::Softmax</a></div><div class="ttdoc">Softmax activation layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00451">nn_layers.h:451</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Softmax_html_a62cc4acc056fbd4f8a365637e1159a9e"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Softmax.html#a62cc4acc056fbd4f8a365637e1159a9e">tensor4d::nn::Softmax::forward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; forward(const Tensor&lt; T, 2 &gt; &amp;input) override</div><div class="ttdoc">Forward pass through the layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00455">nn_layers.h:455</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Softmax_html_aa600157567a716269b66a30f701921d5"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Softmax.html#aa600157567a716269b66a30f701921d5">tensor4d::nn::Softmax::Softmax</a></div><div class="ttdeci">Softmax()</div><div class="ttdef"><b>Definition</b> <a href="#l00453">nn_layers.h:453</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Softmax_html_abda8fce936f0e7f02d1779a120db27de"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Softmax.html#abda8fce936f0e7f02d1779a120db27de">tensor4d::nn::Softmax::backward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; backward(const Tensor&lt; T, 2 &gt; &amp;grad_output) override</div><div class="ttdoc">Backward pass (gradient computation).</div><div class="ttdef"><b>Definition</b> <a href="#l00461">nn_layers.h:461</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Softmax_html_ac1e254f8cb7a007b6ea0491024ceeead"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Softmax.html#ac1e254f8cb7a007b6ea0491024ceeead">tensor4d::nn::Softmax&lt; float &gt;::output_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; output_</div><div class="ttdef"><b>Definition</b> <a href="#l00495">nn_layers.h:495</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Tanh_html"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Tanh.html">tensor4d::nn::Tanh</a></div><div class="ttdoc">Tanh activation layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00251">nn_layers.h:251</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Tanh_html_a0e1b8dd43bc9599b91da6a6beba02e22"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Tanh.html#a0e1b8dd43bc9599b91da6a6beba02e22">tensor4d::nn::Tanh::Tanh</a></div><div class="ttdeci">Tanh()</div><div class="ttdef"><b>Definition</b> <a href="#l00253">nn_layers.h:253</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Tanh_html_a5bf1911c8b5865f171c90efbefbbe437"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Tanh.html#a5bf1911c8b5865f171c90efbefbbe437">tensor4d::nn::Tanh::forward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; forward(const Tensor&lt; T, 2 &gt; &amp;input) override</div><div class="ttdoc">Forward pass through the layer.</div><div class="ttdef"><b>Definition</b> <a href="#l00255">nn_layers.h:255</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Tanh_html_a657c2dab547dc900fda0a86818ea1172"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Tanh.html#a657c2dab547dc900fda0a86818ea1172">tensor4d::nn::Tanh&lt; float &gt;::output_</a></div><div class="ttdeci">Tensor&lt; float, 2 &gt; output_</div><div class="ttdef"><b>Definition</b> <a href="#l00274">nn_layers.h:274</a></div></div>
<div class="ttc" id="aclasstensor4d_1_1nn_1_1Tanh_html_a8211c640d2b962ecad7a5c29203e663c"><div class="ttname"><a href="classtensor4d_1_1nn_1_1Tanh.html#a8211c640d2b962ecad7a5c29203e663c">tensor4d::nn::Tanh::backward</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; backward(const Tensor&lt; T, 2 &gt; &amp;grad_output) override</div><div class="ttdoc">Backward pass (gradient computation).</div><div class="ttdef"><b>Definition</b> <a href="#l00260">nn_layers.h:260</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html"><div class="ttname"><a href="namespacetensor4d_1_1nn.html">tensor4d::nn</a></div><div class="ttdef"><b>Definition</b> <a href="#l00024">nn_layers.h:24</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_a227492f3cd887244ddf283565d4f5327"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#a227492f3cd887244ddf283565d4f5327">tensor4d::nn::Softmaxf</a></div><div class="ttdeci">Softmax&lt; float &gt; Softmaxf</div><div class="ttdef"><b>Definition</b> <a href="#l00511">nn_layers.h:511</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_a322020c52ad6d285a72c09a50ea4b467"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#a322020c52ad6d285a72c09a50ea4b467">tensor4d::nn::Tanhf</a></div><div class="ttdeci">Tanh&lt; float &gt; Tanhf</div><div class="ttdef"><b>Definition</b> <a href="#l00505">nn_layers.h:505</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_a3315ab248317bf7d71c75b77ccc5f985"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#a3315ab248317bf7d71c75b77ccc5f985">tensor4d::nn::Dropoutf</a></div><div class="ttdeci">Dropout&lt; float &gt; Dropoutf</div><div class="ttdef"><b>Definition</b> <a href="#l00507">nn_layers.h:507</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_a35558d36da40b43a93f6743d3aaf91eb"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#a35558d36da40b43a93f6743d3aaf91eb">tensor4d::nn::Softmaxd</a></div><div class="ttdeci">Softmax&lt; double &gt; Softmaxd</div><div class="ttdef"><b>Definition</b> <a href="#l00512">nn_layers.h:512</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_a4d5c82c731007a58002b1973150bb7e5"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#a4d5c82c731007a58002b1973150bb7e5">tensor4d::nn::Sigmoidd</a></div><div class="ttdeci">Sigmoid&lt; double &gt; Sigmoidd</div><div class="ttdef"><b>Definition</b> <a href="#l00504">nn_layers.h:504</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_a676f9d19eb47312f01a5c2ef0262e300"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#a676f9d19eb47312f01a5c2ef0262e300">tensor4d::nn::BatchNorm1dd</a></div><div class="ttdeci">BatchNorm1d&lt; double &gt; BatchNorm1dd</div><div class="ttdef"><b>Definition</b> <a href="#l00510">nn_layers.h:510</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_a684fa7cf2cac6ab7b8d62edc2b1da9d6"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#a684fa7cf2cac6ab7b8d62edc2b1da9d6">tensor4d::nn::Tanhd</a></div><div class="ttdeci">Tanh&lt; double &gt; Tanhd</div><div class="ttdef"><b>Definition</b> <a href="#l00506">nn_layers.h:506</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_a762e1706124db702d93be5bb8b18e629"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#a762e1706124db702d93be5bb8b18e629">tensor4d::nn::Lineard</a></div><div class="ttdeci">Linear&lt; double &gt; Lineard</div><div class="ttdef"><b>Definition</b> <a href="#l00500">nn_layers.h:500</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_a7682f7b06565fe530c833139aa5c52cd"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#a7682f7b06565fe530c833139aa5c52cd">tensor4d::nn::cross_entropy_loss</a></div><div class="ttdeci">T cross_entropy_loss(const Tensor&lt; T, 2 &gt; &amp;predictions, const Tensor&lt; T, 2 &gt; &amp;targets, T epsilon=T(1e-7))</div><div class="ttdoc">Compute cross-entropy loss between predictions and targets.</div><div class="ttdef"><b>Definition</b> <a href="#l00558">nn_layers.h:558</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_a834567aedd488a7451d76fec1bbba55e"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#a834567aedd488a7451d76fec1bbba55e">tensor4d::nn::update_linear_layer</a></div><div class="ttdeci">void update_linear_layer(Linear&lt; T &gt; &amp;layer, T lr)</div><div class="ttdoc">Update weights of a linear layer using SGD.</div><div class="ttdef"><b>Definition</b> <a href="#l00629">nn_layers.h:629</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_a83ad7bb2795465463d3684533657dc45"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#a83ad7bb2795465463d3684533657dc45">tensor4d::nn::Sigmoidf</a></div><div class="ttdeci">Sigmoid&lt; float &gt; Sigmoidf</div><div class="ttdef"><b>Definition</b> <a href="#l00503">nn_layers.h:503</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_aabc59d678ea2b307e86f063f32102470"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#aabc59d678ea2b307e86f063f32102470">tensor4d::nn::compute_accuracy</a></div><div class="ttdeci">T compute_accuracy(const Tensor&lt; T, 2 &gt; &amp;predictions, const std::vector&lt; uint8_t &gt; &amp;labels, size_t offset=0)</div><div class="ttdoc">Compute classification accuracy.</div><div class="ttdef"><b>Definition</b> <a href="#l00590">nn_layers.h:590</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_aaf2467899756d1b58455dff42f976904"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#aaf2467899756d1b58455dff42f976904">tensor4d::nn::Linearf</a></div><div class="ttdeci">Linear&lt; float &gt; Linearf</div><div class="ttdef"><b>Definition</b> <a href="#l00499">nn_layers.h:499</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_ab2f304e755860ad8c80d7c67cfe953b2"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#ab2f304e755860ad8c80d7c67cfe953b2">tensor4d::nn::softmax_jacobian_batch</a></div><div class="ttdeci">std::vector&lt; Tensor&lt; T, 2 &gt; &gt; softmax_jacobian_batch(const Tensor&lt; T, 2 &gt; &amp;softmax_output)</div><div class="ttdoc">Compute softmax Jacobian for batched input (batch_size x num_classes).</div><div class="ttdef"><b>Definition</b> <a href="#l00715">nn_layers.h:715</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_ac2a195a98fc946944abd82c47d997189"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#ac2a195a98fc946944abd82c47d997189">tensor4d::nn::label_to_onehot</a></div><div class="ttdeci">void label_to_onehot(uint8_t label, Tensor&lt; T, 2 &gt; &amp;onehot, size_t batch_idx, size_t num_classes)</div><div class="ttdoc">Convert label to one-hot encoded vector.</div><div class="ttdef"><b>Definition</b> <a href="#l00534">nn_layers.h:534</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_ac2f3327e85a4c471ec2f30634bde0928"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#ac2f3327e85a4c471ec2f30634bde0928">tensor4d::nn::Dropoutd</a></div><div class="ttdeci">Dropout&lt; double &gt; Dropoutd</div><div class="ttdef"><b>Definition</b> <a href="#l00508">nn_layers.h:508</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_ac63545216aad7e4be2c45c0b70509653"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#ac63545216aad7e4be2c45c0b70509653">tensor4d::nn::ReLUf</a></div><div class="ttdeci">ReLU&lt; float &gt; ReLUf</div><div class="ttdef"><b>Definition</b> <a href="#l00501">nn_layers.h:501</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_ad4dcbd0157fcb32c118ed38f1f40c58f"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#ad4dcbd0157fcb32c118ed38f1f40c58f">tensor4d::nn::ReLUd</a></div><div class="ttdeci">ReLU&lt; double &gt; ReLUd</div><div class="ttdef"><b>Definition</b> <a href="#l00502">nn_layers.h:502</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_add8e6f802c9d7c435a6c981f545a37f2"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#add8e6f802c9d7c435a6c981f545a37f2">tensor4d::nn::BatchNorm1df</a></div><div class="ttdeci">BatchNorm1d&lt; float &gt; BatchNorm1df</div><div class="ttdef"><b>Definition</b> <a href="#l00509">nn_layers.h:509</a></div></div>
<div class="ttc" id="anamespacetensor4d_1_1nn_html_af4ed0870610368776140c66a749f99ab"><div class="ttname"><a href="namespacetensor4d_1_1nn.html#af4ed0870610368776140c66a749f99ab">tensor4d::nn::softmax_jacobian</a></div><div class="ttdeci">Tensor&lt; T, 2 &gt; softmax_jacobian(const Tensor&lt; T, 1 &gt; &amp;softmax_output)</div><div class="ttdoc">Compute the Jacobian matrix of softmax for a single row.</div><div class="ttdef"><b>Definition</b> <a href="#l00662">nn_layers.h:662</a></div></div>
<div class="ttc" id="anamespacetensor4d_html"><div class="ttname"><a href="namespacetensor4d.html">tensor4d</a></div><div class="ttdef"><b>Definition</b> <a href="#l00023">nn_layers.h:23</a></div></div>
<div class="ttc" id="atensor_8h_html"><div class="ttname"><a href="tensor_8h.html">tensor.h</a></div><div class="ttdoc">High-performance multi-dimensional tensor library with GPU, BLAS, and autograd support.</div></div>
<div class="ttc" id="atensor_8h_html_a249e6ed21f6e3540a11b62433237175a"><div class="ttname"><a href="tensor_8h.html#a249e6ed21f6e3540a11b62433237175a">zeros</a></div><div class="ttdeci">Tensor&lt; T, N &gt; zeros(const TensorIndices&lt; N &gt; &amp;shape, bool use_gpu=true)</div><div class="ttdoc">Create a tensor filled with zeros (NumPy-compatible).</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l06971">tensor.h:6971</a></div></div>
<div class="ttc" id="atensor_8h_html_a37ee39b629ca93dc83259b6c8591d0d9"><div class="ttname"><a href="tensor_8h.html#a37ee39b629ca93dc83259b6c8591d0d9">ones</a></div><div class="ttdeci">Tensor&lt; T, N &gt; ones(const TensorIndices&lt; N &gt; &amp;shape, bool use_gpu=true)</div><div class="ttdoc">Create a tensor filled with ones (NumPy-compatible).</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l06986">tensor.h:6986</a></div></div>
<div class="ttc" id="atensor_8h_html_adf94b1b28dd9932dd278abaa3cb7a9ee"><div class="ttname"><a href="tensor_8h.html#adf94b1b28dd9932dd278abaa3cb7a9ee">row</a></div><div class="ttdeci">Tensor&lt; T, 1 &gt; row(const Tensor&lt; T, 2 &gt; &amp;matrix, size_t row_idx)</div><div class="ttdoc">Extract a single row from a 2D tensor.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l07481">tensor.h:7481</a></div></div>
<div class="ttc" id="atensor__types_8h_html"><div class="ttname"><a href="tensor__types_8h.html">tensor_types.h</a></div><div class="ttdoc">Type aliases for common Tensor specializations.</div></div>
</div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
</div><!-- container -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a href="dir_d44c64559bbebec7f509842c48db8b23.html">include</a></li><li class="navelem"><a href="nn__layers_8h.html">nn_layers.h</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.15.0 </li>
  </ul>
</div>
</body>
</html>
