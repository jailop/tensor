<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.15.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tensor Library: loss Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tensor Library
   </div>
   <div id="projectbrief">High-performance C++ tensor library with GPU, BLAS, autograd, and linear algebra support</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.15.0 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search/",'.html');
</script>
<script type="text/javascript">
$(function() { codefold.init(); });
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(function(){initNavTree('namespaceloss.html','',''); });
</script>
<div id="container">
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="headertitle"><div class="title">loss Namespace Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Loss functions for neural network training.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 id="header-func-members" class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:aff5180b0c38ddd16d08f058205c01ef7" id="r_aff5180b0c38ddd16d08f058205c01ef7"><td class="memTemplParams" colspan="2">template&lt;typename T, size_t N&gt; </td></tr>
<tr class="memitem:aff5180b0c38ddd16d08f058205c01ef7 template"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aff5180b0c38ddd16d08f058205c01ef7">mse_loss</a> (const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;predictions, const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;targets, const std::string &amp;reduction)</td></tr>
<tr class="memdesc:aff5180b0c38ddd16d08f058205c01ef7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Mean Squared Error (MSE) Loss.  <br /></td></tr>
<tr class="memitem:a75edef9d102d2ab811b15c50f13c4eee" id="r_a75edef9d102d2ab811b15c50f13c4eee"><td class="memTemplParams" colspan="2">template&lt;typename T, size_t N&gt; </td></tr>
<tr class="memitem:a75edef9d102d2ab811b15c50f13c4eee template"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classTensor.html">Tensor</a>&lt; T, 1 &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a75edef9d102d2ab811b15c50f13c4eee">cross_entropy_loss</a> (const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;, const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;, const std::string &amp;=&quot;mean&quot;)</td></tr>
<tr class="memdesc:a75edef9d102d2ab811b15c50f13c4eee"><td class="mdescLeft">&#160;</td><td class="mdescRight">Cross entropy loss for multi-class classification.  <br /></td></tr>
<tr class="memitem:a365059b00ea1e6ec1db86710cfcf3b31" id="r_a365059b00ea1e6ec1db86710cfcf3b31"><td class="memTemplParams" colspan="2">template&lt;typename T, size_t N&gt; </td></tr>
<tr class="memitem:a365059b00ea1e6ec1db86710cfcf3b31 template"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classTensor.html">Tensor</a>&lt; T, 1 &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a365059b00ea1e6ec1db86710cfcf3b31">binary_cross_entropy</a> (const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;, const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;, const std::string &amp;=&quot;mean&quot;)</td></tr>
<tr class="memdesc:a365059b00ea1e6ec1db86710cfcf3b31"><td class="mdescLeft">&#160;</td><td class="mdescRight">Binary cross entropy loss for binary classification.  <br /></td></tr>
<tr class="memitem:ab7fbd0200f34e3d12f66d20b681a948f" id="r_ab7fbd0200f34e3d12f66d20b681a948f"><td class="memTemplParams" colspan="2">template&lt;typename T, size_t N&gt; </td></tr>
<tr class="memitem:ab7fbd0200f34e3d12f66d20b681a948f template"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classTensor.html">Tensor</a>&lt; T, 1 &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ab7fbd0200f34e3d12f66d20b681a948f">l1_loss</a> (const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;, const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;, const std::string &amp;=&quot;mean&quot;)</td></tr>
<tr class="memdesc:ab7fbd0200f34e3d12f66d20b681a948f"><td class="mdescLeft">&#160;</td><td class="mdescRight">L1 loss (Mean Absolute Error).  <br /></td></tr>
<tr class="memitem:a2d2c1dacdb37d834f7dc10e85cd1a742" id="r_a2d2c1dacdb37d834f7dc10e85cd1a742"><td class="memTemplParams" colspan="2">template&lt;typename T, size_t N&gt; </td></tr>
<tr class="memitem:a2d2c1dacdb37d834f7dc10e85cd1a742 template"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classTensor.html">Tensor</a>&lt; T, 1 &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2d2c1dacdb37d834f7dc10e85cd1a742">smooth_l1_loss</a> (const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;, const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;, T=T(1), const std::string &amp;=&quot;mean&quot;)</td></tr>
<tr class="memdesc:a2d2c1dacdb37d834f7dc10e85cd1a742"><td class="mdescLeft">&#160;</td><td class="mdescRight">Smooth L1 loss (Huber loss).  <br /></td></tr>
</table>
<a name="details" id="details"></a><h2 id="header-details" class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Loss functions for neural network training. </p>
<p>Loss functions for training neural networks.</p>
<p>Provides differentiable loss functions with support for:</p><ul>
<li>Different reduction strategies (mean, sum, none)</li>
<li>Automatic gradient computation</li>
<li>Efficient GPU and BLAS implementations</li>
</ul>
<p>Provides common loss functions used in machine learning:</p><ul>
<li>Mean Squared Error (MSE)</li>
<li>Cross Entropy</li>
<li>Binary Cross Entropy</li>
<li>L1 Loss</li>
<li>Smooth L1 Loss</li>
</ul>
<p>All loss functions support automatic differentiation. </p>
</div><a name="doc-func-members" id="doc-func-members"></a><h2 id="header-doc-func-members" class="groupheader">Function Documentation</h2>
<a id="a365059b00ea1e6ec1db86710cfcf3b31" name="a365059b00ea1e6ec1db86710cfcf3b31"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a365059b00ea1e6ec1db86710cfcf3b31">&#9670;&#160;</a></span>binary_cross_entropy()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T, size_t N&gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classTensor.html">Tensor</a>&lt; T, 1 &gt; loss::binary_cross_entropy </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>predictions</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>targets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;</td>          <td class="paramname"><span class="paramname"><em>reduction</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Binary cross entropy loss for binary classification. </p>
<p>Binary Cross Entropy Loss. L = -sum(targets * log(predictions) + (1-targets) * log(1-predictions))</p>
<p>Used for binary classification tasks.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">predictions</td><td>Predicted probabilities (after sigmoid) </td></tr>
    <tr><td class="paramname">targets</td><td>True binary labels (0 or 1) </td></tr>
    <tr><td class="paramname">reduction</td><td>"mean", "sum", or "none" (default: "mean") </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Scalar loss tensor with autograd support </dd></dl>

<p class="definition">Definition at line <a class="el" href="loss__functions_8h_source.html#l00212">212</a> of file <a class="el" href="loss__functions_8h_source.html">loss_functions.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  214</span>                                                               {</div>
<div class="line"><span class="lineno">  215</span>    <span class="keywordflow">if</span> (predictions.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>() != targets.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>()) {</div>
<div class="line"><span class="lineno">  216</span>        <span class="keywordflow">return</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 1&gt;</a>({1}, predictions.<a class="code hl_function" href="classTensor.html#a83ae42925e21d2871d755407d7505c10">uses_gpu</a>(), <span class="keyword">false</span>);</div>
<div class="line"><span class="lineno">  217</span>    }</div>
<div class="line"><span class="lineno">  218</span>    </div>
<div class="line"><span class="lineno">  219</span>    <span class="keywordtype">size_t</span> total = predictions.<a class="code hl_function" href="classTensor.html#aad6aeb4859940befd0c46f0b99388bd7">total_size</a>();</div>
<div class="line"><span class="lineno">  220</span>    <span class="keywordtype">bool</span> track_grad = predictions.<a class="code hl_function" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8">requires_grad</a>();</div>
<div class="line"><span class="lineno">  221</span>    </div>
<div class="line"><span class="lineno">  222</span>    T total_loss = T(0);</div>
<div class="line"><span class="lineno">  223</span>    T epsilon = T(1e-7);  <span class="comment">// For numerical stability</span></div>
<div class="line"><span class="lineno">  224</span>    </div>
<div class="line"><span class="lineno">  225</span>    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; total; ++i) {</div>
<div class="line"><span class="lineno">  226</span>        T pred = std::max(std::min(predictions.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i], T(1) - epsilon), epsilon);</div>
<div class="line"><span class="lineno">  227</span>        T target = targets.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i];</div>
<div class="line"><span class="lineno">  228</span>        </div>
<div class="line"><span class="lineno">  229</span>        total_loss += -(target * std::log(pred) + (T(1) - target) * std::log(T(1) - pred));</div>
<div class="line"><span class="lineno">  230</span>    }</div>
<div class="line"><span class="lineno">  231</span>    </div>
<div class="line"><span class="lineno">  232</span>    <a class="code hl_typedef" href="tensor_8h.html#ab58b5b2b06188fb918bfa78bdfce068b">TensorIndices&lt;1&gt;</a> loss_shape = {1};</div>
<div class="line"><span class="lineno">  233</span>    Tensor&lt;T, 1&gt; loss(loss_shape, predictions.<a class="code hl_function" href="classTensor.html#a83ae42925e21d2871d755407d7505c10">uses_gpu</a>(), track_grad);</div>
<div class="line"><span class="lineno">  234</span>    loss.is_leaf_ = <span class="keyword">false</span>;</div>
<div class="line"><span class="lineno">  235</span>    </div>
<div class="line"><span class="lineno">  236</span>    <span class="keywordflow">if</span> (reduction == <span class="stringliteral">&quot;mean&quot;</span>) {</div>
<div class="line"><span class="lineno">  237</span>        loss[{0}] = total_loss / <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(total);</div>
<div class="line"><span class="lineno">  238</span>    } <span class="keywordflow">else</span> <span class="keywordflow">if</span> (reduction == <span class="stringliteral">&quot;sum&quot;</span>) {</div>
<div class="line"><span class="lineno">  239</span>        loss[{0}] = total_loss;</div>
<div class="line"><span class="lineno">  240</span>    }</div>
<div class="line"><span class="lineno">  241</span>    </div>
<div class="line"><span class="lineno">  242</span>    <span class="comment">// Setup backward pass</span></div>
<div class="line"><span class="lineno">  243</span>    <span class="keywordflow">if</span> (track_grad) {</div>
<div class="line"><span class="lineno">  244</span>        Tensor&lt;T, N&gt; pred_copy = predictions.<a class="code hl_function" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285">detach</a>();</div>
<div class="line"><span class="lineno">  245</span>        Tensor&lt;T, N&gt; targ_copy = targets.<a class="code hl_function" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285">detach</a>();</div>
<div class="line"><span class="lineno">  246</span>        Tensor&lt;T, N&gt;* pred_ptr = <span class="keyword">const_cast&lt;</span>Tensor&lt;T, N&gt;*<span class="keyword">&gt;</span>(&amp;predictions);</div>
<div class="line"><span class="lineno">  247</span>        T scale = (reduction == <span class="stringliteral">&quot;mean&quot;</span>) ? T(1) / <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(total) : T(1);</div>
<div class="line"><span class="lineno">  248</span>        </div>
<div class="line"><span class="lineno">  249</span>        loss.register_backward([pred_ptr, pred_copy, targ_copy, scale, total, epsilon]</div>
<div class="line"><span class="lineno">  250</span>                              (<span class="keyword">const</span> Tensor&lt;T, 1&gt;&amp; grad) {</div>
<div class="line"><span class="lineno">  251</span>            <span class="comment">// Gradient of BCE: -(y/x - (1-y)/(1-x))</span></div>
<div class="line"><span class="lineno">  252</span>            <span class="keywordflow">if</span> (pred_ptr-&gt;<a class="code hl_function" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8">requires_grad</a>()) {</div>
<div class="line"><span class="lineno">  253</span>                <span class="keywordflow">if</span> (!pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>) {</div>
<div class="line"><span class="lineno">  254</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a> = std::make_unique&lt;Tensor&lt;T, N&gt;&gt;(pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a43fadce1ce3dd9ddb47c8b82168edb52">dims_</a>, </div>
<div class="line"><span class="lineno">  255</span>                                                                      pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#aa3214ec02474d17494e1eb95d5006262">use_gpu_</a>, <span class="keyword">false</span>);</div>
<div class="line"><span class="lineno">  256</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>-&gt;fill(T(0));</div>
<div class="line"><span class="lineno">  257</span>                }</div>
<div class="line"><span class="lineno">  258</span>                </div>
<div class="line"><span class="lineno">  259</span>                <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; total; ++i) {</div>
<div class="line"><span class="lineno">  260</span>                    T pred = std::max(std::min(pred_copy.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i], T(1) - epsilon), epsilon);</div>
<div class="line"><span class="lineno">  261</span>                    T target = targ_copy.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i];</div>
<div class="line"><span class="lineno">  262</span>                    </div>
<div class="line"><span class="lineno">  263</span>                    T grad_val = grad[{0}] * scale * (-(target / pred) + (T(1) - target) / (T(1) - pred));</div>
<div class="line"><span class="lineno">  264</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>-&gt;data()[i] += grad_val;</div>
<div class="line"><span class="lineno">  265</span>                }</div>
<div class="line"><span class="lineno">  266</span>                </div>
<div class="line"><span class="lineno">  267</span>                <span class="keywordflow">if</span> (!pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#abf7257d130e6bc46f6d2cd7de4ea2909">is_leaf_</a> &amp;&amp; !pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a89d50a2ba15e719d4bf025178150d308">backward_funcs_</a>.empty()) {</div>
<div class="line"><span class="lineno">  268</span>                    <span class="keywordflow">for</span> (<span class="keyword">auto</span>&amp; func : pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a89d50a2ba15e719d4bf025178150d308">backward_funcs_</a>) {</div>
<div class="line"><span class="lineno">  269</span>                        func(*pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>);</div>
<div class="line"><span class="lineno">  270</span>                    }</div>
<div class="line"><span class="lineno">  271</span>                }</div>
<div class="line"><span class="lineno">  272</span>            }</div>
<div class="line"><span class="lineno">  273</span>        });</div>
<div class="line"><span class="lineno">  274</span>    }</div>
<div class="line"><span class="lineno">  275</span>    </div>
<div class="line"><span class="lineno">  276</span>    <span class="keywordflow">return</span> loss;</div>
<div class="line"><span class="lineno">  277</span>}</div>
<div class="ttc" id="aclassTensor_html"><div class="ttname"><a href="classTensor.html">Tensor</a></div><div class="ttdoc">Forward declaration for autograd.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00444">tensor.h:444</a></div></div>
<div class="ttc" id="aclassTensor_html_a03d314303958ffc5644796d8025cd012"><div class="ttname"><a href="classTensor.html#a03d314303958ffc5644796d8025cd012">Tensor::dims</a></div><div class="ttdeci">TensorIndices&lt; N &gt; dims() const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00644">tensor.h:644</a></div></div>
<div class="ttc" id="aclassTensor_html_a07bf856a863505d2960b0bfad9c3b3f4"><div class="ttname"><a href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">Tensor::data</a></div><div class="ttdeci">const T * data() const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00523">tensor.h:523</a></div></div>
<div class="ttc" id="aclassTensor_html_a43fadce1ce3dd9ddb47c8b82168edb52"><div class="ttname"><a href="classTensor.html#a43fadce1ce3dd9ddb47c8b82168edb52">Tensor::dims_</a></div><div class="ttdeci">TensorIndices&lt; N &gt; dims_</div><div class="ttdoc">Dimensions of the tensor.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00447">tensor.h:447</a></div></div>
<div class="ttc" id="aclassTensor_html_a4d61cfe1428b0302cd1683dc1e6fd733"><div class="ttname"><a href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">Tensor::grad_</a></div><div class="ttdeci">std::unique_ptr&lt; Tensor&lt; T, N &gt; &gt; grad_</div><div class="ttdoc">Gradient tensor.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00454">tensor.h:454</a></div></div>
<div class="ttc" id="aclassTensor_html_a83ae42925e21d2871d755407d7505c10"><div class="ttname"><a href="classTensor.html#a83ae42925e21d2871d755407d7505c10">Tensor::uses_gpu</a></div><div class="ttdeci">bool uses_gpu() const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00656">tensor.h:656</a></div></div>
<div class="ttc" id="aclassTensor_html_a89d50a2ba15e719d4bf025178150d308"><div class="ttname"><a href="classTensor.html#a89d50a2ba15e719d4bf025178150d308">Tensor::backward_funcs_</a></div><div class="ttdeci">std::vector&lt; BackwardFunc&lt; T, N &gt; &gt; backward_funcs_</div><div class="ttdoc">Functions to compute gradients.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00455">tensor.h:455</a></div></div>
<div class="ttc" id="aclassTensor_html_a947326cf96b1bf3423b40861e1213ab8"><div class="ttname"><a href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8">Tensor::requires_grad</a></div><div class="ttdeci">bool requires_grad() const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00728">tensor.h:728</a></div></div>
<div class="ttc" id="aclassTensor_html_aa3214ec02474d17494e1eb95d5006262"><div class="ttname"><a href="classTensor.html#aa3214ec02474d17494e1eb95d5006262">Tensor::use_gpu_</a></div><div class="ttdeci">bool use_gpu_</div><div class="ttdoc">Flag to indicate if GPU should be used.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00449">tensor.h:449</a></div></div>
<div class="ttc" id="aclassTensor_html_aad6aeb4859940befd0c46f0b99388bd7"><div class="ttname"><a href="classTensor.html#aad6aeb4859940befd0c46f0b99388bd7">Tensor::total_size</a></div><div class="ttdeci">size_t total_size() const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00511">tensor.h:511</a></div></div>
<div class="ttc" id="aclassTensor_html_abf7257d130e6bc46f6d2cd7de4ea2909"><div class="ttname"><a href="classTensor.html#abf7257d130e6bc46f6d2cd7de4ea2909">Tensor::is_leaf_</a></div><div class="ttdeci">bool is_leaf_</div><div class="ttdoc">Whether this is a leaf node in the graph.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00456">tensor.h:456</a></div></div>
<div class="ttc" id="aclassTensor_html_ae5b52299c11f3ba81e781cb008542285"><div class="ttname"><a href="classTensor.html#ae5b52299c11f3ba81e781cb008542285">Tensor::detach</a></div><div class="ttdeci">Tensor&lt; T, N &gt; detach() const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00822">tensor.h:822</a></div></div>
<div class="ttc" id="atensor_8h_html_ab58b5b2b06188fb918bfa78bdfce068b"><div class="ttname"><a href="tensor_8h.html#ab58b5b2b06188fb918bfa78bdfce068b">TensorIndices</a></div><div class="ttdeci">std::array&lt; size_t, N &gt; TensorIndices</div><div class="ttdoc">Type alias for tensor indices/coordinates.</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00310">tensor.h:310</a></div></div>
</div><!-- fragment --><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespaceloss_a365059b00ea1e6ec1db86710cfcf3b31_cgraph.png" border="0" usemap="#anamespaceloss_a365059b00ea1e6ec1db86710cfcf3b31_cgraph" loading="lazy" alt=""/></div>
<map name="anamespaceloss_a365059b00ea1e6ec1db86710cfcf3b31_cgraph" id="anamespaceloss_a365059b00ea1e6ec1db86710cfcf3b31_cgraph">
<area shape="rect" title="Binary cross entropy loss for binary classification." alt="" coords="5,124,133,167"/>
<area shape="rect" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4" title=" " alt="" coords="207,5,300,32"/>
<area shape="poly" title=" " alt="" coords="87,122,127,81,179,42,192,35,194,40,182,46,131,85,91,125"/>
<area shape="rect" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285" title=" " alt="" coords="200,56,308,83"/>
<area shape="poly" title=" " alt="" coords="114,121,180,92,196,86,198,91,182,97,117,126"/>
<area shape="rect" href="classTensor.html#a03d314303958ffc5644796d8025cd012" title=" " alt="" coords="206,107,302,133"/>
<area shape="poly" title=" " alt="" coords="133,134,190,126,191,131,133,139"/>
<area shape="rect" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8" title=" " alt="" coords="181,157,327,184"/>
<area shape="poly" title=" " alt="" coords="133,151,165,156,165,161,133,157"/>
<area shape="rect" href="classTensor.html#aad6aeb4859940befd0c46f0b99388bd7" title=" " alt="" coords="192,208,316,235"/>
<area shape="poly" title=" " alt="" coords="117,165,182,194,198,200,196,205,180,198,114,170"/>
<area shape="rect" href="classTensor.html#a83ae42925e21d2871d755407d7505c10" title=" " alt="" coords="192,259,316,285"/>
<area shape="poly" title=" " alt="" coords="91,165,131,205,182,244,192,250,190,254,179,249,127,209,87,169"/>
</map>
</div>

</div>
</div>
<a id="a75edef9d102d2ab811b15c50f13c4eee" name="a75edef9d102d2ab811b15c50f13c4eee"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a75edef9d102d2ab811b15c50f13c4eee">&#9670;&#160;</a></span>cross_entropy_loss()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T, size_t N&gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classTensor.html">Tensor</a>&lt; T, 1 &gt; loss::cross_entropy_loss </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>logits</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>targets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;</td>          <td class="paramname"><span class="paramname"><em>reduction</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Cross entropy loss for multi-class classification. </p>
<p>Cross Entropy Loss with logits. L = -sum(targets * log(softmax(predictions)))</p>
<p>Numerically stable implementation that combines softmax and log. Commonly used for multi-class classification.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">logits</td><td>Raw prediction scores (before softmax) </td></tr>
    <tr><td class="paramname">targets</td><td>Target class indices or one-hot encoded targets </td></tr>
    <tr><td class="paramname">reduction</td><td>"mean" or "sum" (default: "mean") </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Scalar loss tensor with autograd support </dd></dl>

<p class="definition">Definition at line <a class="el" href="loss__functions_8h_source.html#l00159">159</a> of file <a class="el" href="loss__functions_8h_source.html">loss_functions.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  161</span>                                                             {</div>
<div class="line"><span class="lineno">  162</span>    <span class="comment">// For now, implement for 2D tensors (batch_size, num_classes)</span></div>
<div class="line"><span class="lineno">  163</span>    <span class="keyword">static_assert</span>(N == 2, <span class="stringliteral">&quot;Cross entropy currently supports 2D tensors&quot;</span>);</div>
<div class="line"><span class="lineno">  164</span>    </div>
<div class="line"><span class="lineno">  165</span>    <span class="keywordtype">size_t</span> batch_size = logits.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>()[0];</div>
<div class="line"><span class="lineno">  166</span>    <span class="keywordtype">size_t</span> num_classes = logits.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>()[1];</div>
<div class="line"><span class="lineno">  167</span>    </div>
<div class="line"><span class="lineno">  168</span>    <span class="keywordtype">bool</span> track_grad = logits.<a class="code hl_function" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8">requires_grad</a>();</div>
<div class="line"><span class="lineno">  169</span>    </div>
<div class="line"><span class="lineno">  170</span>    <span class="comment">// Compute log_softmax for numerical stability</span></div>
<div class="line"><span class="lineno">  171</span>    <span class="keyword">auto</span> log_probs = logits.<a class="code hl_function" href="classTensor.html#a834720ab65874a6c5503f7670c4f22fe">log_softmax</a>(-1);</div>
<div class="line"><span class="lineno">  172</span>    </div>
<div class="line"><span class="lineno">  173</span>    <span class="comment">// Compute negative log likelihood</span></div>
<div class="line"><span class="lineno">  174</span>    <a class="code hl_typedef" href="tensor_8h.html#ab58b5b2b06188fb918bfa78bdfce068b">TensorIndices&lt;1&gt;</a> loss_shape = {1};</div>
<div class="line"><span class="lineno">  175</span>    <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 1&gt;</a> <a class="code hl_namespace" href="namespaceloss.html">loss</a>(loss_shape, logits.<a class="code hl_function" href="classTensor.html#a83ae42925e21d2871d755407d7505c10">uses_gpu</a>(), track_grad);</div>
<div class="line"><span class="lineno">  176</span>    <a class="code hl_namespace" href="namespaceloss.html">loss</a>.is_leaf_ = <span class="keyword">false</span>;</div>
<div class="line"><span class="lineno">  177</span>    </div>
<div class="line"><span class="lineno">  178</span>    T total_loss = T(0);</div>
<div class="line"><span class="lineno">  179</span>    </div>
<div class="line"><span class="lineno">  180</span>    <span class="comment">// Assuming targets are class indices (not one-hot)</span></div>
<div class="line"><span class="lineno">  181</span>    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; batch_size; ++i) {</div>
<div class="line"><span class="lineno">  182</span>        <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> j = 0; j &lt; num_classes; ++j) {</div>
<div class="line"><span class="lineno">  183</span>            <span class="comment">// If targets are one-hot, multiply by target[i,j]</span></div>
<div class="line"><span class="lineno">  184</span>            <span class="comment">// If targets are indices, only use the correct class</span></div>
<div class="line"><span class="lineno">  185</span>            total_loss += -targets[{i, j}] * log_probs[{i, j}];</div>
<div class="line"><span class="lineno">  186</span>        }</div>
<div class="line"><span class="lineno">  187</span>    }</div>
<div class="line"><span class="lineno">  188</span>    </div>
<div class="line"><span class="lineno">  189</span>    <span class="keywordflow">if</span> (reduction == <span class="stringliteral">&quot;mean&quot;</span>) {</div>
<div class="line"><span class="lineno">  190</span>        loss[{0}] = total_loss / <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(batch_size);</div>
<div class="line"><span class="lineno">  191</span>    } <span class="keywordflow">else</span> {</div>
<div class="line"><span class="lineno">  192</span>        loss[{0}] = total_loss;</div>
<div class="line"><span class="lineno">  193</span>    }</div>
<div class="line"><span class="lineno">  194</span>    </div>
<div class="line"><span class="lineno">  195</span>    <span class="comment">// Backward pass is handled by log_softmax</span></div>
<div class="line"><span class="lineno">  196</span>    </div>
<div class="line"><span class="lineno">  197</span>    <span class="keywordflow">return</span> loss;</div>
<div class="line"><span class="lineno">  198</span>}</div>
<div class="ttc" id="aclassTensor_html_a834720ab65874a6c5503f7670c4f22fe"><div class="ttname"><a href="classTensor.html#a834720ab65874a6c5503f7670c4f22fe">Tensor::log_softmax</a></div><div class="ttdeci">Tensor&lt; T, N &gt; log_softmax(int axis=-1) const</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l03831">tensor.h:3831</a></div></div>
<div class="ttc" id="anamespaceloss_html"><div class="ttname"><a href="namespaceloss.html">loss</a></div><div class="ttdoc">Loss functions for neural network training.</div></div>
</div><!-- fragment --><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespaceloss_a75edef9d102d2ab811b15c50f13c4eee_cgraph.png" border="0" usemap="#anamespaceloss_a75edef9d102d2ab811b15c50f13c4eee_cgraph" loading="lazy" alt=""/></div>
<map name="anamespaceloss_a75edef9d102d2ab811b15c50f13c4eee_cgraph" id="anamespaceloss_a75edef9d102d2ab811b15c50f13c4eee_cgraph">
<area shape="rect" title="Cross entropy loss for multi&#45;class classification." alt="" coords="5,81,172,108"/>
<area shape="rect" href="classTensor.html#a03d314303958ffc5644796d8025cd012" title=" " alt="" coords="245,5,341,32"/>
<area shape="poly" title=" " alt="" coords="123,78,219,42,236,35,238,40,221,46,125,83"/>
<area shape="rect" href="classTensor.html#a834720ab65874a6c5503f7670c4f22fe" title=" " alt="" coords="223,56,362,83"/>
<area shape="poly" title=" " alt="" coords="172,82,208,77,208,82,172,87"/>
<area shape="rect" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8" title=" " alt="" coords="220,107,366,133"/>
<area shape="poly" title=" " alt="" coords="172,102,205,106,204,112,172,108"/>
<area shape="rect" href="classTensor.html#a83ae42925e21d2871d755407d7505c10" title=" " alt="" coords="231,157,355,184"/>
<area shape="poly" title=" " alt="" coords="125,106,221,143,238,149,236,154,219,148,123,111"/>
</map>
</div>

</div>
</div>
<a id="ab7fbd0200f34e3d12f66d20b681a948f" name="ab7fbd0200f34e3d12f66d20b681a948f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab7fbd0200f34e3d12f66d20b681a948f">&#9670;&#160;</a></span>l1_loss()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T, size_t N&gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classTensor.html">Tensor</a>&lt; T, 1 &gt; loss::l1_loss </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>predictions</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>targets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;</td>          <td class="paramname"><span class="paramname"><em>reduction</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>L1 loss (Mean Absolute Error). </p>
<p>L1 Loss (Mean Absolute Error). L = (1/n) * sum(|predictions - targets|)</p>
<p>More robust to outliers than MSE.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">predictions</td><td>Predicted values </td></tr>
    <tr><td class="paramname">targets</td><td>True target values </td></tr>
    <tr><td class="paramname">reduction</td><td>"mean", "sum", or "none" (default: "mean") </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Scalar loss tensor with autograd support </dd></dl>

<p class="definition">Definition at line <a class="el" href="loss__functions_8h_source.html#l00291">291</a> of file <a class="el" href="loss__functions_8h_source.html">loss_functions.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  292</span>                                                 {</div>
<div class="line"><span class="lineno">  293</span>    <span class="keywordflow">if</span> (predictions.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>() != targets.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>()) {</div>
<div class="line"><span class="lineno">  294</span>        <span class="keywordflow">return</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 1&gt;</a>({1}, predictions.<a class="code hl_function" href="classTensor.html#a83ae42925e21d2871d755407d7505c10">uses_gpu</a>(), <span class="keyword">false</span>);</div>
<div class="line"><span class="lineno">  295</span>    }</div>
<div class="line"><span class="lineno">  296</span>    </div>
<div class="line"><span class="lineno">  297</span>    <span class="keywordtype">size_t</span> total = predictions.<a class="code hl_function" href="classTensor.html#aad6aeb4859940befd0c46f0b99388bd7">total_size</a>();</div>
<div class="line"><span class="lineno">  298</span>    <span class="keywordtype">bool</span> track_grad = predictions.<a class="code hl_function" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8">requires_grad</a>();</div>
<div class="line"><span class="lineno">  299</span>    </div>
<div class="line"><span class="lineno">  300</span>    T total_loss = T(0);</div>
<div class="line"><span class="lineno">  301</span>    </div>
<div class="line"><span class="lineno">  302</span>    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; total; ++i) {</div>
<div class="line"><span class="lineno">  303</span>        total_loss += std::abs(predictions.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i] - targets.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i]);</div>
<div class="line"><span class="lineno">  304</span>    }</div>
<div class="line"><span class="lineno">  305</span>    </div>
<div class="line"><span class="lineno">  306</span>    <a class="code hl_typedef" href="tensor_8h.html#ab58b5b2b06188fb918bfa78bdfce068b">TensorIndices&lt;1&gt;</a> loss_shape = {1};</div>
<div class="line"><span class="lineno">  307</span>    Tensor&lt;T, 1&gt; loss(loss_shape, predictions.<a class="code hl_function" href="classTensor.html#a83ae42925e21d2871d755407d7505c10">uses_gpu</a>(), track_grad);</div>
<div class="line"><span class="lineno">  308</span>    loss.is_leaf_ = <span class="keyword">false</span>;</div>
<div class="line"><span class="lineno">  309</span>    </div>
<div class="line"><span class="lineno">  310</span>    <span class="keywordflow">if</span> (reduction == <span class="stringliteral">&quot;mean&quot;</span>) {</div>
<div class="line"><span class="lineno">  311</span>        loss[{0}] = total_loss / <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(total);</div>
<div class="line"><span class="lineno">  312</span>    } <span class="keywordflow">else</span> <span class="keywordflow">if</span> (reduction == <span class="stringliteral">&quot;sum&quot;</span>) {</div>
<div class="line"><span class="lineno">  313</span>        loss[{0}] = total_loss;</div>
<div class="line"><span class="lineno">  314</span>    }</div>
<div class="line"><span class="lineno">  315</span>    </div>
<div class="line"><span class="lineno">  316</span>    <span class="comment">// Setup backward pass</span></div>
<div class="line"><span class="lineno">  317</span>    <span class="keywordflow">if</span> (track_grad) {</div>
<div class="line"><span class="lineno">  318</span>        Tensor&lt;T, N&gt; pred_copy = predictions.<a class="code hl_function" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285">detach</a>();</div>
<div class="line"><span class="lineno">  319</span>        Tensor&lt;T, N&gt; targ_copy = targets.<a class="code hl_function" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285">detach</a>();</div>
<div class="line"><span class="lineno">  320</span>        Tensor&lt;T, N&gt;* pred_ptr = <span class="keyword">const_cast&lt;</span>Tensor&lt;T, N&gt;*<span class="keyword">&gt;</span>(&amp;predictions);</div>
<div class="line"><span class="lineno">  321</span>        T scale = (reduction == <span class="stringliteral">&quot;mean&quot;</span>) ? T(1) / <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(total) : T(1);</div>
<div class="line"><span class="lineno">  322</span>        </div>
<div class="line"><span class="lineno">  323</span>        loss.register_backward([pred_ptr, pred_copy, targ_copy, scale, total]</div>
<div class="line"><span class="lineno">  324</span>                              (<span class="keyword">const</span> Tensor&lt;T, 1&gt;&amp; grad) {</div>
<div class="line"><span class="lineno">  325</span>            <span class="comment">// Gradient of L1: sign(x - y)</span></div>
<div class="line"><span class="lineno">  326</span>            <span class="keywordflow">if</span> (pred_ptr-&gt;<a class="code hl_function" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8">requires_grad</a>()) {</div>
<div class="line"><span class="lineno">  327</span>                <span class="keywordflow">if</span> (!pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>) {</div>
<div class="line"><span class="lineno">  328</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a> = std::make_unique&lt;Tensor&lt;T, N&gt;&gt;(pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a43fadce1ce3dd9ddb47c8b82168edb52">dims_</a>, </div>
<div class="line"><span class="lineno">  329</span>                                                                      pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#aa3214ec02474d17494e1eb95d5006262">use_gpu_</a>, <span class="keyword">false</span>);</div>
<div class="line"><span class="lineno">  330</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>-&gt;fill(T(0));</div>
<div class="line"><span class="lineno">  331</span>                }</div>
<div class="line"><span class="lineno">  332</span>                </div>
<div class="line"><span class="lineno">  333</span>                <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; total; ++i) {</div>
<div class="line"><span class="lineno">  334</span>                    T diff = pred_copy.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i] - targ_copy.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i];</div>
<div class="line"><span class="lineno">  335</span>                    T sign = (diff &gt; T(0)) ? T(1) : ((diff &lt; T(0)) ? T(-1) : T(0));</div>
<div class="line"><span class="lineno">  336</span>                    </div>
<div class="line"><span class="lineno">  337</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>-&gt;data()[i] += grad[{0}] * scale * sign;</div>
<div class="line"><span class="lineno">  338</span>                }</div>
<div class="line"><span class="lineno">  339</span>                </div>
<div class="line"><span class="lineno">  340</span>                <span class="keywordflow">if</span> (!pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#abf7257d130e6bc46f6d2cd7de4ea2909">is_leaf_</a> &amp;&amp; !pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a89d50a2ba15e719d4bf025178150d308">backward_funcs_</a>.empty()) {</div>
<div class="line"><span class="lineno">  341</span>                    <span class="keywordflow">for</span> (<span class="keyword">auto</span>&amp; func : pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a89d50a2ba15e719d4bf025178150d308">backward_funcs_</a>) {</div>
<div class="line"><span class="lineno">  342</span>                        func(*pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>);</div>
<div class="line"><span class="lineno">  343</span>                    }</div>
<div class="line"><span class="lineno">  344</span>                }</div>
<div class="line"><span class="lineno">  345</span>            }</div>
<div class="line"><span class="lineno">  346</span>        });</div>
<div class="line"><span class="lineno">  347</span>    }</div>
<div class="line"><span class="lineno">  348</span>    </div>
<div class="line"><span class="lineno">  349</span>    <span class="keywordflow">return</span> loss;</div>
<div class="line"><span class="lineno">  350</span>}</div>
</div><!-- fragment --><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespaceloss_ab7fbd0200f34e3d12f66d20b681a948f_cgraph.png" border="0" usemap="#anamespaceloss_ab7fbd0200f34e3d12f66d20b681a948f_cgraph" loading="lazy" alt=""/></div>
<map name="anamespaceloss_ab7fbd0200f34e3d12f66d20b681a948f_cgraph" id="anamespaceloss_ab7fbd0200f34e3d12f66d20b681a948f_cgraph">
<area shape="rect" title="L1 loss (Mean Absolute Error)." alt="" coords="5,132,100,159"/>
<area shape="rect" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4" title=" " alt="" coords="174,5,268,32"/>
<area shape="poly" title=" " alt="" coords="61,130,95,87,119,63,146,42,159,35,161,39,149,46,123,67,99,91,65,133"/>
<area shape="rect" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285" title=" " alt="" coords="167,56,274,83"/>
<area shape="poly" title=" " alt="" coords="76,129,147,92,161,86,163,91,149,97,78,134"/>
<area shape="rect" href="classTensor.html#a03d314303958ffc5644796d8025cd012" title=" " alt="" coords="173,107,269,133"/>
<area shape="poly" title=" " alt="" coords="100,136,157,127,158,132,100,141"/>
<area shape="rect" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8" title=" " alt="" coords="148,157,294,184"/>
<area shape="poly" title=" " alt="" coords="100,150,133,155,132,160,100,155"/>
<area shape="rect" href="classTensor.html#aad6aeb4859940befd0c46f0b99388bd7" title=" " alt="" coords="159,208,283,235"/>
<area shape="poly" title=" " alt="" coords="78,157,149,194,163,200,161,205,147,198,76,162"/>
<area shape="rect" href="classTensor.html#a83ae42925e21d2871d755407d7505c10" title=" " alt="" coords="159,259,283,285"/>
<area shape="poly" title=" " alt="" coords="65,157,99,200,123,224,149,244,157,249,155,254,146,249,119,228,95,204,61,161"/>
</map>
</div>

</div>
</div>
<a id="aff5180b0c38ddd16d08f058205c01ef7" name="aff5180b0c38ddd16d08f058205c01ef7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aff5180b0c38ddd16d08f058205c01ef7">&#9670;&#160;</a></span>mse_loss()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T, size_t N&gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; loss::mse_loss </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>predictions</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>targets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;</td>          <td class="paramname"><span class="paramname"><em>reduction</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Mean Squared Error (MSE) Loss. </p>
<p>Mean squared error loss.</p>
<p>Computes: L = (1/n) * sum((predictions - targets)^2)</p>
<p>Commonly used for regression tasks. The gradient is smooth and proportional to the error magnitude, making it well-suited for problems where large errors should be heavily penalized.</p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">T</td><td>Data type (float, double) </td></tr>
    <tr><td class="paramname">N</td><td>Number of dimensions </td></tr>
  </table>
  </dd>
</dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">predictions</td><td>Predicted values from the model </td></tr>
    <tr><td class="paramname">targets</td><td>True target values </td></tr>
    <tr><td class="paramname">reduction</td><td>Reduction strategy: "mean" (default), "sum", or "none" </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Loss tensor with autograd support, or zero tensor if shapes don't match</dd></dl>
<h1 class="doxsection"><a class="anchor" id="example_mse"></a>
Example</h1>
<div class="fragment"><div class="line"><span class="comment">// Regression example</span></div>
<div class="line"><a class="code hl_class" href="classTensor.html">Tensor&lt;float, 2&gt;</a> predictions({32, 10}, <span class="keyword">true</span>, <span class="keyword">true</span>);  <span class="comment">// batch_size=32, output_dim=10</span></div>
<div class="line"><a class="code hl_class" href="classTensor.html">Tensor&lt;float, 2&gt;</a> targets({32, 10});</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Compute MSE loss</span></div>
<div class="line"><span class="keyword">auto</span> loss = <a class="code hl_function" href="#aff5180b0c38ddd16d08f058205c01ef7">loss::mse_loss</a>(predictions, targets, <span class="stringliteral">&quot;mean&quot;</span>);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Backward pass</span></div>
<div class="line">loss.<a class="code hl_function" href="classTensor.html#ace4d96d1fc0d364577334241662fb60e">backward</a>();</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Gradients are now in predictions.grad()</span></div>
<div class="line"><span class="comment">// Gradient: d/dx[(x-y)^2] = 2(x-y)/n</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Different reduction modes:</span></div>
<div class="line"><span class="keyword">auto</span> loss_sum = <a class="code hl_function" href="#aff5180b0c38ddd16d08f058205c01ef7">loss::mse_loss</a>(pred, target, <span class="stringliteral">&quot;sum&quot;</span>);     <span class="comment">// No averaging</span></div>
<div class="line"><span class="keyword">auto</span> loss_none = <a class="code hl_function" href="#aff5180b0c38ddd16d08f058205c01ef7">loss::mse_loss</a>(pred, target, <span class="stringliteral">&quot;none&quot;</span>);   <span class="comment">// Element-wise</span></div>
<div class="ttc" id="aclassTensor_html_ace4d96d1fc0d364577334241662fb60e"><div class="ttname"><a href="classTensor.html#ace4d96d1fc0d364577334241662fb60e">Tensor::backward</a></div><div class="ttdeci">std::optional&lt; TensorError &gt; backward(const Tensor&lt; T, N &gt; *gradient=nullptr)</div><div class="ttdoc">Perform backward pass to compute gradients (autograd).</div><div class="ttdef"><b>Definition</b> <a href="tensor_8h_source.html#l00783">tensor.h:783</a></div></div>
<div class="ttc" id="anamespaceloss_html_aff5180b0c38ddd16d08f058205c01ef7"><div class="ttname"><a href="#aff5180b0c38ddd16d08f058205c01ef7">loss::mse_loss</a></div><div class="ttdeci">Tensor&lt; T, N &gt; mse_loss(const Tensor&lt; T, N &gt; &amp;predictions, const Tensor&lt; T, N &gt; &amp;targets, const std::string &amp;reduction)</div><div class="ttdoc">Mean Squared Error (MSE) Loss.</div><div class="ttdef"><b>Definition</b> <a href="loss__functions_8h_source.html#l00087">loss_functions.h:87</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="loss__functions_8h_source.html#l00087">87</a> of file <a class="el" href="loss__functions_8h_source.html">loss_functions.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   88</span>                                                  {</div>
<div class="line"><span class="lineno">   89</span>    <span class="keywordflow">if</span> (predictions.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>() != targets.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>()) {</div>
<div class="line"><span class="lineno">   90</span>        <span class="keywordflow">return</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, N&gt;</a>({1}, predictions.<a class="code hl_function" href="classTensor.html#a83ae42925e21d2871d755407d7505c10">uses_gpu</a>(), <span class="keyword">false</span>);</div>
<div class="line"><span class="lineno">   91</span>    }</div>
<div class="line"><span class="lineno">   92</span>    </div>
<div class="line"><span class="lineno">   93</span>    <span class="keywordtype">bool</span> track_grad = predictions.<a class="code hl_function" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8">requires_grad</a>();</div>
<div class="line"><span class="lineno">   94</span>    <span class="keywordtype">size_t</span> total = predictions.<a class="code hl_function" href="classTensor.html#aad6aeb4859940befd0c46f0b99388bd7">total_size</a>();</div>
<div class="line"><span class="lineno">   95</span>    </div>
<div class="line"><span class="lineno">   96</span>    <span class="comment">// Compute mean squared error</span></div>
<div class="line"><span class="lineno">   97</span>    T loss_value = T(0);</div>
<div class="line"><span class="lineno">   98</span>    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; total; ++i) {</div>
<div class="line"><span class="lineno">   99</span>        T d = predictions.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i] - targets.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i];</div>
<div class="line"><span class="lineno">  100</span>        loss_value += d * d;</div>
<div class="line"><span class="lineno">  101</span>    }</div>
<div class="line"><span class="lineno">  102</span>    </div>
<div class="line"><span class="lineno">  103</span>    <span class="keywordflow">if</span> (reduction == <span class="stringliteral">&quot;mean&quot;</span>) {</div>
<div class="line"><span class="lineno">  104</span>        loss_value /= <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(total);</div>
<div class="line"><span class="lineno">  105</span>    }</div>
<div class="line"><span class="lineno">  106</span>    </div>
<div class="line"><span class="lineno">  107</span>    <span class="comment">// Create result tensor filled with the loss value</span></div>
<div class="line"><span class="lineno">  108</span>    Tensor&lt;T, N&gt; result(predictions.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>(), predictions.<a class="code hl_function" href="classTensor.html#a83ae42925e21d2871d755407d7505c10">uses_gpu</a>(), track_grad);</div>
<div class="line"><span class="lineno">  109</span>    result.fill(loss_value);</div>
<div class="line"><span class="lineno">  110</span>    result.is_leaf_ = <span class="keyword">false</span>;</div>
<div class="line"><span class="lineno">  111</span>    </div>
<div class="line"><span class="lineno">  112</span>    <span class="comment">// Setup backward pass</span></div>
<div class="line"><span class="lineno">  113</span>    <span class="keywordflow">if</span> (track_grad) {</div>
<div class="line"><span class="lineno">  114</span>        Tensor&lt;T, N&gt; pred_copy = predictions.<a class="code hl_function" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285">detach</a>();</div>
<div class="line"><span class="lineno">  115</span>        Tensor&lt;T, N&gt; targ_copy = targets.<a class="code hl_function" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285">detach</a>();</div>
<div class="line"><span class="lineno">  116</span>        Tensor&lt;T, N&gt;* pred_ptr = <span class="keyword">const_cast&lt;</span>Tensor&lt;T, N&gt;*<span class="keyword">&gt;</span>(&amp;predictions);</div>
<div class="line"><span class="lineno">  117</span>        T scale = (reduction == <span class="stringliteral">&quot;mean&quot;</span>) ? T(2) / <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(total) : T(2);</div>
<div class="line"><span class="lineno">  118</span>        </div>
<div class="line"><span class="lineno">  119</span>        result.register_backward([pred_ptr, pred_copy, targ_copy, scale, total]</div>
<div class="line"><span class="lineno">  120</span>                                 (<span class="keyword">const</span> Tensor&lt;T, N&gt;&amp; grad) {</div>
<div class="line"><span class="lineno">  121</span>            <span class="comment">// Gradient of MSE: d/dx[(x-y)^2] = 2(x-y)</span></div>
<div class="line"><span class="lineno">  122</span>            <span class="keywordflow">if</span> (pred_ptr-&gt;<a class="code hl_function" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8">requires_grad</a>()) {</div>
<div class="line"><span class="lineno">  123</span>                <span class="keywordflow">if</span> (!pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>) {</div>
<div class="line"><span class="lineno">  124</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a> = std::make_unique&lt;Tensor&lt;T, N&gt;&gt;(pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a43fadce1ce3dd9ddb47c8b82168edb52">dims_</a>, </div>
<div class="line"><span class="lineno">  125</span>                                                                      pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#aa3214ec02474d17494e1eb95d5006262">use_gpu_</a>, <span class="keyword">false</span>);</div>
<div class="line"><span class="lineno">  126</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>-&gt;fill(T(0));</div>
<div class="line"><span class="lineno">  127</span>                }</div>
<div class="line"><span class="lineno">  128</span>                </div>
<div class="line"><span class="lineno">  129</span>                <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; total; ++i) {</div>
<div class="line"><span class="lineno">  130</span>                    T diff = pred_copy.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i] - targ_copy.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i];</div>
<div class="line"><span class="lineno">  131</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>-&gt;data()[i] += grad.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i] * scale * diff;</div>
<div class="line"><span class="lineno">  132</span>                }</div>
<div class="line"><span class="lineno">  133</span>                </div>
<div class="line"><span class="lineno">  134</span>                <span class="keywordflow">if</span> (!pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#abf7257d130e6bc46f6d2cd7de4ea2909">is_leaf_</a> &amp;&amp; !pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a89d50a2ba15e719d4bf025178150d308">backward_funcs_</a>.empty()) {</div>
<div class="line"><span class="lineno">  135</span>                    <span class="keywordflow">for</span> (<span class="keyword">auto</span>&amp; func : pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a89d50a2ba15e719d4bf025178150d308">backward_funcs_</a>) {</div>
<div class="line"><span class="lineno">  136</span>                        func(*pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>);</div>
<div class="line"><span class="lineno">  137</span>                    }</div>
<div class="line"><span class="lineno">  138</span>                }</div>
<div class="line"><span class="lineno">  139</span>            }</div>
<div class="line"><span class="lineno">  140</span>        });</div>
<div class="line"><span class="lineno">  141</span>    }</div>
<div class="line"><span class="lineno">  142</span>    </div>
<div class="line"><span class="lineno">  143</span>    <span class="keywordflow">return</span> result;</div>
<div class="line"><span class="lineno">  144</span>}</div>
</div><!-- fragment --><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespaceloss_aff5180b0c38ddd16d08f058205c01ef7_cgraph.png" border="0" usemap="#anamespaceloss_aff5180b0c38ddd16d08f058205c01ef7_cgraph" loading="lazy" alt=""/></div>
<map name="anamespaceloss_aff5180b0c38ddd16d08f058205c01ef7_cgraph" id="anamespaceloss_aff5180b0c38ddd16d08f058205c01ef7_cgraph">
<area shape="rect" title="Mean Squared Error (MSE) Loss." alt="" coords="5,183,115,209"/>
<area shape="rect" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4" title=" " alt="" coords="203,5,296,32"/>
<area shape="poly" title=" " alt="" coords="64,181,98,115,126,75,161,42,187,28,189,33,164,46,130,79,102,117,69,184"/>
<area shape="rect" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285" title=" " alt="" coords="196,56,304,83"/>
<area shape="poly" title=" " alt="" coords="70,180,107,137,132,113,161,92,181,83,183,88,164,97,136,117,111,141,74,184"/>
<area shape="rect" href="classTensor.html#a03d314303958ffc5644796d8025cd012" title=" " alt="" coords="202,107,298,133"/>
<area shape="poly" title=" " alt="" coords="85,180,162,143,186,134,188,139,164,148,87,184"/>
<area shape="rect" href="classTensor.html#a234f74cd16bd13561f7963ba29efb163" title="Fill the tensor with a specified value." alt="" coords="209,157,290,184"/>
<area shape="poly" title=" " alt="" coords="115,186,193,175,194,181,115,191"/>
<area shape="rect" href="classTensor.html#af274147e833dd1ab399ce08155ad169b" title=" " alt="" coords="163,208,337,235"/>
<area shape="poly" title=" " alt="" coords="115,201,148,205,147,210,115,206"/>
<area shape="rect" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8" title=" " alt="" coords="177,259,323,285"/>
<area shape="poly" title=" " alt="" coords="87,208,164,244,183,251,181,256,162,249,85,212"/>
<area shape="rect" href="classTensor.html#aad6aeb4859940befd0c46f0b99388bd7" title=" " alt="" coords="188,309,312,336"/>
<area shape="poly" title=" " alt="" coords="74,208,111,251,136,275,164,295,175,301,172,305,161,300,132,279,107,255,70,212"/>
<area shape="rect" href="classTensor.html#a83ae42925e21d2871d755407d7505c10" title=" " alt="" coords="188,360,312,387"/>
<area shape="poly" title=" " alt="" coords="69,208,102,275,130,313,164,346,175,353,172,357,161,350,126,317,98,277,64,211"/>
</map>
</div>

</div>
</div>
<a id="a2d2c1dacdb37d834f7dc10e85cd1a742" name="a2d2c1dacdb37d834f7dc10e85cd1a742"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2d2c1dacdb37d834f7dc10e85cd1a742">&#9670;&#160;</a></span>smooth_l1_loss()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T, size_t N&gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classTensor.html">Tensor</a>&lt; T, 1 &gt; loss::smooth_l1_loss </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>predictions</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classTensor.html">Tensor</a>&lt; T, N &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>targets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T</td>          <td class="paramname"><span class="paramname"><em>beta</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;</td>          <td class="paramname"><span class="paramname"><em>reduction</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Smooth L1 loss (Huber loss). </p>
<p>Smooth L1 Loss (Huber Loss). Combines MSE and L1 loss - less sensitive to outliers than MSE.</p>
<p>L = 0.5 * (x - y)^2 if |x - y| &lt; beta L = beta * (|x - y| - 0.5 * beta) otherwise</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">predictions</td><td>Predicted values </td></tr>
    <tr><td class="paramname">targets</td><td>True target values </td></tr>
    <tr><td class="paramname">beta</td><td>Threshold for switching between L1 and L2 (default: 1.0) </td></tr>
    <tr><td class="paramname">reduction</td><td>"mean" or "sum" (default: "mean") </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Scalar loss tensor with autograd support </dd></dl>

<p class="definition">Definition at line <a class="el" href="loss__functions_8h_source.html#l00366">366</a> of file <a class="el" href="loss__functions_8h_source.html">loss_functions.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  369</span>                                                        {</div>
<div class="line"><span class="lineno">  370</span>    <span class="keywordflow">if</span> (predictions.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>() != targets.<a class="code hl_function" href="classTensor.html#a03d314303958ffc5644796d8025cd012">dims</a>()) {</div>
<div class="line"><span class="lineno">  371</span>        <span class="keywordflow">return</span> <a class="code hl_class" href="classTensor.html">Tensor&lt;T, 1&gt;</a>({1}, predictions.<a class="code hl_function" href="classTensor.html#a83ae42925e21d2871d755407d7505c10">uses_gpu</a>(), <span class="keyword">false</span>);</div>
<div class="line"><span class="lineno">  372</span>    }</div>
<div class="line"><span class="lineno">  373</span>    </div>
<div class="line"><span class="lineno">  374</span>    <span class="keywordtype">size_t</span> total = predictions.<a class="code hl_function" href="classTensor.html#aad6aeb4859940befd0c46f0b99388bd7">total_size</a>();</div>
<div class="line"><span class="lineno">  375</span>    <span class="keywordtype">bool</span> track_grad = predictions.<a class="code hl_function" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8">requires_grad</a>();</div>
<div class="line"><span class="lineno">  376</span>    </div>
<div class="line"><span class="lineno">  377</span>    T total_loss = T(0);</div>
<div class="line"><span class="lineno">  378</span>    </div>
<div class="line"><span class="lineno">  379</span>    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; total; ++i) {</div>
<div class="line"><span class="lineno">  380</span>        T diff = std::abs(predictions.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i] - targets.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i]);</div>
<div class="line"><span class="lineno">  381</span>        </div>
<div class="line"><span class="lineno">  382</span>        <span class="keywordflow">if</span> (diff &lt; beta) {</div>
<div class="line"><span class="lineno">  383</span>            total_loss += T(0.5) * diff * diff / beta;</div>
<div class="line"><span class="lineno">  384</span>        } <span class="keywordflow">else</span> {</div>
<div class="line"><span class="lineno">  385</span>            total_loss += diff - T(0.5) * beta;</div>
<div class="line"><span class="lineno">  386</span>        }</div>
<div class="line"><span class="lineno">  387</span>    }</div>
<div class="line"><span class="lineno">  388</span>    </div>
<div class="line"><span class="lineno">  389</span>    <a class="code hl_typedef" href="tensor_8h.html#ab58b5b2b06188fb918bfa78bdfce068b">TensorIndices&lt;1&gt;</a> loss_shape = {1};</div>
<div class="line"><span class="lineno">  390</span>    Tensor&lt;T, 1&gt; loss(loss_shape, predictions.<a class="code hl_function" href="classTensor.html#a83ae42925e21d2871d755407d7505c10">uses_gpu</a>(), track_grad);</div>
<div class="line"><span class="lineno">  391</span>    loss.is_leaf_ = <span class="keyword">false</span>;</div>
<div class="line"><span class="lineno">  392</span>    </div>
<div class="line"><span class="lineno">  393</span>    <span class="keywordflow">if</span> (reduction == <span class="stringliteral">&quot;mean&quot;</span>) {</div>
<div class="line"><span class="lineno">  394</span>        loss[{0}] = total_loss / <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(total);</div>
<div class="line"><span class="lineno">  395</span>    } <span class="keywordflow">else</span> {</div>
<div class="line"><span class="lineno">  396</span>        loss[{0}] = total_loss;</div>
<div class="line"><span class="lineno">  397</span>    }</div>
<div class="line"><span class="lineno">  398</span>    </div>
<div class="line"><span class="lineno">  399</span>    <span class="comment">// Setup backward pass</span></div>
<div class="line"><span class="lineno">  400</span>    <span class="keywordflow">if</span> (track_grad) {</div>
<div class="line"><span class="lineno">  401</span>        Tensor&lt;T, N&gt; pred_copy = predictions.<a class="code hl_function" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285">detach</a>();</div>
<div class="line"><span class="lineno">  402</span>        Tensor&lt;T, N&gt; targ_copy = targets.<a class="code hl_function" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285">detach</a>();</div>
<div class="line"><span class="lineno">  403</span>        Tensor&lt;T, N&gt;* pred_ptr = <span class="keyword">const_cast&lt;</span>Tensor&lt;T, N&gt;*<span class="keyword">&gt;</span>(&amp;predictions);</div>
<div class="line"><span class="lineno">  404</span>        T scale = (reduction == <span class="stringliteral">&quot;mean&quot;</span>) ? T(1) / <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(total) : T(1);</div>
<div class="line"><span class="lineno">  405</span>        </div>
<div class="line"><span class="lineno">  406</span>        loss.register_backward([pred_ptr, pred_copy, targ_copy, scale, total, beta]</div>
<div class="line"><span class="lineno">  407</span>                              (<span class="keyword">const</span> Tensor&lt;T, 1&gt;&amp; grad) {</div>
<div class="line"><span class="lineno">  408</span>            <span class="comment">// Gradient of smooth L1</span></div>
<div class="line"><span class="lineno">  409</span>            <span class="keywordflow">if</span> (pred_ptr-&gt;<a class="code hl_function" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8">requires_grad</a>()) {</div>
<div class="line"><span class="lineno">  410</span>                <span class="keywordflow">if</span> (!pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>) {</div>
<div class="line"><span class="lineno">  411</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a> = std::make_unique&lt;Tensor&lt;T, N&gt;&gt;(pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a43fadce1ce3dd9ddb47c8b82168edb52">dims_</a>, </div>
<div class="line"><span class="lineno">  412</span>                                                                      pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#aa3214ec02474d17494e1eb95d5006262">use_gpu_</a>, <span class="keyword">false</span>);</div>
<div class="line"><span class="lineno">  413</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>-&gt;fill(T(0));</div>
<div class="line"><span class="lineno">  414</span>                }</div>
<div class="line"><span class="lineno">  415</span>                </div>
<div class="line"><span class="lineno">  416</span>                <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; total; ++i) {</div>
<div class="line"><span class="lineno">  417</span>                    T diff = pred_copy.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i] - targ_copy.<a class="code hl_function" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4">data</a>()[i];</div>
<div class="line"><span class="lineno">  418</span>                    T abs_diff = std::abs(diff);</div>
<div class="line"><span class="lineno">  419</span>                    T grad_val;</div>
<div class="line"><span class="lineno">  420</span>                    </div>
<div class="line"><span class="lineno">  421</span>                    <span class="keywordflow">if</span> (abs_diff &lt; beta) {</div>
<div class="line"><span class="lineno">  422</span>                        grad_val = diff / beta;</div>
<div class="line"><span class="lineno">  423</span>                    } <span class="keywordflow">else</span> {</div>
<div class="line"><span class="lineno">  424</span>                        grad_val = (diff &gt; T(0)) ? T(1) : T(-1);</div>
<div class="line"><span class="lineno">  425</span>                    }</div>
<div class="line"><span class="lineno">  426</span>                    </div>
<div class="line"><span class="lineno">  427</span>                    pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>-&gt;data()[i] += grad[{0}] * scale * grad_val;</div>
<div class="line"><span class="lineno">  428</span>                }</div>
<div class="line"><span class="lineno">  429</span>                </div>
<div class="line"><span class="lineno">  430</span>                <span class="keywordflow">if</span> (!pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#abf7257d130e6bc46f6d2cd7de4ea2909">is_leaf_</a> &amp;&amp; !pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a89d50a2ba15e719d4bf025178150d308">backward_funcs_</a>.empty()) {</div>
<div class="line"><span class="lineno">  431</span>                    <span class="keywordflow">for</span> (<span class="keyword">auto</span>&amp; func : pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a89d50a2ba15e719d4bf025178150d308">backward_funcs_</a>) {</div>
<div class="line"><span class="lineno">  432</span>                        func(*pred_ptr-&gt;<a class="code hl_variable" href="classTensor.html#a4d61cfe1428b0302cd1683dc1e6fd733">grad_</a>);</div>
<div class="line"><span class="lineno">  433</span>                    }</div>
<div class="line"><span class="lineno">  434</span>                }</div>
<div class="line"><span class="lineno">  435</span>            }</div>
<div class="line"><span class="lineno">  436</span>        });</div>
<div class="line"><span class="lineno">  437</span>    }</div>
<div class="line"><span class="lineno">  438</span>    </div>
<div class="line"><span class="lineno">  439</span>    <span class="keywordflow">return</span> loss;</div>
<div class="line"><span class="lineno">  440</span>}</div>
</div><!-- fragment --><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespaceloss_a2d2c1dacdb37d834f7dc10e85cd1a742_cgraph.png" border="0" usemap="#anamespaceloss_a2d2c1dacdb37d834f7dc10e85cd1a742_cgraph" loading="lazy" alt=""/></div>
<map name="anamespaceloss_a2d2c1dacdb37d834f7dc10e85cd1a742_cgraph" id="anamespaceloss_a2d2c1dacdb37d834f7dc10e85cd1a742_cgraph">
<area shape="rect" title="Smooth L1 loss (Huber loss)." alt="" coords="5,132,150,159"/>
<area shape="rect" href="classTensor.html#a07bf856a863505d2960b0bfad9c3b3f4" title=" " alt="" coords="224,5,318,32"/>
<area shape="poly" title=" " alt="" coords="90,129,135,87,164,63,196,42,208,36,211,40,199,46,167,67,138,91,94,133"/>
<area shape="rect" href="classTensor.html#ae5b52299c11f3ba81e781cb008542285" title=" " alt="" coords="217,56,324,83"/>
<area shape="poly" title=" " alt="" coords="109,129,197,92,214,86,216,91,199,97,111,134"/>
<area shape="rect" href="classTensor.html#a03d314303958ffc5644796d8025cd012" title=" " alt="" coords="223,107,319,133"/>
<area shape="poly" title=" " alt="" coords="149,133,207,126,208,131,150,139"/>
<area shape="rect" href="classTensor.html#a947326cf96b1bf3423b40861e1213ab8" title=" " alt="" coords="198,157,344,184"/>
<area shape="poly" title=" " alt="" coords="150,152,182,156,182,162,149,157"/>
<area shape="rect" href="classTensor.html#aad6aeb4859940befd0c46f0b99388bd7" title=" " alt="" coords="209,208,333,235"/>
<area shape="poly" title=" " alt="" coords="111,157,199,194,216,200,214,205,197,198,109,162"/>
<area shape="rect" href="classTensor.html#a83ae42925e21d2871d755407d7505c10" title=" " alt="" coords="209,259,333,285"/>
<area shape="poly" title=" " alt="" coords="94,157,138,200,167,223,199,244,209,250,207,254,196,249,164,228,135,204,90,161"/>
</map>
</div>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<div id="page-nav" class="page-nav-panel">
<div id="page-nav-resize-handle"></div>
<div id="page-nav-tree">
<div id="page-nav-contents">
</div><!-- page-nav-contents -->
</div><!-- page-nav-tree -->
</div><!-- page-nav -->
</div><!-- container -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a href="namespaceloss.html">loss</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.15.0 </li>
  </ul>
</div>
</body>
</html>
